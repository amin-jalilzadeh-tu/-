{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "05adb6e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: Directory not found: --f=c:\\Users\\aminj\\AppData\\Roaming\\jupyter\\runtime\\kernel-v35fb68adb961166f7b060bf192bf7b9f4e0a470d4.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "1",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\New folder (2)\\Anaconda\\DDsaie\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3534: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "E+ Workflow Dependency Tracker and Validator\n",
    "\n",
    "This script analyzes the E+ workflow, tracks dependencies between steps,\n",
    "and validates that inputs are properly generated and ready to use.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import json\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Tuple, Optional, Set\n",
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "class WorkflowStep:\n",
    "    \"\"\"Represents a single step in the workflow\"\"\"\n",
    "    def __init__(self, name: str, enabled_flag: str, config_section: str):\n",
    "        self.name = name\n",
    "        self.enabled_flag = enabled_flag\n",
    "        self.config_section = config_section\n",
    "        self.inputs = []\n",
    "        self.outputs = []\n",
    "        self.dependencies = []\n",
    "        self.validation_rules = []\n",
    "\n",
    "class WorkflowTracker:\n",
    "    \"\"\"Tracks and validates the E+ workflow execution\"\"\"\n",
    "    \n",
    "    def __init__(self, job_output_dir: str):\n",
    "        self.job_output_dir = Path(job_output_dir)\n",
    "        self.job_id = self.job_output_dir.name\n",
    "        self.base_dir = self.job_output_dir.parent.parent\n",
    "        \n",
    "        # Setup logging\n",
    "        self.setup_logging()\n",
    "        \n",
    "        # Define workflow steps with their dependencies\n",
    "        self.workflow_steps = self._define_workflow_steps()\n",
    "        \n",
    "        # Track execution status\n",
    "        self.execution_status = {}\n",
    "        \n",
    "    def setup_logging(self):\n",
    "        \"\"\"Setup logging configuration\"\"\"\n",
    "        log_file = self.job_output_dir / \"workflow_tracker.log\"\n",
    "        logging.basicConfig(\n",
    "            level=logging.INFO,\n",
    "            format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "            handlers=[\n",
    "                logging.FileHandler(log_file),\n",
    "                logging.StreamHandler()\n",
    "            ]\n",
    "        )\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        \n",
    "    def _define_workflow_steps(self) -> Dict[str, WorkflowStep]:\n",
    "        \"\"\"Define all workflow steps and their dependencies\"\"\"\n",
    "        steps = {}\n",
    "        \n",
    "        # 1. IDF Creation Step\n",
    "        idf_step = WorkflowStep(\n",
    "            name=\"IDF Creation\",\n",
    "            enabled_flag=\"perform_idf_creation\",\n",
    "            config_section=\"idf_creation\"\n",
    "        )\n",
    "        idf_step.inputs = [\n",
    "            (\"config\", \"user_configs_folder\"),\n",
    "            (\"data\", \"idf_templates\"),\n",
    "            (\"data\", \"weather_files\")\n",
    "        ]\n",
    "        idf_step.outputs = [\n",
    "            (\"output_IDFs\", \"*.idf\"),\n",
    "            (\"idf_tracker.json\", None)\n",
    "        ]\n",
    "        steps[\"idf_creation\"] = idf_step\n",
    "        \n",
    "        # 2. Simulation Step\n",
    "        sim_step = WorkflowStep(\n",
    "            name=\"Simulation\",\n",
    "            enabled_flag=\"run_simulations\",\n",
    "            config_section=\"idf_creation\"\n",
    "        )\n",
    "        sim_step.inputs = [\n",
    "            (\"output_IDFs\", \"*.idf\"),\n",
    "            (\"weather\", \"*.epw\")\n",
    "        ]\n",
    "        sim_step.outputs = [\n",
    "            (\"Sim_Results\", \"*.sql\"),\n",
    "            (\"Sim_Results\", \"*.htm\"),\n",
    "            (\"Sim_Results\", \"*.csv\")\n",
    "        ]\n",
    "        sim_step.dependencies = [\"idf_creation\"]\n",
    "        steps[\"simulation\"] = sim_step\n",
    "        \n",
    "        # 3. Parsing Step\n",
    "        parse_step = WorkflowStep(\n",
    "            name=\"Parsing\",\n",
    "            enabled_flag=\"perform_parsing\",\n",
    "            config_section=\"parsing\"\n",
    "        )\n",
    "        parse_step.inputs = [\n",
    "            (\"Sim_Results\", \"*.sql\"),\n",
    "            (\"output_IDFs\", \"*.idf\")\n",
    "        ]\n",
    "        parse_step.outputs = [\n",
    "            (\"parsed_data\", \"*.parquet\"),\n",
    "            (\"parsed_data/idf_data\", \"*.parquet\"),\n",
    "            (\"parsed_data/output_data\", \"*.parquet\")\n",
    "        ]\n",
    "        parse_step.dependencies = [\"simulation\"]\n",
    "        steps[\"parsing\"] = parse_step\n",
    "        \n",
    "        # 4. Modification Step\n",
    "        mod_step = WorkflowStep(\n",
    "            name=\"Modification\",\n",
    "            enabled_flag=\"perform_modification\",\n",
    "            config_section=\"modification\"\n",
    "        )\n",
    "        mod_step.inputs = [\n",
    "            (\"output_IDFs\", \"*.idf\"),\n",
    "            (\"parsed_data\", \"*.parquet\")\n",
    "        ]\n",
    "        mod_step.outputs = [\n",
    "            (\"modified_idfs\", \"*.idf\"),\n",
    "            (\"modified_idfs/modification_summary.json\", None),\n",
    "            (\"modified_idfs/*/modification_report.json\", None)\n",
    "        ]\n",
    "        mod_step.dependencies = [\"parsing\"]\n",
    "        steps[\"modification\"] = mod_step\n",
    "        \n",
    "        # 5. Modified Simulation Step\n",
    "        mod_sim_step = WorkflowStep(\n",
    "            name=\"Modified Simulation\",\n",
    "            enabled_flag=\"run_modified_simulations\",\n",
    "            config_section=\"modification.post_modification\"\n",
    "        )\n",
    "        mod_sim_step.inputs = [\n",
    "            (\"modified_idfs\", \"*.idf\")\n",
    "        ]\n",
    "        mod_sim_step.outputs = [\n",
    "            (\"Modified_Sim_Results\", \"*.sql\"),\n",
    "            (\"Modified_Sim_Results\", \"*.csv\")\n",
    "        ]\n",
    "        mod_sim_step.dependencies = [\"modification\"]\n",
    "        steps[\"modified_simulation\"] = mod_sim_step\n",
    "        \n",
    "        # 6. Modified Parsing Step\n",
    "        mod_parse_step = WorkflowStep(\n",
    "            name=\"Modified Parsing\",\n",
    "            enabled_flag=\"parse_modified_results\",\n",
    "            config_section=\"modification.post_modification\"\n",
    "        )\n",
    "        mod_parse_step.inputs = [\n",
    "            (\"Modified_Sim_Results\", \"*.sql\"),\n",
    "            (\"modified_idfs\", \"*.idf\")\n",
    "        ]\n",
    "        mod_parse_step.outputs = [\n",
    "            (\"parsed_modified_results\", \"*.parquet\")\n",
    "        ]\n",
    "        mod_parse_step.dependencies = [\"modified_simulation\"]\n",
    "        steps[\"modified_parsing\"] = mod_parse_step\n",
    "        \n",
    "        # 7. Validation Step\n",
    "        val_step = WorkflowStep(\n",
    "            name=\"Validation\",\n",
    "            enabled_flag=\"perform_validation\",\n",
    "            config_section=\"validation\"\n",
    "        )\n",
    "        val_step.inputs = [\n",
    "            (\"parsed_data\", \"*.parquet\"),\n",
    "            (\"parsed_modified_results\", \"*.parquet\"),\n",
    "            (\"measured_data.csv\", None)\n",
    "        ]\n",
    "        val_step.outputs = [\n",
    "            (\"validation_results\", \"*.json\"),\n",
    "            (\"validation_results\", \"*.html\")\n",
    "        ]\n",
    "        val_step.dependencies = [\"parsing\", \"modified_parsing\"]\n",
    "        steps[\"validation\"] = val_step\n",
    "        \n",
    "        # 8. Sensitivity Analysis Step\n",
    "        sens_step = WorkflowStep(\n",
    "            name=\"Sensitivity Analysis\",\n",
    "            enabled_flag=\"perform_sensitivity\",\n",
    "            config_section=\"sensitivity\"\n",
    "        )\n",
    "        sens_step.inputs = [\n",
    "            (\"parsed_data\", \"*.parquet\"),\n",
    "            (\"parsed_modified_results\", \"*.parquet\")\n",
    "        ]\n",
    "        sens_step.outputs = [\n",
    "            (\"sensitivity_results\", \"*.parquet\"),\n",
    "            (\"sensitivity_results\", \"*.json\"),\n",
    "            (\"sensitivity_results\", \"*.html\")\n",
    "        ]\n",
    "        sens_step.dependencies = [\"parsing\", \"modified_parsing\"]\n",
    "        steps[\"sensitivity\"] = sens_step\n",
    "        \n",
    "        # 9. Surrogate Modeling Step\n",
    "        surr_step = WorkflowStep(\n",
    "            name=\"Surrogate Modeling\",\n",
    "            enabled_flag=\"perform_surrogate\",\n",
    "            config_section=\"surrogate\"\n",
    "        )\n",
    "        surr_step.inputs = [\n",
    "            (\"parsed_data\", \"*.parquet\"),\n",
    "            (\"parsed_modified_results\", \"*.parquet\"),\n",
    "            (\"sensitivity_results\", \"*.parquet\")\n",
    "        ]\n",
    "        surr_step.outputs = [\n",
    "            (\"surrogate_models\", \"*.pkl\"),\n",
    "            (\"surrogate_models\", \"*.json\"),\n",
    "            (\"surrogate_models/validation_report.json\", None)\n",
    "        ]\n",
    "        surr_step.dependencies = [\"sensitivity\"]\n",
    "        steps[\"surrogate\"] = surr_step\n",
    "        \n",
    "        # 10. Calibration Step\n",
    "        cal_step = WorkflowStep(\n",
    "            name=\"Calibration\",\n",
    "            enabled_flag=\"perform_calibration\",\n",
    "            config_section=\"calibration\"\n",
    "        )\n",
    "        cal_step.inputs = [\n",
    "            (\"surrogate_models\", \"*.pkl\"),\n",
    "            (\"validation_results\", \"*.json\")\n",
    "        ]\n",
    "        cal_step.outputs = [\n",
    "            (\"calibration_results\", \"*.json\"),\n",
    "            (\"calibration_results\", \"*.parquet\")\n",
    "        ]\n",
    "        cal_step.dependencies = [\"surrogate\", \"validation\"]\n",
    "        steps[\"calibration\"] = cal_step\n",
    "        \n",
    "        return steps\n",
    "    \n",
    "    def check_file_exists(self, relative_path: str, pattern: Optional[str] = None) -> Tuple[bool, List[str]]:\n",
    "        \"\"\"Check if file(s) exist in the job output directory\"\"\"\n",
    "        full_path = self.job_output_dir / relative_path\n",
    "        \n",
    "        if pattern:\n",
    "            # Check for pattern match\n",
    "            search_path = full_path / pattern if full_path.is_dir() else full_path.parent / pattern\n",
    "            matches = glob.glob(str(search_path))\n",
    "            return len(matches) > 0, matches\n",
    "        else:\n",
    "            # Check for specific file\n",
    "            return full_path.exists(), [str(full_path)] if full_path.exists() else []\n",
    "    \n",
    "    def validate_step_inputs(self, step: WorkflowStep) -> Dict[str, bool]:\n",
    "        \"\"\"Validate all inputs for a workflow step\"\"\"\n",
    "        input_status = {}\n",
    "        \n",
    "        for input_path, pattern in step.inputs:\n",
    "            exists, files = self.check_file_exists(input_path, pattern)\n",
    "            input_key = f\"{input_path}/{pattern}\" if pattern else input_path\n",
    "            input_status[input_key] = {\n",
    "                \"exists\": exists,\n",
    "                \"files\": files,\n",
    "                \"count\": len(files)\n",
    "            }\n",
    "            \n",
    "        return input_status\n",
    "    \n",
    "    def validate_step_outputs(self, step: WorkflowStep) -> Dict[str, bool]:\n",
    "        \"\"\"Validate all outputs for a workflow step\"\"\"\n",
    "        output_status = {}\n",
    "        \n",
    "        for output_path, pattern in step.outputs:\n",
    "            exists, files = self.check_file_exists(output_path, pattern)\n",
    "            output_key = f\"{output_path}/{pattern}\" if pattern else output_path\n",
    "            output_status[output_key] = {\n",
    "                \"exists\": exists,\n",
    "                \"files\": files,\n",
    "                \"count\": len(files)\n",
    "            }\n",
    "            \n",
    "        return output_status\n",
    "    \n",
    "    def check_dependencies_met(self, step_name: str) -> Tuple[bool, List[str]]:\n",
    "        \"\"\"Check if all dependencies for a step are met\"\"\"\n",
    "        step = self.workflow_steps.get(step_name)\n",
    "        if not step:\n",
    "            return False, [f\"Step {step_name} not found\"]\n",
    "        \n",
    "        missing_deps = []\n",
    "        for dep in step.dependencies:\n",
    "            dep_step = self.workflow_steps.get(dep)\n",
    "            if dep_step:\n",
    "                # Check if dependency step has completed (all outputs exist)\n",
    "                output_status = self.validate_step_outputs(dep_step)\n",
    "                for output, status in output_status.items():\n",
    "                    if not status[\"exists\"]:\n",
    "                        missing_deps.append(f\"{dep}: {output}\")\n",
    "        \n",
    "        return len(missing_deps) == 0, missing_deps\n",
    "    \n",
    "    def analyze_workflow(self) -> Dict[str, Any]:\n",
    "        \"\"\"Analyze the entire workflow and return comprehensive status\"\"\"\n",
    "        analysis = {\n",
    "            \"job_id\": self.job_id,\n",
    "            \"job_output_dir\": str(self.job_output_dir),\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"steps\": {}\n",
    "        }\n",
    "        \n",
    "        # Check configuration file\n",
    "        config_files = list(self.job_output_dir.glob(\"combined*.json\"))\n",
    "        if config_files:\n",
    "            with open(config_files[0], 'r') as f:\n",
    "                config = json.load(f)\n",
    "                main_config = config.get(\"main_config\", {})\n",
    "        else:\n",
    "            main_config = {}\n",
    "            self.logger.warning(\"No configuration file found\")\n",
    "        \n",
    "        # Analyze each step\n",
    "        for step_name, step in self.workflow_steps.items():\n",
    "            self.logger.info(f\"\\n{'='*60}\")\n",
    "            self.logger.info(f\"Analyzing step: {step.name}\")\n",
    "            self.logger.info(f\"{'='*60}\")\n",
    "            \n",
    "            # Check if step is enabled\n",
    "            config_section = main_config\n",
    "            for section in step.config_section.split('.'):\n",
    "                config_section = config_section.get(section, {})\n",
    "            \n",
    "            enabled = config_section.get(step.enabled_flag, False)\n",
    "            \n",
    "            # Check dependencies\n",
    "            deps_met, missing_deps = self.check_dependencies_met(step_name)\n",
    "            \n",
    "            # Validate inputs and outputs\n",
    "            input_status = self.validate_step_inputs(step)\n",
    "            output_status = self.validate_step_outputs(step)\n",
    "            \n",
    "            # Determine step status\n",
    "            inputs_ready = all(status[\"exists\"] for status in input_status.values())\n",
    "            outputs_ready = all(status[\"exists\"] for status in output_status.values())\n",
    "            \n",
    "            if not enabled:\n",
    "                status = \"DISABLED\"\n",
    "            elif not deps_met:\n",
    "                status = \"BLOCKED\"\n",
    "            elif not inputs_ready:\n",
    "                status = \"MISSING_INPUTS\"\n",
    "            elif outputs_ready:\n",
    "                status = \"COMPLETED\"\n",
    "            else:\n",
    "                status = \"READY\"\n",
    "            \n",
    "            step_analysis = {\n",
    "                \"name\": step.name,\n",
    "                \"enabled\": enabled,\n",
    "                \"status\": status,\n",
    "                \"dependencies_met\": deps_met,\n",
    "                \"missing_dependencies\": missing_deps,\n",
    "                \"inputs\": input_status,\n",
    "                \"outputs\": output_status,\n",
    "                \"input_summary\": {\n",
    "                    \"total\": len(input_status),\n",
    "                    \"ready\": sum(1 for s in input_status.values() if s[\"exists\"]),\n",
    "                    \"missing\": sum(1 for s in input_status.values() if not s[\"exists\"])\n",
    "                },\n",
    "                \"output_summary\": {\n",
    "                    \"total\": len(output_status),\n",
    "                    \"ready\": sum(1 for s in output_status.values() if s[\"exists\"]),\n",
    "                    \"missing\": sum(1 for s in output_status.values() if not s[\"exists\"])\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            analysis[\"steps\"][step_name] = step_analysis\n",
    "            \n",
    "            # Log summary\n",
    "            self.logger.info(f\"Status: {status}\")\n",
    "            self.logger.info(f\"Enabled: {enabled}\")\n",
    "            self.logger.info(f\"Dependencies met: {deps_met}\")\n",
    "            self.logger.info(f\"Inputs ready: {step_analysis['input_summary']['ready']}/{step_analysis['input_summary']['total']}\")\n",
    "            self.logger.info(f\"Outputs ready: {step_analysis['output_summary']['ready']}/{step_analysis['output_summary']['total']}\")\n",
    "            \n",
    "            if missing_deps:\n",
    "                self.logger.warning(f\"Missing dependencies: {missing_deps}\")\n",
    "            \n",
    "            # Log missing inputs\n",
    "            for input_name, status in input_status.items():\n",
    "                if not status[\"exists\"]:\n",
    "                    self.logger.warning(f\"Missing input: {input_name}\")\n",
    "            \n",
    "            # Log generated outputs\n",
    "            for output_name, status in output_status.items():\n",
    "                if status[\"exists\"]:\n",
    "                    self.logger.info(f\"Generated output: {output_name} ({status['count']} files)\")\n",
    "        \n",
    "        return analysis\n",
    "    \n",
    "    def generate_workflow_report(self, analysis: Dict[str, Any]) -> str:\n",
    "        \"\"\"Generate a detailed HTML report of the workflow status\"\"\"\n",
    "        html = f\"\"\"\n",
    "        <!DOCTYPE html>\n",
    "        <html>\n",
    "        <head>\n",
    "            <title>E+ Workflow Analysis Report</title>\n",
    "            <style>\n",
    "                body {{ font-family: Arial, sans-serif; margin: 20px; }}\n",
    "                h1, h2, h3 {{ color: #333; }}\n",
    "                .metadata {{ background: #f0f0f0; padding: 10px; border-radius: 5px; margin-bottom: 20px; }}\n",
    "                .step {{ border: 1px solid #ddd; margin: 10px 0; padding: 15px; border-radius: 5px; }}\n",
    "                .status-COMPLETED {{ background: #d4f4dd; }}\n",
    "                .status-READY {{ background: #fff3cd; }}\n",
    "                .status-BLOCKED {{ background: #f8d7da; }}\n",
    "                .status-DISABLED {{ background: #e7e7e7; }}\n",
    "                .status-MISSING_INPUTS {{ background: #ffeaa7; }}\n",
    "                .summary {{ display: flex; gap: 20px; margin: 10px 0; }}\n",
    "                .summary-box {{ padding: 10px; background: #f9f9f9; border-radius: 3px; }}\n",
    "                .file-list {{ font-size: 0.9em; color: #666; margin-left: 20px; }}\n",
    "                .missing {{ color: #d32f2f; }}\n",
    "                .exists {{ color: #388e3c; }}\n",
    "                table {{ border-collapse: collapse; width: 100%; margin: 10px 0; }}\n",
    "                th, td {{ border: 1px solid #ddd; padding: 8px; text-align: left; }}\n",
    "                th {{ background: #f2f2f2; }}\n",
    "                .dependency-graph {{ margin: 20px 0; padding: 20px; background: #fafafa; border-radius: 5px; }}\n",
    "            </style>\n",
    "        </head>\n",
    "        <body>\n",
    "            <h1>E+ Workflow Analysis Report</h1>\n",
    "            \n",
    "            <div class=\"metadata\">\n",
    "                <strong>Job ID:</strong> {analysis['job_id']}<br>\n",
    "                <strong>Output Directory:</strong> {analysis['job_output_dir']}<br>\n",
    "                <strong>Analysis Time:</strong> {analysis['timestamp']}\n",
    "            </div>\n",
    "            \n",
    "            <h2>Workflow Summary</h2>\n",
    "            <table>\n",
    "                <tr>\n",
    "                    <th>Step</th>\n",
    "                    <th>Status</th>\n",
    "                    <th>Enabled</th>\n",
    "                    <th>Dependencies Met</th>\n",
    "                    <th>Inputs Ready</th>\n",
    "                    <th>Outputs Generated</th>\n",
    "                </tr>\n",
    "        \"\"\"\n",
    "        \n",
    "        for step_name, step_data in analysis['steps'].items():\n",
    "            html += f\"\"\"\n",
    "                <tr class=\"status-{step_data['status']}\">\n",
    "                    <td>{step_data['name']}</td>\n",
    "                    <td><strong>{step_data['status']}</strong></td>\n",
    "                    <td>{'✓' if step_data['enabled'] else '✗'}</td>\n",
    "                    <td>{'✓' if step_data['dependencies_met'] else '✗'}</td>\n",
    "                    <td>{step_data['input_summary']['ready']}/{step_data['input_summary']['total']}</td>\n",
    "                    <td>{step_data['output_summary']['ready']}/{step_data['output_summary']['total']}</td>\n",
    "                </tr>\n",
    "            \"\"\"\n",
    "        \n",
    "        html += \"\"\"\n",
    "            </table>\n",
    "            \n",
    "            <h2>Detailed Step Analysis</h2>\n",
    "        \"\"\"\n",
    "        \n",
    "        for step_name, step_data in analysis['steps'].items():\n",
    "            html += f\"\"\"\n",
    "            <div class=\"step status-{step_data['status']}\">\n",
    "                <h3>{step_data['name']}</h3>\n",
    "                <div class=\"summary\">\n",
    "                    <div class=\"summary-box\">\n",
    "                        <strong>Status:</strong> {step_data['status']}\n",
    "                    </div>\n",
    "                    <div class=\"summary-box\">\n",
    "                        <strong>Enabled:</strong> {'Yes' if step_data['enabled'] else 'No'}\n",
    "                    </div>\n",
    "                    <div class=\"summary-box\">\n",
    "                        <strong>Dependencies:</strong> {'Met' if step_data['dependencies_met'] else 'Not Met'}\n",
    "                    </div>\n",
    "                </div>\n",
    "            \"\"\"\n",
    "            \n",
    "            if step_data['missing_dependencies']:\n",
    "                html += f\"\"\"\n",
    "                <div class=\"missing\">\n",
    "                    <strong>Missing Dependencies:</strong>\n",
    "                    <ul>\n",
    "                        {''.join(f'<li>{dep}</li>' for dep in step_data['missing_dependencies'])}\n",
    "                    </ul>\n",
    "                </div>\n",
    "                \"\"\"\n",
    "            \n",
    "            # Inputs section\n",
    "            html += \"<h4>Inputs:</h4><ul>\"\n",
    "            for input_name, status in step_data['inputs'].items():\n",
    "                class_name = \"exists\" if status['exists'] else \"missing\"\n",
    "                html += f\"\"\"\n",
    "                <li class=\"{class_name}\">\n",
    "                    {input_name}: {'✓' if status['exists'] else '✗'} ({status['count']} files)\n",
    "                \"\"\"\n",
    "                if status['exists'] and status['count'] <= 5:\n",
    "                    html += \"<div class='file-list'>\"\n",
    "                    for file in status['files']:\n",
    "                        html += f\"{os.path.basename(file)}<br>\"\n",
    "                    html += \"</div>\"\n",
    "                html += \"</li>\"\n",
    "            html += \"</ul>\"\n",
    "            \n",
    "            # Outputs section\n",
    "            html += \"<h4>Outputs:</h4><ul>\"\n",
    "            for output_name, status in step_data['outputs'].items():\n",
    "                class_name = \"exists\" if status['exists'] else \"missing\"\n",
    "                html += f\"\"\"\n",
    "                <li class=\"{class_name}\">\n",
    "                    {output_name}: {'✓' if status['exists'] else '✗'} ({status['count']} files)\n",
    "                \"\"\"\n",
    "                if status['exists'] and status['count'] <= 5:\n",
    "                    html += \"<div class='file-list'>\"\n",
    "                    for file in status['files']:\n",
    "                        html += f\"{os.path.basename(file)}<br>\"\n",
    "                    html += \"</div>\"\n",
    "                html += \"</li>\"\n",
    "            html += \"</ul>\"\n",
    "            \n",
    "            html += \"</div>\"\n",
    "        \n",
    "        html += \"\"\"\n",
    "            <h2>Workflow Dependency Graph</h2>\n",
    "            <div class=\"dependency-graph\">\n",
    "                <pre>\n",
    "IDF Creation\n",
    "    ↓\n",
    "Simulation\n",
    "    ↓\n",
    "Parsing ─────────────┐\n",
    "    ↓                 ↓\n",
    "Modification     Validation\n",
    "    ↓                 ↓\n",
    "Modified Simulation   ↓\n",
    "    ↓                 ↓\n",
    "Modified Parsing ─────┤\n",
    "    ↓                 ↓\n",
    "Sensitivity Analysis  ↓\n",
    "    ↓                 ↓\n",
    "Surrogate Modeling ───┤\n",
    "    ↓                 ↓\n",
    "Calibration ←─────────┘\n",
    "                </pre>\n",
    "            </div>\n",
    "            \n",
    "        </body>\n",
    "        </html>\n",
    "        \"\"\"\n",
    "        \n",
    "        return html\n",
    "    \n",
    "    def validate_data_quality(self) -> Dict[str, Any]:\n",
    "        \"\"\"Validate the quality of data at each step\"\"\"\n",
    "        quality_report = {\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"checks\": {}\n",
    "        }\n",
    "        \n",
    "        # Check parsed data quality\n",
    "        parsed_data_dir = self.job_output_dir / \"parsed_data\"\n",
    "        if parsed_data_dir.exists():\n",
    "            quality_report[\"checks\"][\"parsed_data\"] = self._check_parquet_files(parsed_data_dir)\n",
    "        \n",
    "        # Check modified results quality\n",
    "        modified_data_dir = self.job_output_dir / \"parsed_modified_results\"\n",
    "        if modified_data_dir.exists():\n",
    "            quality_report[\"checks\"][\"modified_data\"] = self._check_parquet_files(modified_data_dir)\n",
    "        \n",
    "        # Check sensitivity results\n",
    "        sens_dir = self.job_output_dir / \"sensitivity_results\"\n",
    "        if sens_dir.exists():\n",
    "            sens_files = list(sens_dir.glob(\"*.parquet\"))\n",
    "            if sens_files:\n",
    "                quality_report[\"checks\"][\"sensitivity\"] = {\n",
    "                    \"file_count\": len(sens_files),\n",
    "                    \"files\": [f.name for f in sens_files]\n",
    "                }\n",
    "        \n",
    "        return quality_report\n",
    "    \n",
    "    def _check_parquet_files(self, directory: Path) -> Dict[str, Any]:\n",
    "        \"\"\"Check quality of parquet files in a directory\"\"\"\n",
    "        report = {\n",
    "            \"total_files\": 0,\n",
    "            \"total_rows\": 0,\n",
    "            \"files\": {}\n",
    "        }\n",
    "        \n",
    "        for parquet_file in directory.rglob(\"*.parquet\"):\n",
    "            try:\n",
    "                df = pd.read_parquet(parquet_file)\n",
    "                report[\"files\"][str(parquet_file.relative_to(directory))] = {\n",
    "                    \"rows\": len(df),\n",
    "                    \"columns\": list(df.columns),\n",
    "                    \"missing_values\": df.isnull().sum().to_dict()\n",
    "                }\n",
    "                report[\"total_files\"] += 1\n",
    "                report[\"total_rows\"] += len(df)\n",
    "            except Exception as e:\n",
    "                report[\"files\"][str(parquet_file.relative_to(directory))] = {\n",
    "                    \"error\": str(e)\n",
    "                }\n",
    "        \n",
    "        return report\n",
    "    \n",
    "    def run_full_analysis(self):\n",
    "        \"\"\"Run complete workflow analysis and generate reports\"\"\"\n",
    "        self.logger.info(\"=\"*80)\n",
    "        self.logger.info(\"E+ WORKFLOW DEPENDENCY TRACKER AND VALIDATOR\")\n",
    "        self.logger.info(\"=\"*80)\n",
    "        \n",
    "        # Run workflow analysis\n",
    "        analysis = self.analyze_workflow()\n",
    "        \n",
    "        # Save analysis as JSON\n",
    "        analysis_file = self.job_output_dir / \"workflow_analysis.json\"\n",
    "        with open(analysis_file, 'w') as f:\n",
    "            json.dump(analysis, f, indent=2)\n",
    "        self.logger.info(f\"\\nSaved analysis to: {analysis_file}\")\n",
    "        \n",
    "        # Generate HTML report\n",
    "        html_report = self.generate_workflow_report(analysis)\n",
    "        report_file = self.job_output_dir / \"workflow_analysis_report.html\"\n",
    "        with open(report_file, 'w') as f:\n",
    "            f.write(html_report)\n",
    "        self.logger.info(f\"Generated HTML report: {report_file}\")\n",
    "        \n",
    "        # Run data quality checks\n",
    "        quality_report = self.validate_data_quality()\n",
    "        quality_file = self.job_output_dir / \"data_quality_report.json\"\n",
    "        with open(quality_file, 'w') as f:\n",
    "            json.dump(quality_report, f, indent=2)\n",
    "        self.logger.info(f\"Generated data quality report: {quality_file}\")\n",
    "        \n",
    "        # Print summary\n",
    "        self.logger.info(\"\\n\" + \"=\"*80)\n",
    "        self.logger.info(\"WORKFLOW SUMMARY\")\n",
    "        self.logger.info(\"=\"*80)\n",
    "        \n",
    "        status_counts = {}\n",
    "        for step_data in analysis['steps'].values():\n",
    "            status = step_data['status']\n",
    "            status_counts[status] = status_counts.get(status, 0) + 1\n",
    "        \n",
    "        for status, count in status_counts.items():\n",
    "            self.logger.info(f\"{status}: {count} steps\")\n",
    "        \n",
    "        # Identify next actionable steps\n",
    "        self.logger.info(\"\\n\" + \"=\"*80)\n",
    "        self.logger.info(\"NEXT ACTIONABLE STEPS\")\n",
    "        self.logger.info(\"=\"*80)\n",
    "        \n",
    "        for step_name, step_data in analysis['steps'].items():\n",
    "            if step_data['status'] == 'READY':\n",
    "                self.logger.info(f\"- {step_data['name']} is ready to run\")\n",
    "            elif step_data['status'] == 'MISSING_INPUTS':\n",
    "                self.logger.info(f\"- {step_data['name']} is missing inputs:\")\n",
    "                for input_name, status in step_data['inputs'].items():\n",
    "                    if not status['exists']:\n",
    "                        self.logger.info(f\"  * {input_name}\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main entry point for the workflow tracker\"\"\"\n",
    "    import sys\n",
    "    \n",
    "    if len(sys.argv) < 2:\n",
    "        print(\"Usage: python workflow_tracker.py <job_output_directory>\")\n",
    "        print(\"\\nExample paths from your last run:\")\n",
    "        print(\"D:\\Documents\\daily\\E_Plus_2040_py\\output\\5f3924b1-189c-4c95-bf33-52d0602d79d9\")\n",
    "        sys.exit(1)\n",
    "    \n",
    "    job_output_dir = sys.argv[1]\n",
    "    \n",
    "    if not os.path.exists(job_output_dir):\n",
    "        print(f\"Error: Directory not found: {job_output_dir}\")\n",
    "        sys.exit(1)\n",
    "    \n",
    "    # Create and run tracker\n",
    "    tracker = WorkflowTracker(job_output_dir)\n",
    "    tracker.run_full_analysis()\n",
    "    \n",
    "    print(f\"\\nAnalysis complete! Check the following files in {job_output_dir}:\")\n",
    "    print(\"  - workflow_analysis.json: Detailed analysis data\")\n",
    "    print(\"  - workflow_analysis_report.html: Visual HTML report\")\n",
    "    print(\"  - data_quality_report.json: Data quality checks\")\n",
    "    print(\"  - workflow_tracker.log: Detailed execution log\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd4f9b2",
   "metadata": {},
   "source": [
    "## Surrogate Checker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae8626b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking surrogate data structure in: D:\\Documents\\daily\\E_Plus_2040_py\\output\\5f3924b1-189c-4c95-bf33-52d0602d79d9\n",
      "\n",
      "Checking: parsed_data/idf_data/by_category/lighting.parquet\n",
      "Checking: parsed_data/idf_data/by_category/hvac_equipment.parquet\n",
      "Checking: parsed_data/idf_data/by_category/materials_materials.parquet\n",
      "Checking: parsed_data/idf_data/by_category/infiltration.parquet\n",
      "Checking: parsed_data/idf_data/by_category/ventilation.parquet\n",
      "Checking: parsed_data/sql_results/timeseries/aggregated/daily/zones_daily.parquet\n",
      "Checking: parsed_data/sql_results/timeseries/aggregated/daily/hvac_daily.parquet\n",
      "Checking: parsed_data/sql_results/timeseries/aggregated/daily/ventilation_daily.parquet\n",
      "Checking: parsed_data/relationships/zone_mappings.parquet\n",
      "Checking: parsed_data/metadata/building_registry.parquet\n",
      "Checking: parsed_modified_results/sql_results/timeseries/aggregated/daily/zones_daily.parquet\n",
      "Checking: modified_idfs/modifications_detail_*.parquet\n",
      "Checking: sensitivity_results/sensitivity_for_surrogate.parquet\n",
      "\n",
      "================================================================================\n",
      "SUMMARY\n",
      "================================================================================\n",
      "Check performed at: 2025-06-26T22:18:55.067428\n",
      "Total files expected: 13\n",
      "Files found: 12\n",
      "Files missing: 1\n",
      "Expected columns total: 68\n",
      "Expected columns found: 43\n",
      "Extra columns discovered: 169\n",
      "Total issues: 26\n",
      "\n",
      "MISSING FILES: 1\n",
      "  - sensitivity_results/sensitivity_for_surrogate.parquet\n",
      "\n",
      "DISCOVERED EXTRA COLUMNS BY FILE:\n",
      "  - building_registry.parquet: 5 extra columns\n",
      "  - hvac_daily.parquet: 3 extra columns\n",
      "  - hvac_equipment.parquet: 38 extra columns\n",
      "  - infiltration.parquet: 16 extra columns\n",
      "  - lighting.parquet: 22 extra columns\n",
      "  - materials_materials.parquet: 21 extra columns\n",
      "  - modifications_detail_*.parquet: 2 extra columns\n",
      "  - ventilation.parquet: 51 extra columns\n",
      "  - ventilation_daily.parquet: 3 extra columns\n",
      "  - zone_mappings.parquet: 4 extra columns\n",
      "\n",
      "================================================================================\n",
      "DETAILED RESULTS (First 50 rows)\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Drives\\Temp\\ipykernel_2700\\506503610.py:522: FutureWarning: Styler.applymap has been deprecated. Use Styler.map instead.\n",
      "  styled_df = df_results.style.applymap(style_status, subset=['Status'])\\\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_1bd03_row0_col2, #T_1bd03_row0_col8, #T_1bd03_row0_col9, #T_1bd03_row1_col2, #T_1bd03_row1_col8, #T_1bd03_row1_col9, #T_1bd03_row2_col2, #T_1bd03_row2_col8, #T_1bd03_row2_col9, #T_1bd03_row3_col2, #T_1bd03_row4_col2, #T_1bd03_row5_col2, #T_1bd03_row6_col2, #T_1bd03_row7_col2, #T_1bd03_row7_col8, #T_1bd03_row8_col2, #T_1bd03_row8_col8, #T_1bd03_row9_col2, #T_1bd03_row9_col8, #T_1bd03_row10_col2, #T_1bd03_row10_col8, #T_1bd03_row11_col2, #T_1bd03_row11_col8, #T_1bd03_row12_col2, #T_1bd03_row12_col8, #T_1bd03_row13_col2, #T_1bd03_row13_col8, #T_1bd03_row14_col2, #T_1bd03_row14_col8, #T_1bd03_row15_col2, #T_1bd03_row15_col8, #T_1bd03_row16_col2, #T_1bd03_row16_col8, #T_1bd03_row17_col2, #T_1bd03_row17_col8, #T_1bd03_row18_col2, #T_1bd03_row18_col8, #T_1bd03_row19_col2, #T_1bd03_row19_col8, #T_1bd03_row20_col2, #T_1bd03_row20_col8, #T_1bd03_row21_col2, #T_1bd03_row21_col8, #T_1bd03_row22_col2, #T_1bd03_row22_col8, #T_1bd03_row23_col2, #T_1bd03_row23_col8, #T_1bd03_row24_col2, #T_1bd03_row24_col8, #T_1bd03_row25_col2, #T_1bd03_row25_col8, #T_1bd03_row26_col2, #T_1bd03_row26_col8, #T_1bd03_row27_col2, #T_1bd03_row27_col8, #T_1bd03_row28_col2, #T_1bd03_row28_col8, #T_1bd03_row29_col2, #T_1bd03_row29_col8, #T_1bd03_row29_col9, #T_1bd03_row30_col2, #T_1bd03_row30_col8, #T_1bd03_row30_col9, #T_1bd03_row31_col2, #T_1bd03_row31_col8, #T_1bd03_row31_col9, #T_1bd03_row32_col2, #T_1bd03_row32_col8, #T_1bd03_row32_col9, #T_1bd03_row33_col2, #T_1bd03_row34_col2, #T_1bd03_row35_col2, #T_1bd03_row35_col8, #T_1bd03_row36_col2, #T_1bd03_row36_col8, #T_1bd03_row37_col2, #T_1bd03_row37_col8, #T_1bd03_row38_col2, #T_1bd03_row38_col8, #T_1bd03_row39_col2, #T_1bd03_row39_col8, #T_1bd03_row40_col2, #T_1bd03_row40_col8, #T_1bd03_row41_col2, #T_1bd03_row41_col8, #T_1bd03_row42_col2, #T_1bd03_row42_col8, #T_1bd03_row43_col2, #T_1bd03_row43_col8, #T_1bd03_row44_col2, #T_1bd03_row44_col8, #T_1bd03_row45_col2, #T_1bd03_row45_col8, #T_1bd03_row46_col2, #T_1bd03_row46_col8, #T_1bd03_row47_col2, #T_1bd03_row47_col8, #T_1bd03_row48_col2, #T_1bd03_row48_col8, #T_1bd03_row49_col2, #T_1bd03_row49_col8 {\n",
       "  color: green;\n",
       "  font-weight: bold;\n",
       "}\n",
       "#T_1bd03_row0_col13, #T_1bd03_row1_col13, #T_1bd03_row2_col13, #T_1bd03_row29_col13, #T_1bd03_row30_col13, #T_1bd03_row31_col13, #T_1bd03_row32_col13 {\n",
       "  background-color: #90EE90;\n",
       "}\n",
       "#T_1bd03_row3_col8, #T_1bd03_row3_col9, #T_1bd03_row4_col8, #T_1bd03_row4_col9, #T_1bd03_row5_col8, #T_1bd03_row5_col9, #T_1bd03_row6_col8, #T_1bd03_row6_col9, #T_1bd03_row33_col8, #T_1bd03_row33_col9, #T_1bd03_row34_col8, #T_1bd03_row34_col9 {\n",
       "  color: red;\n",
       "  font-weight: bold;\n",
       "}\n",
       "#T_1bd03_row3_col13, #T_1bd03_row4_col13, #T_1bd03_row5_col13, #T_1bd03_row6_col13, #T_1bd03_row33_col13, #T_1bd03_row34_col13 {\n",
       "  background-color: #FFB6C1;\n",
       "}\n",
       "#T_1bd03_row7_col5, #T_1bd03_row8_col5, #T_1bd03_row9_col5, #T_1bd03_row10_col5, #T_1bd03_row11_col5, #T_1bd03_row12_col5, #T_1bd03_row13_col5, #T_1bd03_row14_col5, #T_1bd03_row15_col5, #T_1bd03_row16_col5, #T_1bd03_row17_col5, #T_1bd03_row18_col5, #T_1bd03_row19_col5, #T_1bd03_row20_col5, #T_1bd03_row21_col5, #T_1bd03_row22_col5, #T_1bd03_row23_col5, #T_1bd03_row24_col5, #T_1bd03_row25_col5, #T_1bd03_row26_col5, #T_1bd03_row27_col5, #T_1bd03_row28_col5, #T_1bd03_row35_col5, #T_1bd03_row36_col5, #T_1bd03_row37_col5, #T_1bd03_row38_col5, #T_1bd03_row39_col5, #T_1bd03_row40_col5, #T_1bd03_row41_col5, #T_1bd03_row42_col5, #T_1bd03_row43_col5, #T_1bd03_row44_col5, #T_1bd03_row45_col5, #T_1bd03_row46_col5, #T_1bd03_row47_col5, #T_1bd03_row48_col5, #T_1bd03_row49_col5 {\n",
       "  color: blue;\n",
       "  font-weight: bold;\n",
       "}\n",
       "#T_1bd03_row7_col13, #T_1bd03_row8_col13, #T_1bd03_row9_col13, #T_1bd03_row10_col13, #T_1bd03_row11_col13, #T_1bd03_row12_col13, #T_1bd03_row13_col13, #T_1bd03_row14_col13, #T_1bd03_row15_col13, #T_1bd03_row16_col13, #T_1bd03_row17_col13, #T_1bd03_row18_col13, #T_1bd03_row19_col13, #T_1bd03_row20_col13, #T_1bd03_row21_col13, #T_1bd03_row22_col13, #T_1bd03_row23_col13, #T_1bd03_row24_col13, #T_1bd03_row25_col13, #T_1bd03_row26_col13, #T_1bd03_row27_col13, #T_1bd03_row28_col13, #T_1bd03_row35_col13, #T_1bd03_row36_col13, #T_1bd03_row37_col13, #T_1bd03_row38_col13, #T_1bd03_row39_col13, #T_1bd03_row40_col13, #T_1bd03_row41_col13, #T_1bd03_row42_col13, #T_1bd03_row43_col13, #T_1bd03_row44_col13, #T_1bd03_row45_col13, #T_1bd03_row46_col13, #T_1bd03_row47_col13, #T_1bd03_row48_col13, #T_1bd03_row49_col13 {\n",
       "  background-color: #87CEEB;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_1bd03\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_1bd03_level0_col0\" class=\"col_heading level0 col0\" >File</th>\n",
       "      <th id=\"T_1bd03_level0_col1\" class=\"col_heading level0 col1\" >Path</th>\n",
       "      <th id=\"T_1bd03_level0_col2\" class=\"col_heading level0 col2\" >File_Exists</th>\n",
       "      <th id=\"T_1bd03_level0_col3\" class=\"col_heading level0 col3\" >Rows</th>\n",
       "      <th id=\"T_1bd03_level0_col4\" class=\"col_heading level0 col4\" >Column</th>\n",
       "      <th id=\"T_1bd03_level0_col5\" class=\"col_heading level0 col5\" >Column_Type</th>\n",
       "      <th id=\"T_1bd03_level0_col6\" class=\"col_heading level0 col6\" >Expected_Type</th>\n",
       "      <th id=\"T_1bd03_level0_col7\" class=\"col_heading level0 col7\" >Actual_Type</th>\n",
       "      <th id=\"T_1bd03_level0_col8\" class=\"col_heading level0 col8\" >Column_Exists</th>\n",
       "      <th id=\"T_1bd03_level0_col9\" class=\"col_heading level0 col9\" >Type_Match</th>\n",
       "      <th id=\"T_1bd03_level0_col10\" class=\"col_heading level0 col10\" >Null_Count</th>\n",
       "      <th id=\"T_1bd03_level0_col11\" class=\"col_heading level0 col11\" >Unique_Values</th>\n",
       "      <th id=\"T_1bd03_level0_col12\" class=\"col_heading level0 col12\" >Sample_Values</th>\n",
       "      <th id=\"T_1bd03_level0_col13\" class=\"col_heading level0 col13\" >Status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_1bd03_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_1bd03_row0_col0\" class=\"data row0 col0\" >lighting.parquet</td>\n",
       "      <td id=\"T_1bd03_row0_col1\" class=\"data row0 col1\" >parsed_data/idf_data/by_category/lighting.parquet</td>\n",
       "      <td id=\"T_1bd03_row0_col2\" class=\"data row0 col2\" >✓</td>\n",
       "      <td id=\"T_1bd03_row0_col3\" class=\"data row0 col3\" >3</td>\n",
       "      <td id=\"T_1bd03_row0_col4\" class=\"data row0 col4\" >building_id</td>\n",
       "      <td id=\"T_1bd03_row0_col5\" class=\"data row0 col5\" >Expected</td>\n",
       "      <td id=\"T_1bd03_row0_col6\" class=\"data row0 col6\" >str</td>\n",
       "      <td id=\"T_1bd03_row0_col7\" class=\"data row0 col7\" >str</td>\n",
       "      <td id=\"T_1bd03_row0_col8\" class=\"data row0 col8\" >✓</td>\n",
       "      <td id=\"T_1bd03_row0_col9\" class=\"data row0 col9\" >✓</td>\n",
       "      <td id=\"T_1bd03_row0_col10\" class=\"data row0 col10\" >0.000000</td>\n",
       "      <td id=\"T_1bd03_row0_col11\" class=\"data row0 col11\" >3.000000</td>\n",
       "      <td id=\"T_1bd03_row0_col12\" class=\"data row0 col12\" >['4136733', '4136737', '4136738']</td>\n",
       "      <td id=\"T_1bd03_row0_col13\" class=\"data row0 col13\" >OK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1bd03_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_1bd03_row1_col0\" class=\"data row1 col0\" >lighting.parquet</td>\n",
       "      <td id=\"T_1bd03_row1_col1\" class=\"data row1 col1\" >parsed_data/idf_data/by_category/lighting.parquet</td>\n",
       "      <td id=\"T_1bd03_row1_col2\" class=\"data row1 col2\" >✓</td>\n",
       "      <td id=\"T_1bd03_row1_col3\" class=\"data row1 col3\" >3</td>\n",
       "      <td id=\"T_1bd03_row1_col4\" class=\"data row1 col4\" >zone_name</td>\n",
       "      <td id=\"T_1bd03_row1_col5\" class=\"data row1 col5\" >Expected</td>\n",
       "      <td id=\"T_1bd03_row1_col6\" class=\"data row1 col6\" >str</td>\n",
       "      <td id=\"T_1bd03_row1_col7\" class=\"data row1 col7\" >str</td>\n",
       "      <td id=\"T_1bd03_row1_col8\" class=\"data row1 col8\" >✓</td>\n",
       "      <td id=\"T_1bd03_row1_col9\" class=\"data row1 col9\" >✓</td>\n",
       "      <td id=\"T_1bd03_row1_col10\" class=\"data row1 col10\" >0.000000</td>\n",
       "      <td id=\"T_1bd03_row1_col11\" class=\"data row1 col11\" >1.000000</td>\n",
       "      <td id=\"T_1bd03_row1_col12\" class=\"data row1 col12\" >['ALL_ZONES']</td>\n",
       "      <td id=\"T_1bd03_row1_col13\" class=\"data row1 col13\" >OK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1bd03_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_1bd03_row2_col0\" class=\"data row2 col0\" >lighting.parquet</td>\n",
       "      <td id=\"T_1bd03_row2_col1\" class=\"data row2 col1\" >parsed_data/idf_data/by_category/lighting.parquet</td>\n",
       "      <td id=\"T_1bd03_row2_col2\" class=\"data row2 col2\" >✓</td>\n",
       "      <td id=\"T_1bd03_row2_col3\" class=\"data row2 col3\" >3</td>\n",
       "      <td id=\"T_1bd03_row2_col4\" class=\"data row2 col4\" >object_name</td>\n",
       "      <td id=\"T_1bd03_row2_col5\" class=\"data row2 col5\" >Expected</td>\n",
       "      <td id=\"T_1bd03_row2_col6\" class=\"data row2 col6\" >str</td>\n",
       "      <td id=\"T_1bd03_row2_col7\" class=\"data row2 col7\" >str</td>\n",
       "      <td id=\"T_1bd03_row2_col8\" class=\"data row2 col8\" >✓</td>\n",
       "      <td id=\"T_1bd03_row2_col9\" class=\"data row2 col9\" >✓</td>\n",
       "      <td id=\"T_1bd03_row2_col10\" class=\"data row2 col10\" >0.000000</td>\n",
       "      <td id=\"T_1bd03_row2_col11\" class=\"data row2 col11\" >1.000000</td>\n",
       "      <td id=\"T_1bd03_row2_col12\" class=\"data row2 col12\" >['Lights_ALL_ZONES']</td>\n",
       "      <td id=\"T_1bd03_row2_col13\" class=\"data row2 col13\" >OK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1bd03_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_1bd03_row3_col0\" class=\"data row3 col0\" >lighting.parquet</td>\n",
       "      <td id=\"T_1bd03_row3_col1\" class=\"data row3 col1\" >parsed_data/idf_data/by_category/lighting.parquet</td>\n",
       "      <td id=\"T_1bd03_row3_col2\" class=\"data row3 col2\" >✓</td>\n",
       "      <td id=\"T_1bd03_row3_col3\" class=\"data row3 col3\" >3</td>\n",
       "      <td id=\"T_1bd03_row3_col4\" class=\"data row3 col4\" >LightingLevel</td>\n",
       "      <td id=\"T_1bd03_row3_col5\" class=\"data row3 col5\" >Expected</td>\n",
       "      <td id=\"T_1bd03_row3_col6\" class=\"data row3 col6\" >float</td>\n",
       "      <td id=\"T_1bd03_row3_col7\" class=\"data row3 col7\" >None</td>\n",
       "      <td id=\"T_1bd03_row3_col8\" class=\"data row3 col8\" >✗</td>\n",
       "      <td id=\"T_1bd03_row3_col9\" class=\"data row3 col9\" >✗</td>\n",
       "      <td id=\"T_1bd03_row3_col10\" class=\"data row3 col10\" >nan</td>\n",
       "      <td id=\"T_1bd03_row3_col11\" class=\"data row3 col11\" >nan</td>\n",
       "      <td id=\"T_1bd03_row3_col12\" class=\"data row3 col12\" >N/A</td>\n",
       "      <td id=\"T_1bd03_row3_col13\" class=\"data row3 col13\" >ISSUE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1bd03_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_1bd03_row4_col0\" class=\"data row4 col0\" >lighting.parquet</td>\n",
       "      <td id=\"T_1bd03_row4_col1\" class=\"data row4 col1\" >parsed_data/idf_data/by_category/lighting.parquet</td>\n",
       "      <td id=\"T_1bd03_row4_col2\" class=\"data row4 col2\" >✓</td>\n",
       "      <td id=\"T_1bd03_row4_col3\" class=\"data row4 col3\" >3</td>\n",
       "      <td id=\"T_1bd03_row4_col4\" class=\"data row4 col4\" >FractionRadiant</td>\n",
       "      <td id=\"T_1bd03_row4_col5\" class=\"data row4 col5\" >Expected</td>\n",
       "      <td id=\"T_1bd03_row4_col6\" class=\"data row4 col6\" >float</td>\n",
       "      <td id=\"T_1bd03_row4_col7\" class=\"data row4 col7\" >None</td>\n",
       "      <td id=\"T_1bd03_row4_col8\" class=\"data row4 col8\" >✗</td>\n",
       "      <td id=\"T_1bd03_row4_col9\" class=\"data row4 col9\" >✗</td>\n",
       "      <td id=\"T_1bd03_row4_col10\" class=\"data row4 col10\" >nan</td>\n",
       "      <td id=\"T_1bd03_row4_col11\" class=\"data row4 col11\" >nan</td>\n",
       "      <td id=\"T_1bd03_row4_col12\" class=\"data row4 col12\" >N/A</td>\n",
       "      <td id=\"T_1bd03_row4_col13\" class=\"data row4 col13\" >ISSUE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1bd03_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "      <td id=\"T_1bd03_row5_col0\" class=\"data row5 col0\" >lighting.parquet</td>\n",
       "      <td id=\"T_1bd03_row5_col1\" class=\"data row5 col1\" >parsed_data/idf_data/by_category/lighting.parquet</td>\n",
       "      <td id=\"T_1bd03_row5_col2\" class=\"data row5 col2\" >✓</td>\n",
       "      <td id=\"T_1bd03_row5_col3\" class=\"data row5 col3\" >3</td>\n",
       "      <td id=\"T_1bd03_row5_col4\" class=\"data row5 col4\" >FractionVisible</td>\n",
       "      <td id=\"T_1bd03_row5_col5\" class=\"data row5 col5\" >Expected</td>\n",
       "      <td id=\"T_1bd03_row5_col6\" class=\"data row5 col6\" >float</td>\n",
       "      <td id=\"T_1bd03_row5_col7\" class=\"data row5 col7\" >None</td>\n",
       "      <td id=\"T_1bd03_row5_col8\" class=\"data row5 col8\" >✗</td>\n",
       "      <td id=\"T_1bd03_row5_col9\" class=\"data row5 col9\" >✗</td>\n",
       "      <td id=\"T_1bd03_row5_col10\" class=\"data row5 col10\" >nan</td>\n",
       "      <td id=\"T_1bd03_row5_col11\" class=\"data row5 col11\" >nan</td>\n",
       "      <td id=\"T_1bd03_row5_col12\" class=\"data row5 col12\" >N/A</td>\n",
       "      <td id=\"T_1bd03_row5_col13\" class=\"data row5 col13\" >ISSUE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1bd03_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
       "      <td id=\"T_1bd03_row6_col0\" class=\"data row6 col0\" >lighting.parquet</td>\n",
       "      <td id=\"T_1bd03_row6_col1\" class=\"data row6 col1\" >parsed_data/idf_data/by_category/lighting.parquet</td>\n",
       "      <td id=\"T_1bd03_row6_col2\" class=\"data row6 col2\" >✓</td>\n",
       "      <td id=\"T_1bd03_row6_col3\" class=\"data row6 col3\" >3</td>\n",
       "      <td id=\"T_1bd03_row6_col4\" class=\"data row6 col4\" >ScheduleName</td>\n",
       "      <td id=\"T_1bd03_row6_col5\" class=\"data row6 col5\" >Expected</td>\n",
       "      <td id=\"T_1bd03_row6_col6\" class=\"data row6 col6\" >str</td>\n",
       "      <td id=\"T_1bd03_row6_col7\" class=\"data row6 col7\" >None</td>\n",
       "      <td id=\"T_1bd03_row6_col8\" class=\"data row6 col8\" >✗</td>\n",
       "      <td id=\"T_1bd03_row6_col9\" class=\"data row6 col9\" >✗</td>\n",
       "      <td id=\"T_1bd03_row6_col10\" class=\"data row6 col10\" >nan</td>\n",
       "      <td id=\"T_1bd03_row6_col11\" class=\"data row6 col11\" >nan</td>\n",
       "      <td id=\"T_1bd03_row6_col12\" class=\"data row6 col12\" >N/A</td>\n",
       "      <td id=\"T_1bd03_row6_col13\" class=\"data row6 col13\" >ISSUE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1bd03_level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
       "      <td id=\"T_1bd03_row7_col0\" class=\"data row7 col0\" >lighting.parquet</td>\n",
       "      <td id=\"T_1bd03_row7_col1\" class=\"data row7 col1\" >parsed_data/idf_data/by_category/lighting.parquet</td>\n",
       "      <td id=\"T_1bd03_row7_col2\" class=\"data row7 col2\" >✓</td>\n",
       "      <td id=\"T_1bd03_row7_col3\" class=\"data row7 col3\" >3</td>\n",
       "      <td id=\"T_1bd03_row7_col4\" class=\"data row7 col4\" >object_type</td>\n",
       "      <td id=\"T_1bd03_row7_col5\" class=\"data row7 col5\" >DISCOVERED</td>\n",
       "      <td id=\"T_1bd03_row7_col6\" class=\"data row7 col6\" >N/A</td>\n",
       "      <td id=\"T_1bd03_row7_col7\" class=\"data row7 col7\" >str</td>\n",
       "      <td id=\"T_1bd03_row7_col8\" class=\"data row7 col8\" >✓</td>\n",
       "      <td id=\"T_1bd03_row7_col9\" class=\"data row7 col9\" >N/A</td>\n",
       "      <td id=\"T_1bd03_row7_col10\" class=\"data row7 col10\" >0.000000</td>\n",
       "      <td id=\"T_1bd03_row7_col11\" class=\"data row7 col11\" >1.000000</td>\n",
       "      <td id=\"T_1bd03_row7_col12\" class=\"data row7 col12\" >['LIGHTS']</td>\n",
       "      <td id=\"T_1bd03_row7_col13\" class=\"data row7 col13\" >EXTRA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1bd03_level0_row8\" class=\"row_heading level0 row8\" >8</th>\n",
       "      <td id=\"T_1bd03_row8_col0\" class=\"data row8 col0\" >lighting.parquet</td>\n",
       "      <td id=\"T_1bd03_row8_col1\" class=\"data row8 col1\" >parsed_data/idf_data/by_category/lighting.parquet</td>\n",
       "      <td id=\"T_1bd03_row8_col2\" class=\"data row8 col2\" >✓</td>\n",
       "      <td id=\"T_1bd03_row8_col3\" class=\"data row8 col3\" >3</td>\n",
       "      <td id=\"T_1bd03_row8_col4\" class=\"data row8 col4\" >name</td>\n",
       "      <td id=\"T_1bd03_row8_col5\" class=\"data row8 col5\" >DISCOVERED</td>\n",
       "      <td id=\"T_1bd03_row8_col6\" class=\"data row8 col6\" >N/A</td>\n",
       "      <td id=\"T_1bd03_row8_col7\" class=\"data row8 col7\" >str</td>\n",
       "      <td id=\"T_1bd03_row8_col8\" class=\"data row8 col8\" >✓</td>\n",
       "      <td id=\"T_1bd03_row8_col9\" class=\"data row8 col9\" >N/A</td>\n",
       "      <td id=\"T_1bd03_row8_col10\" class=\"data row8 col10\" >0.000000</td>\n",
       "      <td id=\"T_1bd03_row8_col11\" class=\"data row8 col11\" >1.000000</td>\n",
       "      <td id=\"T_1bd03_row8_col12\" class=\"data row8 col12\" >['Lights_ALL_ZONES']</td>\n",
       "      <td id=\"T_1bd03_row8_col13\" class=\"data row8 col13\" >EXTRA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1bd03_level0_row9\" class=\"row_heading level0 row9\" >9</th>\n",
       "      <td id=\"T_1bd03_row9_col0\" class=\"data row9 col0\" >lighting.parquet</td>\n",
       "      <td id=\"T_1bd03_row9_col1\" class=\"data row9 col1\" >parsed_data/idf_data/by_category/lighting.parquet</td>\n",
       "      <td id=\"T_1bd03_row9_col2\" class=\"data row9 col2\" >✓</td>\n",
       "      <td id=\"T_1bd03_row9_col3\" class=\"data row9 col3\" >3</td>\n",
       "      <td id=\"T_1bd03_row9_col4\" class=\"data row9 col4\" >zone_or_zonelist_name</td>\n",
       "      <td id=\"T_1bd03_row9_col5\" class=\"data row9 col5\" >DISCOVERED</td>\n",
       "      <td id=\"T_1bd03_row9_col6\" class=\"data row9 col6\" >N/A</td>\n",
       "      <td id=\"T_1bd03_row9_col7\" class=\"data row9 col7\" >str</td>\n",
       "      <td id=\"T_1bd03_row9_col8\" class=\"data row9 col8\" >✓</td>\n",
       "      <td id=\"T_1bd03_row9_col9\" class=\"data row9 col9\" >N/A</td>\n",
       "      <td id=\"T_1bd03_row9_col10\" class=\"data row9 col10\" >0.000000</td>\n",
       "      <td id=\"T_1bd03_row9_col11\" class=\"data row9 col11\" >1.000000</td>\n",
       "      <td id=\"T_1bd03_row9_col12\" class=\"data row9 col12\" >['ALL_ZONES']</td>\n",
       "      <td id=\"T_1bd03_row9_col13\" class=\"data row9 col13\" >EXTRA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1bd03_level0_row10\" class=\"row_heading level0 row10\" >10</th>\n",
       "      <td id=\"T_1bd03_row10_col0\" class=\"data row10 col0\" >lighting.parquet</td>\n",
       "      <td id=\"T_1bd03_row10_col1\" class=\"data row10 col1\" >parsed_data/idf_data/by_category/lighting.parquet</td>\n",
       "      <td id=\"T_1bd03_row10_col2\" class=\"data row10 col2\" >✓</td>\n",
       "      <td id=\"T_1bd03_row10_col3\" class=\"data row10 col3\" >3</td>\n",
       "      <td id=\"T_1bd03_row10_col4\" class=\"data row10 col4\" >schedule_name</td>\n",
       "      <td id=\"T_1bd03_row10_col5\" class=\"data row10 col5\" >DISCOVERED</td>\n",
       "      <td id=\"T_1bd03_row10_col6\" class=\"data row10 col6\" >N/A</td>\n",
       "      <td id=\"T_1bd03_row10_col7\" class=\"data row10 col7\" >str</td>\n",
       "      <td id=\"T_1bd03_row10_col8\" class=\"data row10 col8\" >✓</td>\n",
       "      <td id=\"T_1bd03_row10_col9\" class=\"data row10 col9\" >N/A</td>\n",
       "      <td id=\"T_1bd03_row10_col10\" class=\"data row10 col10\" >0.000000</td>\n",
       "      <td id=\"T_1bd03_row10_col11\" class=\"data row10 col11\" >1.000000</td>\n",
       "      <td id=\"T_1bd03_row10_col12\" class=\"data row10 col12\" >['LightsSchedule']</td>\n",
       "      <td id=\"T_1bd03_row10_col13\" class=\"data row10 col13\" >EXTRA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1bd03_level0_row11\" class=\"row_heading level0 row11\" >11</th>\n",
       "      <td id=\"T_1bd03_row11_col0\" class=\"data row11 col0\" >lighting.parquet</td>\n",
       "      <td id=\"T_1bd03_row11_col1\" class=\"data row11 col1\" >parsed_data/idf_data/by_category/lighting.parquet</td>\n",
       "      <td id=\"T_1bd03_row11_col2\" class=\"data row11 col2\" >✓</td>\n",
       "      <td id=\"T_1bd03_row11_col3\" class=\"data row11 col3\" >3</td>\n",
       "      <td id=\"T_1bd03_row11_col4\" class=\"data row11 col4\" >design_level_calculation_method</td>\n",
       "      <td id=\"T_1bd03_row11_col5\" class=\"data row11 col5\" >DISCOVERED</td>\n",
       "      <td id=\"T_1bd03_row11_col6\" class=\"data row11 col6\" >N/A</td>\n",
       "      <td id=\"T_1bd03_row11_col7\" class=\"data row11 col7\" >str</td>\n",
       "      <td id=\"T_1bd03_row11_col8\" class=\"data row11 col8\" >✓</td>\n",
       "      <td id=\"T_1bd03_row11_col9\" class=\"data row11 col9\" >N/A</td>\n",
       "      <td id=\"T_1bd03_row11_col10\" class=\"data row11 col10\" >0.000000</td>\n",
       "      <td id=\"T_1bd03_row11_col11\" class=\"data row11 col11\" >1.000000</td>\n",
       "      <td id=\"T_1bd03_row11_col12\" class=\"data row11 col12\" >['Watts/Area']</td>\n",
       "      <td id=\"T_1bd03_row11_col13\" class=\"data row11 col13\" >EXTRA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1bd03_level0_row12\" class=\"row_heading level0 row12\" >12</th>\n",
       "      <td id=\"T_1bd03_row12_col0\" class=\"data row12 col0\" >lighting.parquet</td>\n",
       "      <td id=\"T_1bd03_row12_col1\" class=\"data row12 col1\" >parsed_data/idf_data/by_category/lighting.parquet</td>\n",
       "      <td id=\"T_1bd03_row12_col2\" class=\"data row12 col2\" >✓</td>\n",
       "      <td id=\"T_1bd03_row12_col3\" class=\"data row12 col3\" >3</td>\n",
       "      <td id=\"T_1bd03_row12_col4\" class=\"data row12 col4\" >watts_per_zone_floor_area</td>\n",
       "      <td id=\"T_1bd03_row12_col5\" class=\"data row12 col5\" >DISCOVERED</td>\n",
       "      <td id=\"T_1bd03_row12_col6\" class=\"data row12 col6\" >N/A</td>\n",
       "      <td id=\"T_1bd03_row12_col7\" class=\"data row12 col7\" >str</td>\n",
       "      <td id=\"T_1bd03_row12_col8\" class=\"data row12 col8\" >✓</td>\n",
       "      <td id=\"T_1bd03_row12_col9\" class=\"data row12 col9\" >N/A</td>\n",
       "      <td id=\"T_1bd03_row12_col10\" class=\"data row12 col10\" >0.000000</td>\n",
       "      <td id=\"T_1bd03_row12_col11\" class=\"data row12 col11\" >3.000000</td>\n",
       "      <td id=\"T_1bd03_row12_col12\" class=\"data row12 col12\" >['0', '4.092524414409712', '4.980486110210785']</td>\n",
       "      <td id=\"T_1bd03_row12_col13\" class=\"data row12 col13\" >EXTRA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1bd03_level0_row13\" class=\"row_heading level0 row13\" >13</th>\n",
       "      <td id=\"T_1bd03_row13_col0\" class=\"data row13 col0\" >lighting.parquet</td>\n",
       "      <td id=\"T_1bd03_row13_col1\" class=\"data row13 col1\" >parsed_data/idf_data/by_category/lighting.parquet</td>\n",
       "      <td id=\"T_1bd03_row13_col2\" class=\"data row13 col2\" >✓</td>\n",
       "      <td id=\"T_1bd03_row13_col3\" class=\"data row13 col3\" >3</td>\n",
       "      <td id=\"T_1bd03_row13_col4\" class=\"data row13 col4\" >watts_per_zone_floor_area_numeric</td>\n",
       "      <td id=\"T_1bd03_row13_col5\" class=\"data row13 col5\" >DISCOVERED</td>\n",
       "      <td id=\"T_1bd03_row13_col6\" class=\"data row13 col6\" >N/A</td>\n",
       "      <td id=\"T_1bd03_row13_col7\" class=\"data row13 col7\" >float</td>\n",
       "      <td id=\"T_1bd03_row13_col8\" class=\"data row13 col8\" >✓</td>\n",
       "      <td id=\"T_1bd03_row13_col9\" class=\"data row13 col9\" >N/A</td>\n",
       "      <td id=\"T_1bd03_row13_col10\" class=\"data row13 col10\" >0.000000</td>\n",
       "      <td id=\"T_1bd03_row13_col11\" class=\"data row13 col11\" >3.000000</td>\n",
       "      <td id=\"T_1bd03_row13_col12\" class=\"data row13 col12\" >[0.0, 4.092524414409712, 4.980486110210785]</td>\n",
       "      <td id=\"T_1bd03_row13_col13\" class=\"data row13 col13\" >EXTRA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1bd03_level0_row14\" class=\"row_heading level0 row14\" >14</th>\n",
       "      <td id=\"T_1bd03_row14_col0\" class=\"data row14 col0\" >lighting.parquet</td>\n",
       "      <td id=\"T_1bd03_row14_col1\" class=\"data row14 col1\" >parsed_data/idf_data/by_category/lighting.parquet</td>\n",
       "      <td id=\"T_1bd03_row14_col2\" class=\"data row14 col2\" >✓</td>\n",
       "      <td id=\"T_1bd03_row14_col3\" class=\"data row14 col3\" >3</td>\n",
       "      <td id=\"T_1bd03_row14_col4\" class=\"data row14 col4\" >return_air_fraction</td>\n",
       "      <td id=\"T_1bd03_row14_col5\" class=\"data row14 col5\" >DISCOVERED</td>\n",
       "      <td id=\"T_1bd03_row14_col6\" class=\"data row14 col6\" >N/A</td>\n",
       "      <td id=\"T_1bd03_row14_col7\" class=\"data row14 col7\" >str</td>\n",
       "      <td id=\"T_1bd03_row14_col8\" class=\"data row14 col8\" >✓</td>\n",
       "      <td id=\"T_1bd03_row14_col9\" class=\"data row14 col9\" >N/A</td>\n",
       "      <td id=\"T_1bd03_row14_col10\" class=\"data row14 col10\" >0.000000</td>\n",
       "      <td id=\"T_1bd03_row14_col11\" class=\"data row14 col11\" >1.000000</td>\n",
       "      <td id=\"T_1bd03_row14_col12\" class=\"data row14 col12\" >['0.8']</td>\n",
       "      <td id=\"T_1bd03_row14_col13\" class=\"data row14 col13\" >EXTRA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1bd03_level0_row15\" class=\"row_heading level0 row15\" >15</th>\n",
       "      <td id=\"T_1bd03_row15_col0\" class=\"data row15 col0\" >lighting.parquet</td>\n",
       "      <td id=\"T_1bd03_row15_col1\" class=\"data row15 col1\" >parsed_data/idf_data/by_category/lighting.parquet</td>\n",
       "      <td id=\"T_1bd03_row15_col2\" class=\"data row15 col2\" >✓</td>\n",
       "      <td id=\"T_1bd03_row15_col3\" class=\"data row15 col3\" >3</td>\n",
       "      <td id=\"T_1bd03_row15_col4\" class=\"data row15 col4\" >return_air_fraction_numeric</td>\n",
       "      <td id=\"T_1bd03_row15_col5\" class=\"data row15 col5\" >DISCOVERED</td>\n",
       "      <td id=\"T_1bd03_row15_col6\" class=\"data row15 col6\" >N/A</td>\n",
       "      <td id=\"T_1bd03_row15_col7\" class=\"data row15 col7\" >float</td>\n",
       "      <td id=\"T_1bd03_row15_col8\" class=\"data row15 col8\" >✓</td>\n",
       "      <td id=\"T_1bd03_row15_col9\" class=\"data row15 col9\" >N/A</td>\n",
       "      <td id=\"T_1bd03_row15_col10\" class=\"data row15 col10\" >0.000000</td>\n",
       "      <td id=\"T_1bd03_row15_col11\" class=\"data row15 col11\" >1.000000</td>\n",
       "      <td id=\"T_1bd03_row15_col12\" class=\"data row15 col12\" >[0.8]</td>\n",
       "      <td id=\"T_1bd03_row15_col13\" class=\"data row15 col13\" >EXTRA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1bd03_level0_row16\" class=\"row_heading level0 row16\" >16</th>\n",
       "      <td id=\"T_1bd03_row16_col0\" class=\"data row16 col0\" >lighting.parquet</td>\n",
       "      <td id=\"T_1bd03_row16_col1\" class=\"data row16 col1\" >parsed_data/idf_data/by_category/lighting.parquet</td>\n",
       "      <td id=\"T_1bd03_row16_col2\" class=\"data row16 col2\" >✓</td>\n",
       "      <td id=\"T_1bd03_row16_col3\" class=\"data row16 col3\" >3</td>\n",
       "      <td id=\"T_1bd03_row16_col4\" class=\"data row16 col4\" >fraction_radiant</td>\n",
       "      <td id=\"T_1bd03_row16_col5\" class=\"data row16 col5\" >DISCOVERED</td>\n",
       "      <td id=\"T_1bd03_row16_col6\" class=\"data row16 col6\" >N/A</td>\n",
       "      <td id=\"T_1bd03_row16_col7\" class=\"data row16 col7\" >str</td>\n",
       "      <td id=\"T_1bd03_row16_col8\" class=\"data row16 col8\" >✓</td>\n",
       "      <td id=\"T_1bd03_row16_col9\" class=\"data row16 col9\" >N/A</td>\n",
       "      <td id=\"T_1bd03_row16_col10\" class=\"data row16 col10\" >0.000000</td>\n",
       "      <td id=\"T_1bd03_row16_col11\" class=\"data row16 col11\" >1.000000</td>\n",
       "      <td id=\"T_1bd03_row16_col12\" class=\"data row16 col12\" >['0.1']</td>\n",
       "      <td id=\"T_1bd03_row16_col13\" class=\"data row16 col13\" >EXTRA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1bd03_level0_row17\" class=\"row_heading level0 row17\" >17</th>\n",
       "      <td id=\"T_1bd03_row17_col0\" class=\"data row17 col0\" >lighting.parquet</td>\n",
       "      <td id=\"T_1bd03_row17_col1\" class=\"data row17 col1\" >parsed_data/idf_data/by_category/lighting.parquet</td>\n",
       "      <td id=\"T_1bd03_row17_col2\" class=\"data row17 col2\" >✓</td>\n",
       "      <td id=\"T_1bd03_row17_col3\" class=\"data row17 col3\" >3</td>\n",
       "      <td id=\"T_1bd03_row17_col4\" class=\"data row17 col4\" >fraction_radiant_numeric</td>\n",
       "      <td id=\"T_1bd03_row17_col5\" class=\"data row17 col5\" >DISCOVERED</td>\n",
       "      <td id=\"T_1bd03_row17_col6\" class=\"data row17 col6\" >N/A</td>\n",
       "      <td id=\"T_1bd03_row17_col7\" class=\"data row17 col7\" >float</td>\n",
       "      <td id=\"T_1bd03_row17_col8\" class=\"data row17 col8\" >✓</td>\n",
       "      <td id=\"T_1bd03_row17_col9\" class=\"data row17 col9\" >N/A</td>\n",
       "      <td id=\"T_1bd03_row17_col10\" class=\"data row17 col10\" >0.000000</td>\n",
       "      <td id=\"T_1bd03_row17_col11\" class=\"data row17 col11\" >1.000000</td>\n",
       "      <td id=\"T_1bd03_row17_col12\" class=\"data row17 col12\" >[0.1]</td>\n",
       "      <td id=\"T_1bd03_row17_col13\" class=\"data row17 col13\" >EXTRA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1bd03_level0_row18\" class=\"row_heading level0 row18\" >18</th>\n",
       "      <td id=\"T_1bd03_row18_col0\" class=\"data row18 col0\" >lighting.parquet</td>\n",
       "      <td id=\"T_1bd03_row18_col1\" class=\"data row18 col1\" >parsed_data/idf_data/by_category/lighting.parquet</td>\n",
       "      <td id=\"T_1bd03_row18_col2\" class=\"data row18 col2\" >✓</td>\n",
       "      <td id=\"T_1bd03_row18_col3\" class=\"data row18 col3\" >3</td>\n",
       "      <td id=\"T_1bd03_row18_col4\" class=\"data row18 col4\" >fraction_visible</td>\n",
       "      <td id=\"T_1bd03_row18_col5\" class=\"data row18 col5\" >DISCOVERED</td>\n",
       "      <td id=\"T_1bd03_row18_col6\" class=\"data row18 col6\" >N/A</td>\n",
       "      <td id=\"T_1bd03_row18_col7\" class=\"data row18 col7\" >str</td>\n",
       "      <td id=\"T_1bd03_row18_col8\" class=\"data row18 col8\" >✓</td>\n",
       "      <td id=\"T_1bd03_row18_col9\" class=\"data row18 col9\" >N/A</td>\n",
       "      <td id=\"T_1bd03_row18_col10\" class=\"data row18 col10\" >0.000000</td>\n",
       "      <td id=\"T_1bd03_row18_col11\" class=\"data row18 col11\" >1.000000</td>\n",
       "      <td id=\"T_1bd03_row18_col12\" class=\"data row18 col12\" >['0.1']</td>\n",
       "      <td id=\"T_1bd03_row18_col13\" class=\"data row18 col13\" >EXTRA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1bd03_level0_row19\" class=\"row_heading level0 row19\" >19</th>\n",
       "      <td id=\"T_1bd03_row19_col0\" class=\"data row19 col0\" >lighting.parquet</td>\n",
       "      <td id=\"T_1bd03_row19_col1\" class=\"data row19 col1\" >parsed_data/idf_data/by_category/lighting.parquet</td>\n",
       "      <td id=\"T_1bd03_row19_col2\" class=\"data row19 col2\" >✓</td>\n",
       "      <td id=\"T_1bd03_row19_col3\" class=\"data row19 col3\" >3</td>\n",
       "      <td id=\"T_1bd03_row19_col4\" class=\"data row19 col4\" >fraction_visible_numeric</td>\n",
       "      <td id=\"T_1bd03_row19_col5\" class=\"data row19 col5\" >DISCOVERED</td>\n",
       "      <td id=\"T_1bd03_row19_col6\" class=\"data row19 col6\" >N/A</td>\n",
       "      <td id=\"T_1bd03_row19_col7\" class=\"data row19 col7\" >float</td>\n",
       "      <td id=\"T_1bd03_row19_col8\" class=\"data row19 col8\" >✓</td>\n",
       "      <td id=\"T_1bd03_row19_col9\" class=\"data row19 col9\" >N/A</td>\n",
       "      <td id=\"T_1bd03_row19_col10\" class=\"data row19 col10\" >0.000000</td>\n",
       "      <td id=\"T_1bd03_row19_col11\" class=\"data row19 col11\" >1.000000</td>\n",
       "      <td id=\"T_1bd03_row19_col12\" class=\"data row19 col12\" >[0.1]</td>\n",
       "      <td id=\"T_1bd03_row19_col13\" class=\"data row19 col13\" >EXTRA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1bd03_level0_row20\" class=\"row_heading level0 row20\" >20</th>\n",
       "      <td id=\"T_1bd03_row20_col0\" class=\"data row20 col0\" >lighting.parquet</td>\n",
       "      <td id=\"T_1bd03_row20_col1\" class=\"data row20 col1\" >parsed_data/idf_data/by_category/lighting.parquet</td>\n",
       "      <td id=\"T_1bd03_row20_col2\" class=\"data row20 col2\" >✓</td>\n",
       "      <td id=\"T_1bd03_row20_col3\" class=\"data row20 col3\" >3</td>\n",
       "      <td id=\"T_1bd03_row20_col4\" class=\"data row20 col4\" >fraction_replaceable</td>\n",
       "      <td id=\"T_1bd03_row20_col5\" class=\"data row20 col5\" >DISCOVERED</td>\n",
       "      <td id=\"T_1bd03_row20_col6\" class=\"data row20 col6\" >N/A</td>\n",
       "      <td id=\"T_1bd03_row20_col7\" class=\"data row20 col7\" >str</td>\n",
       "      <td id=\"T_1bd03_row20_col8\" class=\"data row20 col8\" >✓</td>\n",
       "      <td id=\"T_1bd03_row20_col9\" class=\"data row20 col9\" >N/A</td>\n",
       "      <td id=\"T_1bd03_row20_col10\" class=\"data row20 col10\" >0.000000</td>\n",
       "      <td id=\"T_1bd03_row20_col11\" class=\"data row20 col11\" >1.000000</td>\n",
       "      <td id=\"T_1bd03_row20_col12\" class=\"data row20 col12\" >['1']</td>\n",
       "      <td id=\"T_1bd03_row20_col13\" class=\"data row20 col13\" >EXTRA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1bd03_level0_row21\" class=\"row_heading level0 row21\" >21</th>\n",
       "      <td id=\"T_1bd03_row21_col0\" class=\"data row21 col0\" >lighting.parquet</td>\n",
       "      <td id=\"T_1bd03_row21_col1\" class=\"data row21 col1\" >parsed_data/idf_data/by_category/lighting.parquet</td>\n",
       "      <td id=\"T_1bd03_row21_col2\" class=\"data row21 col2\" >✓</td>\n",
       "      <td id=\"T_1bd03_row21_col3\" class=\"data row21 col3\" >3</td>\n",
       "      <td id=\"T_1bd03_row21_col4\" class=\"data row21 col4\" >fraction_replaceable_numeric</td>\n",
       "      <td id=\"T_1bd03_row21_col5\" class=\"data row21 col5\" >DISCOVERED</td>\n",
       "      <td id=\"T_1bd03_row21_col6\" class=\"data row21 col6\" >N/A</td>\n",
       "      <td id=\"T_1bd03_row21_col7\" class=\"data row21 col7\" >float</td>\n",
       "      <td id=\"T_1bd03_row21_col8\" class=\"data row21 col8\" >✓</td>\n",
       "      <td id=\"T_1bd03_row21_col9\" class=\"data row21 col9\" >N/A</td>\n",
       "      <td id=\"T_1bd03_row21_col10\" class=\"data row21 col10\" >0.000000</td>\n",
       "      <td id=\"T_1bd03_row21_col11\" class=\"data row21 col11\" >1.000000</td>\n",
       "      <td id=\"T_1bd03_row21_col12\" class=\"data row21 col12\" >[1.0]</td>\n",
       "      <td id=\"T_1bd03_row21_col13\" class=\"data row21 col13\" >EXTRA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1bd03_level0_row22\" class=\"row_heading level0 row22\" >22</th>\n",
       "      <td id=\"T_1bd03_row22_col0\" class=\"data row22 col0\" >lighting.parquet</td>\n",
       "      <td id=\"T_1bd03_row22_col1\" class=\"data row22 col1\" >parsed_data/idf_data/by_category/lighting.parquet</td>\n",
       "      <td id=\"T_1bd03_row22_col2\" class=\"data row22 col2\" >✓</td>\n",
       "      <td id=\"T_1bd03_row22_col3\" class=\"data row22 col3\" >3</td>\n",
       "      <td id=\"T_1bd03_row22_col4\" class=\"data row22 col4\" >end_use_subcategory</td>\n",
       "      <td id=\"T_1bd03_row22_col5\" class=\"data row22 col5\" >DISCOVERED</td>\n",
       "      <td id=\"T_1bd03_row22_col6\" class=\"data row22 col6\" >N/A</td>\n",
       "      <td id=\"T_1bd03_row22_col7\" class=\"data row22 col7\" >str</td>\n",
       "      <td id=\"T_1bd03_row22_col8\" class=\"data row22 col8\" >✓</td>\n",
       "      <td id=\"T_1bd03_row22_col9\" class=\"data row22 col9\" >N/A</td>\n",
       "      <td id=\"T_1bd03_row22_col10\" class=\"data row22 col10\" >0.000000</td>\n",
       "      <td id=\"T_1bd03_row22_col11\" class=\"data row22 col11\" >1.000000</td>\n",
       "      <td id=\"T_1bd03_row22_col12\" class=\"data row22 col12\" >['General']</td>\n",
       "      <td id=\"T_1bd03_row22_col13\" class=\"data row22 col13\" >EXTRA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1bd03_level0_row23\" class=\"row_heading level0 row23\" >23</th>\n",
       "      <td id=\"T_1bd03_row23_col0\" class=\"data row23 col0\" >lighting.parquet</td>\n",
       "      <td id=\"T_1bd03_row23_col1\" class=\"data row23 col1\" >parsed_data/idf_data/by_category/lighting.parquet</td>\n",
       "      <td id=\"T_1bd03_row23_col2\" class=\"data row23 col2\" >✓</td>\n",
       "      <td id=\"T_1bd03_row23_col3\" class=\"data row23 col3\" >3</td>\n",
       "      <td id=\"T_1bd03_row23_col4\" class=\"data row23 col4\" >return_air_fraction_calculated_from_plenum_temperature</td>\n",
       "      <td id=\"T_1bd03_row23_col5\" class=\"data row23 col5\" >DISCOVERED</td>\n",
       "      <td id=\"T_1bd03_row23_col6\" class=\"data row23 col6\" >N/A</td>\n",
       "      <td id=\"T_1bd03_row23_col7\" class=\"data row23 col7\" >str</td>\n",
       "      <td id=\"T_1bd03_row23_col8\" class=\"data row23 col8\" >✓</td>\n",
       "      <td id=\"T_1bd03_row23_col9\" class=\"data row23 col9\" >N/A</td>\n",
       "      <td id=\"T_1bd03_row23_col10\" class=\"data row23 col10\" >0.000000</td>\n",
       "      <td id=\"T_1bd03_row23_col11\" class=\"data row23 col11\" >1.000000</td>\n",
       "      <td id=\"T_1bd03_row23_col12\" class=\"data row23 col12\" >['No']</td>\n",
       "      <td id=\"T_1bd03_row23_col13\" class=\"data row23 col13\" >EXTRA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1bd03_level0_row24\" class=\"row_heading level0 row24\" >24</th>\n",
       "      <td id=\"T_1bd03_row24_col0\" class=\"data row24 col0\" >lighting.parquet</td>\n",
       "      <td id=\"T_1bd03_row24_col1\" class=\"data row24 col1\" >parsed_data/idf_data/by_category/lighting.parquet</td>\n",
       "      <td id=\"T_1bd03_row24_col2\" class=\"data row24 col2\" >✓</td>\n",
       "      <td id=\"T_1bd03_row24_col3\" class=\"data row24 col3\" >3</td>\n",
       "      <td id=\"T_1bd03_row24_col4\" class=\"data row24 col4\" >return_air_fraction_function_of_plenum_temperature_coefficient_1</td>\n",
       "      <td id=\"T_1bd03_row24_col5\" class=\"data row24 col5\" >DISCOVERED</td>\n",
       "      <td id=\"T_1bd03_row24_col6\" class=\"data row24 col6\" >N/A</td>\n",
       "      <td id=\"T_1bd03_row24_col7\" class=\"data row24 col7\" >str</td>\n",
       "      <td id=\"T_1bd03_row24_col8\" class=\"data row24 col8\" >✓</td>\n",
       "      <td id=\"T_1bd03_row24_col9\" class=\"data row24 col9\" >N/A</td>\n",
       "      <td id=\"T_1bd03_row24_col10\" class=\"data row24 col10\" >0.000000</td>\n",
       "      <td id=\"T_1bd03_row24_col11\" class=\"data row24 col11\" >1.000000</td>\n",
       "      <td id=\"T_1bd03_row24_col12\" class=\"data row24 col12\" >['0']</td>\n",
       "      <td id=\"T_1bd03_row24_col13\" class=\"data row24 col13\" >EXTRA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1bd03_level0_row25\" class=\"row_heading level0 row25\" >25</th>\n",
       "      <td id=\"T_1bd03_row25_col0\" class=\"data row25 col0\" >lighting.parquet</td>\n",
       "      <td id=\"T_1bd03_row25_col1\" class=\"data row25 col1\" >parsed_data/idf_data/by_category/lighting.parquet</td>\n",
       "      <td id=\"T_1bd03_row25_col2\" class=\"data row25 col2\" >✓</td>\n",
       "      <td id=\"T_1bd03_row25_col3\" class=\"data row25 col3\" >3</td>\n",
       "      <td id=\"T_1bd03_row25_col4\" class=\"data row25 col4\" >return_air_fraction_function_of_plenum_temperature_coefficient_1_numeric</td>\n",
       "      <td id=\"T_1bd03_row25_col5\" class=\"data row25 col5\" >DISCOVERED</td>\n",
       "      <td id=\"T_1bd03_row25_col6\" class=\"data row25 col6\" >N/A</td>\n",
       "      <td id=\"T_1bd03_row25_col7\" class=\"data row25 col7\" >float</td>\n",
       "      <td id=\"T_1bd03_row25_col8\" class=\"data row25 col8\" >✓</td>\n",
       "      <td id=\"T_1bd03_row25_col9\" class=\"data row25 col9\" >N/A</td>\n",
       "      <td id=\"T_1bd03_row25_col10\" class=\"data row25 col10\" >0.000000</td>\n",
       "      <td id=\"T_1bd03_row25_col11\" class=\"data row25 col11\" >1.000000</td>\n",
       "      <td id=\"T_1bd03_row25_col12\" class=\"data row25 col12\" >[0.0]</td>\n",
       "      <td id=\"T_1bd03_row25_col13\" class=\"data row25 col13\" >EXTRA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1bd03_level0_row26\" class=\"row_heading level0 row26\" >26</th>\n",
       "      <td id=\"T_1bd03_row26_col0\" class=\"data row26 col0\" >lighting.parquet</td>\n",
       "      <td id=\"T_1bd03_row26_col1\" class=\"data row26 col1\" >parsed_data/idf_data/by_category/lighting.parquet</td>\n",
       "      <td id=\"T_1bd03_row26_col2\" class=\"data row26 col2\" >✓</td>\n",
       "      <td id=\"T_1bd03_row26_col3\" class=\"data row26 col3\" >3</td>\n",
       "      <td id=\"T_1bd03_row26_col4\" class=\"data row26 col4\" >return_air_fraction_function_of_plenum_temperature_coefficient_2</td>\n",
       "      <td id=\"T_1bd03_row26_col5\" class=\"data row26 col5\" >DISCOVERED</td>\n",
       "      <td id=\"T_1bd03_row26_col6\" class=\"data row26 col6\" >N/A</td>\n",
       "      <td id=\"T_1bd03_row26_col7\" class=\"data row26 col7\" >str</td>\n",
       "      <td id=\"T_1bd03_row26_col8\" class=\"data row26 col8\" >✓</td>\n",
       "      <td id=\"T_1bd03_row26_col9\" class=\"data row26 col9\" >N/A</td>\n",
       "      <td id=\"T_1bd03_row26_col10\" class=\"data row26 col10\" >0.000000</td>\n",
       "      <td id=\"T_1bd03_row26_col11\" class=\"data row26 col11\" >1.000000</td>\n",
       "      <td id=\"T_1bd03_row26_col12\" class=\"data row26 col12\" >['0']</td>\n",
       "      <td id=\"T_1bd03_row26_col13\" class=\"data row26 col13\" >EXTRA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1bd03_level0_row27\" class=\"row_heading level0 row27\" >27</th>\n",
       "      <td id=\"T_1bd03_row27_col0\" class=\"data row27 col0\" >lighting.parquet</td>\n",
       "      <td id=\"T_1bd03_row27_col1\" class=\"data row27 col1\" >parsed_data/idf_data/by_category/lighting.parquet</td>\n",
       "      <td id=\"T_1bd03_row27_col2\" class=\"data row27 col2\" >✓</td>\n",
       "      <td id=\"T_1bd03_row27_col3\" class=\"data row27 col3\" >3</td>\n",
       "      <td id=\"T_1bd03_row27_col4\" class=\"data row27 col4\" >return_air_fraction_function_of_plenum_temperature_coefficient_2_numeric</td>\n",
       "      <td id=\"T_1bd03_row27_col5\" class=\"data row27 col5\" >DISCOVERED</td>\n",
       "      <td id=\"T_1bd03_row27_col6\" class=\"data row27 col6\" >N/A</td>\n",
       "      <td id=\"T_1bd03_row27_col7\" class=\"data row27 col7\" >float</td>\n",
       "      <td id=\"T_1bd03_row27_col8\" class=\"data row27 col8\" >✓</td>\n",
       "      <td id=\"T_1bd03_row27_col9\" class=\"data row27 col9\" >N/A</td>\n",
       "      <td id=\"T_1bd03_row27_col10\" class=\"data row27 col10\" >0.000000</td>\n",
       "      <td id=\"T_1bd03_row27_col11\" class=\"data row27 col11\" >1.000000</td>\n",
       "      <td id=\"T_1bd03_row27_col12\" class=\"data row27 col12\" >[0.0]</td>\n",
       "      <td id=\"T_1bd03_row27_col13\" class=\"data row27 col13\" >EXTRA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1bd03_level0_row28\" class=\"row_heading level0 row28\" >28</th>\n",
       "      <td id=\"T_1bd03_row28_col0\" class=\"data row28 col0\" >lighting.parquet</td>\n",
       "      <td id=\"T_1bd03_row28_col1\" class=\"data row28 col1\" >parsed_data/idf_data/by_category/lighting.parquet</td>\n",
       "      <td id=\"T_1bd03_row28_col2\" class=\"data row28 col2\" >✓</td>\n",
       "      <td id=\"T_1bd03_row28_col3\" class=\"data row28 col3\" >3</td>\n",
       "      <td id=\"T_1bd03_row28_col4\" class=\"data row28 col4\" >variant_id</td>\n",
       "      <td id=\"T_1bd03_row28_col5\" class=\"data row28 col5\" >DISCOVERED</td>\n",
       "      <td id=\"T_1bd03_row28_col6\" class=\"data row28 col6\" >N/A</td>\n",
       "      <td id=\"T_1bd03_row28_col7\" class=\"data row28 col7\" >str</td>\n",
       "      <td id=\"T_1bd03_row28_col8\" class=\"data row28 col8\" >✓</td>\n",
       "      <td id=\"T_1bd03_row28_col9\" class=\"data row28 col9\" >N/A</td>\n",
       "      <td id=\"T_1bd03_row28_col10\" class=\"data row28 col10\" >0.000000</td>\n",
       "      <td id=\"T_1bd03_row28_col11\" class=\"data row28 col11\" >1.000000</td>\n",
       "      <td id=\"T_1bd03_row28_col12\" class=\"data row28 col12\" >['base']</td>\n",
       "      <td id=\"T_1bd03_row28_col13\" class=\"data row28 col13\" >EXTRA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1bd03_level0_row29\" class=\"row_heading level0 row29\" >29</th>\n",
       "      <td id=\"T_1bd03_row29_col0\" class=\"data row29 col0\" >hvac_equipment.parquet</td>\n",
       "      <td id=\"T_1bd03_row29_col1\" class=\"data row29 col1\" >parsed_data/idf_data/by_category/hvac_equipment.parquet</td>\n",
       "      <td id=\"T_1bd03_row29_col2\" class=\"data row29 col2\" >✓</td>\n",
       "      <td id=\"T_1bd03_row29_col3\" class=\"data row29 col3\" >48</td>\n",
       "      <td id=\"T_1bd03_row29_col4\" class=\"data row29 col4\" >building_id</td>\n",
       "      <td id=\"T_1bd03_row29_col5\" class=\"data row29 col5\" >Expected</td>\n",
       "      <td id=\"T_1bd03_row29_col6\" class=\"data row29 col6\" >str</td>\n",
       "      <td id=\"T_1bd03_row29_col7\" class=\"data row29 col7\" >str</td>\n",
       "      <td id=\"T_1bd03_row29_col8\" class=\"data row29 col8\" >✓</td>\n",
       "      <td id=\"T_1bd03_row29_col9\" class=\"data row29 col9\" >✓</td>\n",
       "      <td id=\"T_1bd03_row29_col10\" class=\"data row29 col10\" >0.000000</td>\n",
       "      <td id=\"T_1bd03_row29_col11\" class=\"data row29 col11\" >3.000000</td>\n",
       "      <td id=\"T_1bd03_row29_col12\" class=\"data row29 col12\" >['4136733', '4136737', '4136738']</td>\n",
       "      <td id=\"T_1bd03_row29_col13\" class=\"data row29 col13\" >OK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1bd03_level0_row30\" class=\"row_heading level0 row30\" >30</th>\n",
       "      <td id=\"T_1bd03_row30_col0\" class=\"data row30 col0\" >hvac_equipment.parquet</td>\n",
       "      <td id=\"T_1bd03_row30_col1\" class=\"data row30 col1\" >parsed_data/idf_data/by_category/hvac_equipment.parquet</td>\n",
       "      <td id=\"T_1bd03_row30_col2\" class=\"data row30 col2\" >✓</td>\n",
       "      <td id=\"T_1bd03_row30_col3\" class=\"data row30 col3\" >48</td>\n",
       "      <td id=\"T_1bd03_row30_col4\" class=\"data row30 col4\" >zone_name</td>\n",
       "      <td id=\"T_1bd03_row30_col5\" class=\"data row30 col5\" >Expected</td>\n",
       "      <td id=\"T_1bd03_row30_col6\" class=\"data row30 col6\" >str</td>\n",
       "      <td id=\"T_1bd03_row30_col7\" class=\"data row30 col7\" >str</td>\n",
       "      <td id=\"T_1bd03_row30_col8\" class=\"data row30 col8\" >✓</td>\n",
       "      <td id=\"T_1bd03_row30_col9\" class=\"data row30 col9\" >✓</td>\n",
       "      <td id=\"T_1bd03_row30_col10\" class=\"data row30 col10\" >32.000000</td>\n",
       "      <td id=\"T_1bd03_row30_col11\" class=\"data row30 col11\" >11.000000</td>\n",
       "      <td id=\"T_1bd03_row30_col12\" class=\"data row30 col12\" >['Zone1_FrontPerimeter_EQUIPMENT', 'Zone1_RightPerimeter_EQUIPMENT', 'Zone1_RearPerimeter_EQUIPMENT']</td>\n",
       "      <td id=\"T_1bd03_row30_col13\" class=\"data row30 col13\" >OK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1bd03_level0_row31\" class=\"row_heading level0 row31\" >31</th>\n",
       "      <td id=\"T_1bd03_row31_col0\" class=\"data row31 col0\" >hvac_equipment.parquet</td>\n",
       "      <td id=\"T_1bd03_row31_col1\" class=\"data row31 col1\" >parsed_data/idf_data/by_category/hvac_equipment.parquet</td>\n",
       "      <td id=\"T_1bd03_row31_col2\" class=\"data row31 col2\" >✓</td>\n",
       "      <td id=\"T_1bd03_row31_col3\" class=\"data row31 col3\" >48</td>\n",
       "      <td id=\"T_1bd03_row31_col4\" class=\"data row31 col4\" >object_name</td>\n",
       "      <td id=\"T_1bd03_row31_col5\" class=\"data row31 col5\" >Expected</td>\n",
       "      <td id=\"T_1bd03_row31_col6\" class=\"data row31 col6\" >str</td>\n",
       "      <td id=\"T_1bd03_row31_col7\" class=\"data row31 col7\" >str</td>\n",
       "      <td id=\"T_1bd03_row31_col8\" class=\"data row31 col8\" >✓</td>\n",
       "      <td id=\"T_1bd03_row31_col9\" class=\"data row31 col9\" >✓</td>\n",
       "      <td id=\"T_1bd03_row31_col10\" class=\"data row31 col10\" >0.000000</td>\n",
       "      <td id=\"T_1bd03_row31_col11\" class=\"data row31 col11\" >33.000000</td>\n",
       "      <td id=\"T_1bd03_row31_col12\" class=\"data row31 col12\" >['Zone1_FrontPerimeter_Ideal_Loads', 'Zone1_RightPerimeter_Ideal_Loads', 'Zone1_RearPerimeter_Ideal_Loads']</td>\n",
       "      <td id=\"T_1bd03_row31_col13\" class=\"data row31 col13\" >OK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1bd03_level0_row32\" class=\"row_heading level0 row32\" >32</th>\n",
       "      <td id=\"T_1bd03_row32_col0\" class=\"data row32 col0\" >hvac_equipment.parquet</td>\n",
       "      <td id=\"T_1bd03_row32_col1\" class=\"data row32 col1\" >parsed_data/idf_data/by_category/hvac_equipment.parquet</td>\n",
       "      <td id=\"T_1bd03_row32_col2\" class=\"data row32 col2\" >✓</td>\n",
       "      <td id=\"T_1bd03_row32_col3\" class=\"data row32 col3\" >48</td>\n",
       "      <td id=\"T_1bd03_row32_col4\" class=\"data row32 col4\" >object_type</td>\n",
       "      <td id=\"T_1bd03_row32_col5\" class=\"data row32 col5\" >Expected</td>\n",
       "      <td id=\"T_1bd03_row32_col6\" class=\"data row32 col6\" >str</td>\n",
       "      <td id=\"T_1bd03_row32_col7\" class=\"data row32 col7\" >str</td>\n",
       "      <td id=\"T_1bd03_row32_col8\" class=\"data row32 col8\" >✓</td>\n",
       "      <td id=\"T_1bd03_row32_col9\" class=\"data row32 col9\" >✓</td>\n",
       "      <td id=\"T_1bd03_row32_col10\" class=\"data row32 col10\" >0.000000</td>\n",
       "      <td id=\"T_1bd03_row32_col11\" class=\"data row32 col11\" >3.000000</td>\n",
       "      <td id=\"T_1bd03_row32_col12\" class=\"data row32 col12\" >['ZONEHVAC:IDEALLOADSAIRSYSTEM', 'ZONEHVAC:EQUIPMENTLIST', 'ZONEHVAC:EQUIPMENTCONNECTIONS']</td>\n",
       "      <td id=\"T_1bd03_row32_col13\" class=\"data row32 col13\" >OK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1bd03_level0_row33\" class=\"row_heading level0 row33\" >33</th>\n",
       "      <td id=\"T_1bd03_row33_col0\" class=\"data row33 col0\" >hvac_equipment.parquet</td>\n",
       "      <td id=\"T_1bd03_row33_col1\" class=\"data row33 col1\" >parsed_data/idf_data/by_category/hvac_equipment.parquet</td>\n",
       "      <td id=\"T_1bd03_row33_col2\" class=\"data row33 col2\" >✓</td>\n",
       "      <td id=\"T_1bd03_row33_col3\" class=\"data row33 col3\" >48</td>\n",
       "      <td id=\"T_1bd03_row33_col4\" class=\"data row33 col4\" >MaximumHeatingSupplyAirTemperature</td>\n",
       "      <td id=\"T_1bd03_row33_col5\" class=\"data row33 col5\" >Expected</td>\n",
       "      <td id=\"T_1bd03_row33_col6\" class=\"data row33 col6\" >float</td>\n",
       "      <td id=\"T_1bd03_row33_col7\" class=\"data row33 col7\" >None</td>\n",
       "      <td id=\"T_1bd03_row33_col8\" class=\"data row33 col8\" >✗</td>\n",
       "      <td id=\"T_1bd03_row33_col9\" class=\"data row33 col9\" >✗</td>\n",
       "      <td id=\"T_1bd03_row33_col10\" class=\"data row33 col10\" >nan</td>\n",
       "      <td id=\"T_1bd03_row33_col11\" class=\"data row33 col11\" >nan</td>\n",
       "      <td id=\"T_1bd03_row33_col12\" class=\"data row33 col12\" >N/A</td>\n",
       "      <td id=\"T_1bd03_row33_col13\" class=\"data row33 col13\" >ISSUE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1bd03_level0_row34\" class=\"row_heading level0 row34\" >34</th>\n",
       "      <td id=\"T_1bd03_row34_col0\" class=\"data row34 col0\" >hvac_equipment.parquet</td>\n",
       "      <td id=\"T_1bd03_row34_col1\" class=\"data row34 col1\" >parsed_data/idf_data/by_category/hvac_equipment.parquet</td>\n",
       "      <td id=\"T_1bd03_row34_col2\" class=\"data row34 col2\" >✓</td>\n",
       "      <td id=\"T_1bd03_row34_col3\" class=\"data row34 col3\" >48</td>\n",
       "      <td id=\"T_1bd03_row34_col4\" class=\"data row34 col4\" >MinimumCoolingSupplyAirTemperature</td>\n",
       "      <td id=\"T_1bd03_row34_col5\" class=\"data row34 col5\" >Expected</td>\n",
       "      <td id=\"T_1bd03_row34_col6\" class=\"data row34 col6\" >float</td>\n",
       "      <td id=\"T_1bd03_row34_col7\" class=\"data row34 col7\" >None</td>\n",
       "      <td id=\"T_1bd03_row34_col8\" class=\"data row34 col8\" >✗</td>\n",
       "      <td id=\"T_1bd03_row34_col9\" class=\"data row34 col9\" >✗</td>\n",
       "      <td id=\"T_1bd03_row34_col10\" class=\"data row34 col10\" >nan</td>\n",
       "      <td id=\"T_1bd03_row34_col11\" class=\"data row34 col11\" >nan</td>\n",
       "      <td id=\"T_1bd03_row34_col12\" class=\"data row34 col12\" >N/A</td>\n",
       "      <td id=\"T_1bd03_row34_col13\" class=\"data row34 col13\" >ISSUE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1bd03_level0_row35\" class=\"row_heading level0 row35\" >35</th>\n",
       "      <td id=\"T_1bd03_row35_col0\" class=\"data row35 col0\" >hvac_equipment.parquet</td>\n",
       "      <td id=\"T_1bd03_row35_col1\" class=\"data row35 col1\" >parsed_data/idf_data/by_category/hvac_equipment.parquet</td>\n",
       "      <td id=\"T_1bd03_row35_col2\" class=\"data row35 col2\" >✓</td>\n",
       "      <td id=\"T_1bd03_row35_col3\" class=\"data row35 col3\" >48</td>\n",
       "      <td id=\"T_1bd03_row35_col4\" class=\"data row35 col4\" >name</td>\n",
       "      <td id=\"T_1bd03_row35_col5\" class=\"data row35 col5\" >DISCOVERED</td>\n",
       "      <td id=\"T_1bd03_row35_col6\" class=\"data row35 col6\" >N/A</td>\n",
       "      <td id=\"T_1bd03_row35_col7\" class=\"data row35 col7\" >str</td>\n",
       "      <td id=\"T_1bd03_row35_col8\" class=\"data row35 col8\" >✓</td>\n",
       "      <td id=\"T_1bd03_row35_col9\" class=\"data row35 col9\" >N/A</td>\n",
       "      <td id=\"T_1bd03_row35_col10\" class=\"data row35 col10\" >32.000000</td>\n",
       "      <td id=\"T_1bd03_row35_col11\" class=\"data row35 col11\" >11.000000</td>\n",
       "      <td id=\"T_1bd03_row35_col12\" class=\"data row35 col12\" >['Zone1_FrontPerimeter_Ideal_Loads', 'Zone1_RightPerimeter_Ideal_Loads', 'Zone1_RearPerimeter_Ideal_Loads']</td>\n",
       "      <td id=\"T_1bd03_row35_col13\" class=\"data row35 col13\" >EXTRA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1bd03_level0_row36\" class=\"row_heading level0 row36\" >36</th>\n",
       "      <td id=\"T_1bd03_row36_col0\" class=\"data row36 col0\" >hvac_equipment.parquet</td>\n",
       "      <td id=\"T_1bd03_row36_col1\" class=\"data row36 col1\" >parsed_data/idf_data/by_category/hvac_equipment.parquet</td>\n",
       "      <td id=\"T_1bd03_row36_col2\" class=\"data row36 col2\" >✓</td>\n",
       "      <td id=\"T_1bd03_row36_col3\" class=\"data row36 col3\" >48</td>\n",
       "      <td id=\"T_1bd03_row36_col4\" class=\"data row36 col4\" >availability_schedule_name</td>\n",
       "      <td id=\"T_1bd03_row36_col5\" class=\"data row36 col5\" >DISCOVERED</td>\n",
       "      <td id=\"T_1bd03_row36_col6\" class=\"data row36 col6\" >N/A</td>\n",
       "      <td id=\"T_1bd03_row36_col7\" class=\"data row36 col7\" >str</td>\n",
       "      <td id=\"T_1bd03_row36_col8\" class=\"data row36 col8\" >✓</td>\n",
       "      <td id=\"T_1bd03_row36_col9\" class=\"data row36 col9\" >N/A</td>\n",
       "      <td id=\"T_1bd03_row36_col10\" class=\"data row36 col10\" >32.000000</td>\n",
       "      <td id=\"T_1bd03_row36_col11\" class=\"data row36 col11\" >2.000000</td>\n",
       "      <td id=\"T_1bd03_row36_col12\" class=\"data row36 col12\" >['VentSched_Twoandahalfstory_House', 'HVAC_Avail_Sched']</td>\n",
       "      <td id=\"T_1bd03_row36_col13\" class=\"data row36 col13\" >EXTRA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1bd03_level0_row37\" class=\"row_heading level0 row37\" >37</th>\n",
       "      <td id=\"T_1bd03_row37_col0\" class=\"data row37 col0\" >hvac_equipment.parquet</td>\n",
       "      <td id=\"T_1bd03_row37_col1\" class=\"data row37 col1\" >parsed_data/idf_data/by_category/hvac_equipment.parquet</td>\n",
       "      <td id=\"T_1bd03_row37_col2\" class=\"data row37 col2\" >✓</td>\n",
       "      <td id=\"T_1bd03_row37_col3\" class=\"data row37 col3\" >48</td>\n",
       "      <td id=\"T_1bd03_row37_col4\" class=\"data row37 col4\" >zone_supply_air_node_name</td>\n",
       "      <td id=\"T_1bd03_row37_col5\" class=\"data row37 col5\" >DISCOVERED</td>\n",
       "      <td id=\"T_1bd03_row37_col6\" class=\"data row37 col6\" >N/A</td>\n",
       "      <td id=\"T_1bd03_row37_col7\" class=\"data row37 col7\" >str</td>\n",
       "      <td id=\"T_1bd03_row37_col8\" class=\"data row37 col8\" >✓</td>\n",
       "      <td id=\"T_1bd03_row37_col9\" class=\"data row37 col9\" >N/A</td>\n",
       "      <td id=\"T_1bd03_row37_col10\" class=\"data row37 col10\" >32.000000</td>\n",
       "      <td id=\"T_1bd03_row37_col11\" class=\"data row37 col11\" >11.000000</td>\n",
       "      <td id=\"T_1bd03_row37_col12\" class=\"data row37 col12\" >['Zone1_FrontPerimeter_INLETS', 'Zone1_RightPerimeter_INLETS', 'Zone1_RearPerimeter_INLETS']</td>\n",
       "      <td id=\"T_1bd03_row37_col13\" class=\"data row37 col13\" >EXTRA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1bd03_level0_row38\" class=\"row_heading level0 row38\" >38</th>\n",
       "      <td id=\"T_1bd03_row38_col0\" class=\"data row38 col0\" >hvac_equipment.parquet</td>\n",
       "      <td id=\"T_1bd03_row38_col1\" class=\"data row38 col1\" >parsed_data/idf_data/by_category/hvac_equipment.parquet</td>\n",
       "      <td id=\"T_1bd03_row38_col2\" class=\"data row38 col2\" >✓</td>\n",
       "      <td id=\"T_1bd03_row38_col3\" class=\"data row38 col3\" >48</td>\n",
       "      <td id=\"T_1bd03_row38_col4\" class=\"data row38 col4\" >maximum_heating_supply_air_temperature</td>\n",
       "      <td id=\"T_1bd03_row38_col5\" class=\"data row38 col5\" >DISCOVERED</td>\n",
       "      <td id=\"T_1bd03_row38_col6\" class=\"data row38 col6\" >N/A</td>\n",
       "      <td id=\"T_1bd03_row38_col7\" class=\"data row38 col7\" >str</td>\n",
       "      <td id=\"T_1bd03_row38_col8\" class=\"data row38 col8\" >✓</td>\n",
       "      <td id=\"T_1bd03_row38_col9\" class=\"data row38 col9\" >N/A</td>\n",
       "      <td id=\"T_1bd03_row38_col10\" class=\"data row38 col10\" >32.000000</td>\n",
       "      <td id=\"T_1bd03_row38_col11\" class=\"data row38 col11\" >3.000000</td>\n",
       "      <td id=\"T_1bd03_row38_col12\" class=\"data row38 col12\" >['53.68235607082006', '66.71646764117767', '62.25159520578147']</td>\n",
       "      <td id=\"T_1bd03_row38_col13\" class=\"data row38 col13\" >EXTRA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1bd03_level0_row39\" class=\"row_heading level0 row39\" >39</th>\n",
       "      <td id=\"T_1bd03_row39_col0\" class=\"data row39 col0\" >hvac_equipment.parquet</td>\n",
       "      <td id=\"T_1bd03_row39_col1\" class=\"data row39 col1\" >parsed_data/idf_data/by_category/hvac_equipment.parquet</td>\n",
       "      <td id=\"T_1bd03_row39_col2\" class=\"data row39 col2\" >✓</td>\n",
       "      <td id=\"T_1bd03_row39_col3\" class=\"data row39 col3\" >48</td>\n",
       "      <td id=\"T_1bd03_row39_col4\" class=\"data row39 col4\" >maximum_heating_supply_air_temperature_numeric</td>\n",
       "      <td id=\"T_1bd03_row39_col5\" class=\"data row39 col5\" >DISCOVERED</td>\n",
       "      <td id=\"T_1bd03_row39_col6\" class=\"data row39 col6\" >N/A</td>\n",
       "      <td id=\"T_1bd03_row39_col7\" class=\"data row39 col7\" >float</td>\n",
       "      <td id=\"T_1bd03_row39_col8\" class=\"data row39 col8\" >✓</td>\n",
       "      <td id=\"T_1bd03_row39_col9\" class=\"data row39 col9\" >N/A</td>\n",
       "      <td id=\"T_1bd03_row39_col10\" class=\"data row39 col10\" >32.000000</td>\n",
       "      <td id=\"T_1bd03_row39_col11\" class=\"data row39 col11\" >3.000000</td>\n",
       "      <td id=\"T_1bd03_row39_col12\" class=\"data row39 col12\" >[53.68235607082006, 66.71646764117767, 62.25159520578147]</td>\n",
       "      <td id=\"T_1bd03_row39_col13\" class=\"data row39 col13\" >EXTRA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1bd03_level0_row40\" class=\"row_heading level0 row40\" >40</th>\n",
       "      <td id=\"T_1bd03_row40_col0\" class=\"data row40 col0\" >hvac_equipment.parquet</td>\n",
       "      <td id=\"T_1bd03_row40_col1\" class=\"data row40 col1\" >parsed_data/idf_data/by_category/hvac_equipment.parquet</td>\n",
       "      <td id=\"T_1bd03_row40_col2\" class=\"data row40 col2\" >✓</td>\n",
       "      <td id=\"T_1bd03_row40_col3\" class=\"data row40 col3\" >48</td>\n",
       "      <td id=\"T_1bd03_row40_col4\" class=\"data row40 col4\" >minimum_cooling_supply_air_temperature</td>\n",
       "      <td id=\"T_1bd03_row40_col5\" class=\"data row40 col5\" >DISCOVERED</td>\n",
       "      <td id=\"T_1bd03_row40_col6\" class=\"data row40 col6\" >N/A</td>\n",
       "      <td id=\"T_1bd03_row40_col7\" class=\"data row40 col7\" >str</td>\n",
       "      <td id=\"T_1bd03_row40_col8\" class=\"data row40 col8\" >✓</td>\n",
       "      <td id=\"T_1bd03_row40_col9\" class=\"data row40 col9\" >N/A</td>\n",
       "      <td id=\"T_1bd03_row40_col10\" class=\"data row40 col10\" >32.000000</td>\n",
       "      <td id=\"T_1bd03_row40_col11\" class=\"data row40 col11\" >3.000000</td>\n",
       "      <td id=\"T_1bd03_row40_col12\" class=\"data row40 col12\" >['13.353398974845822', '13.585902543310588', '12.058194506536221']</td>\n",
       "      <td id=\"T_1bd03_row40_col13\" class=\"data row40 col13\" >EXTRA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1bd03_level0_row41\" class=\"row_heading level0 row41\" >41</th>\n",
       "      <td id=\"T_1bd03_row41_col0\" class=\"data row41 col0\" >hvac_equipment.parquet</td>\n",
       "      <td id=\"T_1bd03_row41_col1\" class=\"data row41 col1\" >parsed_data/idf_data/by_category/hvac_equipment.parquet</td>\n",
       "      <td id=\"T_1bd03_row41_col2\" class=\"data row41 col2\" >✓</td>\n",
       "      <td id=\"T_1bd03_row41_col3\" class=\"data row41 col3\" >48</td>\n",
       "      <td id=\"T_1bd03_row41_col4\" class=\"data row41 col4\" >minimum_cooling_supply_air_temperature_numeric</td>\n",
       "      <td id=\"T_1bd03_row41_col5\" class=\"data row41 col5\" >DISCOVERED</td>\n",
       "      <td id=\"T_1bd03_row41_col6\" class=\"data row41 col6\" >N/A</td>\n",
       "      <td id=\"T_1bd03_row41_col7\" class=\"data row41 col7\" >float</td>\n",
       "      <td id=\"T_1bd03_row41_col8\" class=\"data row41 col8\" >✓</td>\n",
       "      <td id=\"T_1bd03_row41_col9\" class=\"data row41 col9\" >N/A</td>\n",
       "      <td id=\"T_1bd03_row41_col10\" class=\"data row41 col10\" >32.000000</td>\n",
       "      <td id=\"T_1bd03_row41_col11\" class=\"data row41 col11\" >3.000000</td>\n",
       "      <td id=\"T_1bd03_row41_col12\" class=\"data row41 col12\" >[13.353398974845822, 13.585902543310588, 12.058194506536221]</td>\n",
       "      <td id=\"T_1bd03_row41_col13\" class=\"data row41 col13\" >EXTRA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1bd03_level0_row42\" class=\"row_heading level0 row42\" >42</th>\n",
       "      <td id=\"T_1bd03_row42_col0\" class=\"data row42 col0\" >hvac_equipment.parquet</td>\n",
       "      <td id=\"T_1bd03_row42_col1\" class=\"data row42 col1\" >parsed_data/idf_data/by_category/hvac_equipment.parquet</td>\n",
       "      <td id=\"T_1bd03_row42_col2\" class=\"data row42 col2\" >✓</td>\n",
       "      <td id=\"T_1bd03_row42_col3\" class=\"data row42 col3\" >48</td>\n",
       "      <td id=\"T_1bd03_row42_col4\" class=\"data row42 col4\" >maximum_heating_supply_air_humidity_ratio</td>\n",
       "      <td id=\"T_1bd03_row42_col5\" class=\"data row42 col5\" >DISCOVERED</td>\n",
       "      <td id=\"T_1bd03_row42_col6\" class=\"data row42 col6\" >N/A</td>\n",
       "      <td id=\"T_1bd03_row42_col7\" class=\"data row42 col7\" >str</td>\n",
       "      <td id=\"T_1bd03_row42_col8\" class=\"data row42 col8\" >✓</td>\n",
       "      <td id=\"T_1bd03_row42_col9\" class=\"data row42 col9\" >N/A</td>\n",
       "      <td id=\"T_1bd03_row42_col10\" class=\"data row42 col10\" >32.000000</td>\n",
       "      <td id=\"T_1bd03_row42_col11\" class=\"data row42 col11\" >1.000000</td>\n",
       "      <td id=\"T_1bd03_row42_col12\" class=\"data row42 col12\" >['0.0156']</td>\n",
       "      <td id=\"T_1bd03_row42_col13\" class=\"data row42 col13\" >EXTRA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1bd03_level0_row43\" class=\"row_heading level0 row43\" >43</th>\n",
       "      <td id=\"T_1bd03_row43_col0\" class=\"data row43 col0\" >hvac_equipment.parquet</td>\n",
       "      <td id=\"T_1bd03_row43_col1\" class=\"data row43 col1\" >parsed_data/idf_data/by_category/hvac_equipment.parquet</td>\n",
       "      <td id=\"T_1bd03_row43_col2\" class=\"data row43 col2\" >✓</td>\n",
       "      <td id=\"T_1bd03_row43_col3\" class=\"data row43 col3\" >48</td>\n",
       "      <td id=\"T_1bd03_row43_col4\" class=\"data row43 col4\" >maximum_heating_supply_air_humidity_ratio_numeric</td>\n",
       "      <td id=\"T_1bd03_row43_col5\" class=\"data row43 col5\" >DISCOVERED</td>\n",
       "      <td id=\"T_1bd03_row43_col6\" class=\"data row43 col6\" >N/A</td>\n",
       "      <td id=\"T_1bd03_row43_col7\" class=\"data row43 col7\" >float</td>\n",
       "      <td id=\"T_1bd03_row43_col8\" class=\"data row43 col8\" >✓</td>\n",
       "      <td id=\"T_1bd03_row43_col9\" class=\"data row43 col9\" >N/A</td>\n",
       "      <td id=\"T_1bd03_row43_col10\" class=\"data row43 col10\" >32.000000</td>\n",
       "      <td id=\"T_1bd03_row43_col11\" class=\"data row43 col11\" >1.000000</td>\n",
       "      <td id=\"T_1bd03_row43_col12\" class=\"data row43 col12\" >[0.0156]</td>\n",
       "      <td id=\"T_1bd03_row43_col13\" class=\"data row43 col13\" >EXTRA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1bd03_level0_row44\" class=\"row_heading level0 row44\" >44</th>\n",
       "      <td id=\"T_1bd03_row44_col0\" class=\"data row44 col0\" >hvac_equipment.parquet</td>\n",
       "      <td id=\"T_1bd03_row44_col1\" class=\"data row44 col1\" >parsed_data/idf_data/by_category/hvac_equipment.parquet</td>\n",
       "      <td id=\"T_1bd03_row44_col2\" class=\"data row44 col2\" >✓</td>\n",
       "      <td id=\"T_1bd03_row44_col3\" class=\"data row44 col3\" >48</td>\n",
       "      <td id=\"T_1bd03_row44_col4\" class=\"data row44 col4\" >minimum_cooling_supply_air_humidity_ratio</td>\n",
       "      <td id=\"T_1bd03_row44_col5\" class=\"data row44 col5\" >DISCOVERED</td>\n",
       "      <td id=\"T_1bd03_row44_col6\" class=\"data row44 col6\" >N/A</td>\n",
       "      <td id=\"T_1bd03_row44_col7\" class=\"data row44 col7\" >str</td>\n",
       "      <td id=\"T_1bd03_row44_col8\" class=\"data row44 col8\" >✓</td>\n",
       "      <td id=\"T_1bd03_row44_col9\" class=\"data row44 col9\" >N/A</td>\n",
       "      <td id=\"T_1bd03_row44_col10\" class=\"data row44 col10\" >32.000000</td>\n",
       "      <td id=\"T_1bd03_row44_col11\" class=\"data row44 col11\" >1.000000</td>\n",
       "      <td id=\"T_1bd03_row44_col12\" class=\"data row44 col12\" >['0.0077']</td>\n",
       "      <td id=\"T_1bd03_row44_col13\" class=\"data row44 col13\" >EXTRA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1bd03_level0_row45\" class=\"row_heading level0 row45\" >45</th>\n",
       "      <td id=\"T_1bd03_row45_col0\" class=\"data row45 col0\" >hvac_equipment.parquet</td>\n",
       "      <td id=\"T_1bd03_row45_col1\" class=\"data row45 col1\" >parsed_data/idf_data/by_category/hvac_equipment.parquet</td>\n",
       "      <td id=\"T_1bd03_row45_col2\" class=\"data row45 col2\" >✓</td>\n",
       "      <td id=\"T_1bd03_row45_col3\" class=\"data row45 col3\" >48</td>\n",
       "      <td id=\"T_1bd03_row45_col4\" class=\"data row45 col4\" >minimum_cooling_supply_air_humidity_ratio_numeric</td>\n",
       "      <td id=\"T_1bd03_row45_col5\" class=\"data row45 col5\" >DISCOVERED</td>\n",
       "      <td id=\"T_1bd03_row45_col6\" class=\"data row45 col6\" >N/A</td>\n",
       "      <td id=\"T_1bd03_row45_col7\" class=\"data row45 col7\" >float</td>\n",
       "      <td id=\"T_1bd03_row45_col8\" class=\"data row45 col8\" >✓</td>\n",
       "      <td id=\"T_1bd03_row45_col9\" class=\"data row45 col9\" >N/A</td>\n",
       "      <td id=\"T_1bd03_row45_col10\" class=\"data row45 col10\" >32.000000</td>\n",
       "      <td id=\"T_1bd03_row45_col11\" class=\"data row45 col11\" >1.000000</td>\n",
       "      <td id=\"T_1bd03_row45_col12\" class=\"data row45 col12\" >[0.0077]</td>\n",
       "      <td id=\"T_1bd03_row45_col13\" class=\"data row45 col13\" >EXTRA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1bd03_level0_row46\" class=\"row_heading level0 row46\" >46</th>\n",
       "      <td id=\"T_1bd03_row46_col0\" class=\"data row46 col0\" >hvac_equipment.parquet</td>\n",
       "      <td id=\"T_1bd03_row46_col1\" class=\"data row46 col1\" >parsed_data/idf_data/by_category/hvac_equipment.parquet</td>\n",
       "      <td id=\"T_1bd03_row46_col2\" class=\"data row46 col2\" >✓</td>\n",
       "      <td id=\"T_1bd03_row46_col3\" class=\"data row46 col3\" >48</td>\n",
       "      <td id=\"T_1bd03_row46_col4\" class=\"data row46 col4\" >heating_limit</td>\n",
       "      <td id=\"T_1bd03_row46_col5\" class=\"data row46 col5\" >DISCOVERED</td>\n",
       "      <td id=\"T_1bd03_row46_col6\" class=\"data row46 col6\" >N/A</td>\n",
       "      <td id=\"T_1bd03_row46_col7\" class=\"data row46 col7\" >str</td>\n",
       "      <td id=\"T_1bd03_row46_col8\" class=\"data row46 col8\" >✓</td>\n",
       "      <td id=\"T_1bd03_row46_col9\" class=\"data row46 col9\" >N/A</td>\n",
       "      <td id=\"T_1bd03_row46_col10\" class=\"data row46 col10\" >32.000000</td>\n",
       "      <td id=\"T_1bd03_row46_col11\" class=\"data row46 col11\" >1.000000</td>\n",
       "      <td id=\"T_1bd03_row46_col12\" class=\"data row46 col12\" >['LimitFlowRateAndCapacity']</td>\n",
       "      <td id=\"T_1bd03_row46_col13\" class=\"data row46 col13\" >EXTRA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1bd03_level0_row47\" class=\"row_heading level0 row47\" >47</th>\n",
       "      <td id=\"T_1bd03_row47_col0\" class=\"data row47 col0\" >hvac_equipment.parquet</td>\n",
       "      <td id=\"T_1bd03_row47_col1\" class=\"data row47 col1\" >parsed_data/idf_data/by_category/hvac_equipment.parquet</td>\n",
       "      <td id=\"T_1bd03_row47_col2\" class=\"data row47 col2\" >✓</td>\n",
       "      <td id=\"T_1bd03_row47_col3\" class=\"data row47 col3\" >48</td>\n",
       "      <td id=\"T_1bd03_row47_col4\" class=\"data row47 col4\" >maximum_heating_air_flow_rate</td>\n",
       "      <td id=\"T_1bd03_row47_col5\" class=\"data row47 col5\" >DISCOVERED</td>\n",
       "      <td id=\"T_1bd03_row47_col6\" class=\"data row47 col6\" >N/A</td>\n",
       "      <td id=\"T_1bd03_row47_col7\" class=\"data row47 col7\" >str</td>\n",
       "      <td id=\"T_1bd03_row47_col8\" class=\"data row47 col8\" >✓</td>\n",
       "      <td id=\"T_1bd03_row47_col9\" class=\"data row47 col9\" >N/A</td>\n",
       "      <td id=\"T_1bd03_row47_col10\" class=\"data row47 col10\" >32.000000</td>\n",
       "      <td id=\"T_1bd03_row47_col11\" class=\"data row47 col11\" >1.000000</td>\n",
       "      <td id=\"T_1bd03_row47_col12\" class=\"data row47 col12\" >['Autosize']</td>\n",
       "      <td id=\"T_1bd03_row47_col13\" class=\"data row47 col13\" >EXTRA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1bd03_level0_row48\" class=\"row_heading level0 row48\" >48</th>\n",
       "      <td id=\"T_1bd03_row48_col0\" class=\"data row48 col0\" >hvac_equipment.parquet</td>\n",
       "      <td id=\"T_1bd03_row48_col1\" class=\"data row48 col1\" >parsed_data/idf_data/by_category/hvac_equipment.parquet</td>\n",
       "      <td id=\"T_1bd03_row48_col2\" class=\"data row48 col2\" >✓</td>\n",
       "      <td id=\"T_1bd03_row48_col3\" class=\"data row48 col3\" >48</td>\n",
       "      <td id=\"T_1bd03_row48_col4\" class=\"data row48 col4\" >maximum_sensible_heating_capacity</td>\n",
       "      <td id=\"T_1bd03_row48_col5\" class=\"data row48 col5\" >DISCOVERED</td>\n",
       "      <td id=\"T_1bd03_row48_col6\" class=\"data row48 col6\" >N/A</td>\n",
       "      <td id=\"T_1bd03_row48_col7\" class=\"data row48 col7\" >str</td>\n",
       "      <td id=\"T_1bd03_row48_col8\" class=\"data row48 col8\" >✓</td>\n",
       "      <td id=\"T_1bd03_row48_col9\" class=\"data row48 col9\" >N/A</td>\n",
       "      <td id=\"T_1bd03_row48_col10\" class=\"data row48 col10\" >32.000000</td>\n",
       "      <td id=\"T_1bd03_row48_col11\" class=\"data row48 col11\" >1.000000</td>\n",
       "      <td id=\"T_1bd03_row48_col12\" class=\"data row48 col12\" >['Autosize']</td>\n",
       "      <td id=\"T_1bd03_row48_col13\" class=\"data row48 col13\" >EXTRA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1bd03_level0_row49\" class=\"row_heading level0 row49\" >49</th>\n",
       "      <td id=\"T_1bd03_row49_col0\" class=\"data row49 col0\" >hvac_equipment.parquet</td>\n",
       "      <td id=\"T_1bd03_row49_col1\" class=\"data row49 col1\" >parsed_data/idf_data/by_category/hvac_equipment.parquet</td>\n",
       "      <td id=\"T_1bd03_row49_col2\" class=\"data row49 col2\" >✓</td>\n",
       "      <td id=\"T_1bd03_row49_col3\" class=\"data row49 col3\" >48</td>\n",
       "      <td id=\"T_1bd03_row49_col4\" class=\"data row49 col4\" >cooling_limit</td>\n",
       "      <td id=\"T_1bd03_row49_col5\" class=\"data row49 col5\" >DISCOVERED</td>\n",
       "      <td id=\"T_1bd03_row49_col6\" class=\"data row49 col6\" >N/A</td>\n",
       "      <td id=\"T_1bd03_row49_col7\" class=\"data row49 col7\" >str</td>\n",
       "      <td id=\"T_1bd03_row49_col8\" class=\"data row49 col8\" >✓</td>\n",
       "      <td id=\"T_1bd03_row49_col9\" class=\"data row49 col9\" >N/A</td>\n",
       "      <td id=\"T_1bd03_row49_col10\" class=\"data row49 col10\" >32.000000</td>\n",
       "      <td id=\"T_1bd03_row49_col11\" class=\"data row49 col11\" >1.000000</td>\n",
       "      <td id=\"T_1bd03_row49_col12\" class=\"data row49 col12\" >['LimitFlowRateAndCapacity']</td>\n",
       "      <td id=\"T_1bd03_row49_col13\" class=\"data row49 col13\" >EXTRA</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x2b30dc458b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "DISCOVERED COLUMNS DETAILS (First 20)\n",
      "================================================================================\n",
      "                File                                               Path                                                               Column_Name Data_Type  Non_Null_Count  Unique_Count  Null_Percentage                              Sample_Values      Mean           Std  Min       Max\n",
      "0   lighting.parquet  parsed_data/idf_data/by_category/lighting.parquet                                                               object_type       str               0             1              0.0                                     LIGHTS       NaN           NaN  NaN       NaN\n",
      "1   lighting.parquet  parsed_data/idf_data/by_category/lighting.parquet                                                                      name       str               0             1              0.0                           Lights_ALL_ZONES       NaN           NaN  NaN       NaN\n",
      "2   lighting.parquet  parsed_data/idf_data/by_category/lighting.parquet                                                     zone_or_zonelist_name       str               0             1              0.0                                  ALL_ZONES       NaN           NaN  NaN       NaN\n",
      "3   lighting.parquet  parsed_data/idf_data/by_category/lighting.parquet                                                             schedule_name       str               0             1              0.0                             LightsSchedule       NaN           NaN  NaN       NaN\n",
      "4   lighting.parquet  parsed_data/idf_data/by_category/lighting.parquet                                           design_level_calculation_method       str               0             1              0.0                                 Watts/Area       NaN           NaN  NaN       NaN\n",
      "5   lighting.parquet  parsed_data/idf_data/by_category/lighting.parquet                                                 watts_per_zone_floor_area       str               0             3              0.0    0, 4.092524414409712, 4.980486110210785       NaN           NaN  NaN       NaN\n",
      "6   lighting.parquet  parsed_data/idf_data/by_category/lighting.parquet                                         watts_per_zone_floor_area_numeric     float               0             3              0.0  0.0, 4.092524414409712, 4.980486110210785  3.024337  2.656516e+00  0.0  4.980486\n",
      "7   lighting.parquet  parsed_data/idf_data/by_category/lighting.parquet                                                       return_air_fraction       str               0             1              0.0                                        0.8       NaN           NaN  NaN       NaN\n",
      "8   lighting.parquet  parsed_data/idf_data/by_category/lighting.parquet                                               return_air_fraction_numeric     float               0             1              0.0                                        0.8  0.800000  1.359740e-16  0.8  0.800000\n",
      "9   lighting.parquet  parsed_data/idf_data/by_category/lighting.parquet                                                          fraction_radiant       str               0             1              0.0                                        0.1       NaN           NaN  NaN       NaN\n",
      "10  lighting.parquet  parsed_data/idf_data/by_category/lighting.parquet                                                  fraction_radiant_numeric     float               0             1              0.0                                        0.1  0.100000  1.699675e-17  0.1  0.100000\n",
      "11  lighting.parquet  parsed_data/idf_data/by_category/lighting.parquet                                                          fraction_visible       str               0             1              0.0                                        0.1       NaN           NaN  NaN       NaN\n",
      "12  lighting.parquet  parsed_data/idf_data/by_category/lighting.parquet                                                  fraction_visible_numeric     float               0             1              0.0                                        0.1  0.100000  1.699675e-17  0.1  0.100000\n",
      "13  lighting.parquet  parsed_data/idf_data/by_category/lighting.parquet                                                      fraction_replaceable       str               0             1              0.0                                          1       NaN           NaN  NaN       NaN\n",
      "14  lighting.parquet  parsed_data/idf_data/by_category/lighting.parquet                                              fraction_replaceable_numeric     float               0             1              0.0                                        1.0  1.000000  0.000000e+00  1.0  1.000000\n",
      "15  lighting.parquet  parsed_data/idf_data/by_category/lighting.parquet                                                       end_use_subcategory       str               0             1              0.0                                    General       NaN           NaN  NaN       NaN\n",
      "16  lighting.parquet  parsed_data/idf_data/by_category/lighting.parquet                    return_air_fraction_calculated_from_plenum_temperature       str               0             1              0.0                                         No       NaN           NaN  NaN       NaN\n",
      "17  lighting.parquet  parsed_data/idf_data/by_category/lighting.parquet          return_air_fraction_function_of_plenum_temperature_coefficient_1       str               0             1              0.0                                          0       NaN           NaN  NaN       NaN\n",
      "18  lighting.parquet  parsed_data/idf_data/by_category/lighting.parquet  return_air_fraction_function_of_plenum_temperature_coefficient_1_numeric     float               0             1              0.0                                        0.0  0.000000  0.000000e+00  0.0  0.000000\n",
      "19  lighting.parquet  parsed_data/idf_data/by_category/lighting.parquet          return_air_fraction_function_of_plenum_temperature_coefficient_2       str               0             1              0.0                                          0       NaN           NaN  NaN       NaN\n",
      "\n",
      "Results saved to: D:\\Documents\\daily\\E_Plus_2040_py\\output\\5f3924b1-189c-4c95-bf33-52d0602d79d9\\surrogate_data_check\n",
      "Files created:\n",
      "  - structure_check_details_20250626_221855.csv/xlsx\n",
      "  - structure_check_summary_20250626_221855.json\n",
      "  - discovered_columns_20250626_221855.csv/xlsx\n",
      "  - discovered_columns_full_20250626_221855.json\n",
      "  - structure_report_20250626_221855.md\n",
      "  - issues_only_20250626_221855.csv\n",
      "\n",
      "================================================================================\n",
      "FILES BY STATUS\n",
      "================================================================================\n",
      "Status\n",
      "EXTRA           12\n",
      "ISSUE            8\n",
      "MISSING FILE     1\n",
      "OK              12\n",
      "Name: Path, dtype: int64\n",
      "\n",
      "================================================================================\n",
      "COLUMN TYPE DISTRIBUTION\n",
      "================================================================================\n",
      "Column_Type\n",
      "DISCOVERED    169\n",
      "Expected       68\n",
      "N/A             1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Optional, Any\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "class SurrogateDataChecker:\n",
    "    def __init__(self, job_output_dir: str):\n",
    "        self.job_output_dir = Path(job_output_dir)\n",
    "        self.results = []\n",
    "        self.discovered_columns = {}  # Store discovered extra columns\n",
    "        \n",
    "        # Define expected structure with columns and types\n",
    "        self.expected_structure = {\n",
    "            # IDF Data\n",
    "            'parsed_data/idf_data/by_category/lighting.parquet': {\n",
    "                'columns': {\n",
    "                    'building_id': 'str',\n",
    "                    'zone_name': 'str',\n",
    "                    'object_name': 'str',\n",
    "                    'LightingLevel': 'float',\n",
    "                    'FractionRadiant': 'float',\n",
    "                    'FractionVisible': 'float',\n",
    "                    'ScheduleName': 'str'\n",
    "                }\n",
    "            },\n",
    "            'parsed_data/idf_data/by_category/hvac_equipment.parquet': {\n",
    "                'columns': {\n",
    "                    'building_id': 'str',\n",
    "                    'zone_name': 'str',\n",
    "                    'object_name': 'str',\n",
    "                    'object_type': 'str',\n",
    "                    'MaximumHeatingSupplyAirTemperature': 'float',\n",
    "                    'MinimumCoolingSupplyAirTemperature': 'float'\n",
    "                }\n",
    "            },\n",
    "            'parsed_data/idf_data/by_category/materials_materials.parquet': {\n",
    "                'columns': {\n",
    "                    'building_id': 'str',\n",
    "                    'material_name': 'str',\n",
    "                    'material_type': 'str',\n",
    "                    'Thickness': 'float',\n",
    "                    'Conductivity': 'float',\n",
    "                    'Density': 'float',\n",
    "                    'SpecificHeat': 'float'\n",
    "                }\n",
    "            },\n",
    "            'parsed_data/idf_data/by_category/infiltration.parquet': {\n",
    "                'columns': {\n",
    "                    'building_id': 'str',\n",
    "                    'zone_name': 'str',\n",
    "                    'object_name': 'str',\n",
    "                    'DesignFlowRateCalculationMethod': 'str',\n",
    "                    'DesignFlowRate': 'float',\n",
    "                    'FlowperZoneFloorArea': 'float'\n",
    "                }\n",
    "            },\n",
    "            'parsed_data/idf_data/by_category/ventilation.parquet': {\n",
    "                'columns': {\n",
    "                    'building_id': 'str',\n",
    "                    'zone_name': 'str',\n",
    "                    'object_name': 'str',\n",
    "                    'OutdoorAirMethod': 'str',\n",
    "                    'OutdoorAirFlowperPerson': 'float'\n",
    "                }\n",
    "            },\n",
    "            # SQL Results\n",
    "            'parsed_data/sql_results/timeseries/aggregated/daily/zones_daily.parquet': {\n",
    "                'columns': {\n",
    "                    'building_id': 'str',\n",
    "                    'Zone': 'str',\n",
    "                    'Variable': 'str',\n",
    "                    'Value': 'float',\n",
    "                    'DateTime': 'datetime'\n",
    "                }\n",
    "            },\n",
    "            'parsed_data/sql_results/timeseries/aggregated/daily/hvac_daily.parquet': {\n",
    "                'columns': {\n",
    "                    'building_id': 'str',\n",
    "                    'Zone': 'str',\n",
    "                    'Variable': 'str',\n",
    "                    'Value': 'float'\n",
    "                }\n",
    "            },\n",
    "            'parsed_data/sql_results/timeseries/aggregated/daily/ventilation_daily.parquet': {\n",
    "                'columns': {\n",
    "                    'building_id': 'str',\n",
    "                    'Zone': 'str',\n",
    "                    'Variable': 'str',\n",
    "                    'Value': 'float'\n",
    "                }\n",
    "            },\n",
    "            # Relationships\n",
    "            'parsed_data/relationships/zone_mappings.parquet': {\n",
    "                'columns': {\n",
    "                    'building_id': 'str',\n",
    "                    'zone_name': 'str',\n",
    "                    'sql_zone_name': 'str',\n",
    "                    'zone_index': 'int',\n",
    "                    'floor_area': 'float',\n",
    "                    'volume': 'float'\n",
    "                }\n",
    "            },\n",
    "            'parsed_data/metadata/building_registry.parquet': {\n",
    "                'columns': {\n",
    "                    'building_id': 'str',\n",
    "                    'building_type': 'str',\n",
    "                    'total_floor_area': 'float',\n",
    "                    'num_zones': 'int'\n",
    "                }\n",
    "            },\n",
    "            # Modified results\n",
    "            'parsed_modified_results/sql_results/timeseries/aggregated/daily/zones_daily.parquet': {\n",
    "                'columns': {\n",
    "                    'building_id': 'str',\n",
    "                    'variant_id': 'str',\n",
    "                    'Zone': 'str',\n",
    "                    'Variable': 'str',\n",
    "                    'Value': 'float'\n",
    "                }\n",
    "            },\n",
    "            # Modifications\n",
    "            'modified_idfs/modifications_detail_*.parquet': {\n",
    "                'columns': {\n",
    "                    'building_id': ['str', 'int'],\n",
    "                    'variant_id': 'str',\n",
    "                    'category': 'str',\n",
    "                    'object_type': 'str',\n",
    "                    'object_name': 'str',\n",
    "                    'field_name': 'str',\n",
    "                    'original_value': ['str', 'float'],\n",
    "                    'new_value': ['str', 'float'],\n",
    "                    'relative_change': 'float'\n",
    "                }\n",
    "            },\n",
    "            # Sensitivity\n",
    "            'sensitivity_results/sensitivity_for_surrogate.parquet': {\n",
    "                'columns': {\n",
    "                    'parameter': 'str',\n",
    "                    'sensitivity_score': 'float',\n",
    "                    'elasticity': 'float',\n",
    "                    'p_value': 'float'\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def convert_to_native_types(self, obj: Any) -> Any:\n",
    "        \"\"\"Convert numpy types to native Python types for JSON serialization.\"\"\"\n",
    "        if isinstance(obj, dict):\n",
    "            return {k: self.convert_to_native_types(v) for k, v in obj.items()}\n",
    "        elif isinstance(obj, list):\n",
    "            return [self.convert_to_native_types(item) for item in obj]\n",
    "        elif isinstance(obj, tuple):\n",
    "            return tuple(self.convert_to_native_types(item) for item in obj)\n",
    "        elif isinstance(obj, pd.Timestamp):\n",
    "            # Convert pandas Timestamp to ISO format string\n",
    "            return obj.isoformat()\n",
    "        elif isinstance(obj, datetime):\n",
    "            # Convert datetime to ISO format string\n",
    "            return obj.isoformat()\n",
    "        elif isinstance(obj, np.integer):\n",
    "            return int(obj)\n",
    "        elif isinstance(obj, np.floating):\n",
    "            if np.isnan(obj):\n",
    "                return None\n",
    "            return float(obj)\n",
    "        elif isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        elif pd.isna(obj):\n",
    "            return None\n",
    "        else:\n",
    "            return obj\n",
    "    \n",
    "    def check_file_exists(self, file_path: str) -> Tuple[bool, Optional[str]]:\n",
    "        \"\"\"Check if file exists and return its absolute path.\"\"\"\n",
    "        full_path = self.job_output_dir / file_path\n",
    "        \n",
    "        # Also check with wildcards for files like modifications_detail_*.parquet\n",
    "        if '*' in file_path:\n",
    "            parent = full_path.parent\n",
    "            pattern = full_path.name\n",
    "            if parent.exists():\n",
    "                matching_files = list(parent.glob(pattern))\n",
    "                if matching_files:\n",
    "                    return True, str(matching_files[0])\n",
    "            return False, None\n",
    "        \n",
    "        return full_path.exists(), str(full_path) if full_path.exists() else None\n",
    "    \n",
    "    def get_dtype_string(self, dtype) -> str:\n",
    "        \"\"\"Convert numpy/pandas dtype to simple string.\"\"\"\n",
    "        dtype_str = str(dtype)\n",
    "        \n",
    "        if 'int' in dtype_str:\n",
    "            return 'int'\n",
    "        elif 'float' in dtype_str:\n",
    "            return 'float'\n",
    "        elif 'object' in dtype_str or 'string' in dtype_str:\n",
    "            return 'str'\n",
    "        elif 'datetime' in dtype_str:\n",
    "            return 'datetime'\n",
    "        elif 'bool' in dtype_str:\n",
    "            return 'bool'\n",
    "        else:\n",
    "            return dtype_str\n",
    "    \n",
    "    def get_sample_values(self, series: pd.Series, n_samples: int = 5) -> List:\n",
    "        \"\"\"Get sample values from a series.\"\"\"\n",
    "        unique_vals = series.dropna().unique()\n",
    "        if len(unique_vals) <= n_samples:\n",
    "            return [self.convert_to_native_types(val) for val in unique_vals]\n",
    "        else:\n",
    "            # Get a mix of values\n",
    "            samples = []\n",
    "            if len(unique_vals) > 0:\n",
    "                # Add min/max for numeric\n",
    "                if pd.api.types.is_numeric_dtype(series):\n",
    "                    samples.append(self.convert_to_native_types(series.min()))\n",
    "                    samples.append(self.convert_to_native_types(series.max()))\n",
    "                    # Add some random samples\n",
    "                    remaining = n_samples - 2\n",
    "                    if remaining > 0:\n",
    "                        random_samples = series.dropna().sample(n=min(remaining, len(series))).tolist()\n",
    "                        samples.extend([self.convert_to_native_types(val) for val in random_samples[:remaining]])\n",
    "                else:\n",
    "                    # For non-numeric, just take first n_samples\n",
    "                    samples = [self.convert_to_native_types(val) for val in unique_vals[:n_samples]]\n",
    "            return samples\n",
    "    \n",
    "    def check_parquet_file(self, file_path: str, expected_columns: Dict[str, any]) -> Dict:\n",
    "        \"\"\"Check a parquet file's structure.\"\"\"\n",
    "        exists, abs_path = self.check_file_exists(file_path)\n",
    "        \n",
    "        result = {\n",
    "            'file': file_path,\n",
    "            'exists': exists,\n",
    "            'path': abs_path,\n",
    "            'row_count': 0,\n",
    "            'columns': {},\n",
    "            'extra_columns': {}  # Store discovered columns\n",
    "        }\n",
    "        \n",
    "        if exists and abs_path:\n",
    "            try:\n",
    "                # Read parquet file\n",
    "                df = pd.read_parquet(abs_path)\n",
    "                result['row_count'] = len(df)\n",
    "                \n",
    "                # Get actual columns and types\n",
    "                actual_columns = {}\n",
    "                for col in df.columns:\n",
    "                    actual_columns[col] = self.get_dtype_string(df[col].dtype)\n",
    "                \n",
    "                # Store discovered columns info for this file\n",
    "                if file_path not in self.discovered_columns:\n",
    "                    self.discovered_columns[file_path] = {}\n",
    "                \n",
    "                # Compare with expected\n",
    "                for exp_col, exp_type in expected_columns.items():\n",
    "                    if exp_col in actual_columns:\n",
    "                        actual_type = actual_columns[exp_col]\n",
    "                        \n",
    "                        # Handle multiple expected types\n",
    "                        if isinstance(exp_type, list):\n",
    "                            type_match = actual_type in exp_type\n",
    "                        else:\n",
    "                            type_match = (actual_type == exp_type) or \\\n",
    "                                       (exp_type == 'str' and actual_type == 'object')\n",
    "                        \n",
    "                        result['columns'][exp_col] = {\n",
    "                            'expected_type': exp_type,\n",
    "                            'actual_type': actual_type,\n",
    "                            'exists': True,\n",
    "                            'type_match': type_match,\n",
    "                            'null_count': int(df[exp_col].isnull().sum()),\n",
    "                            'unique_count': int(df[exp_col].nunique()),\n",
    "                            'sample_values': self.get_sample_values(df[exp_col])\n",
    "                        }\n",
    "                    else:\n",
    "                        result['columns'][exp_col] = {\n",
    "                            'expected_type': exp_type,\n",
    "                            'actual_type': None,\n",
    "                            'exists': False,\n",
    "                            'type_match': False,\n",
    "                            'null_count': None,\n",
    "                            'unique_count': None,\n",
    "                            'sample_values': []\n",
    "                        }\n",
    "                \n",
    "                # Add unexpected columns (discovered extras)\n",
    "                for col in actual_columns:\n",
    "                    if col not in expected_columns:\n",
    "                        result['extra_columns'][col] = {\n",
    "                            'actual_type': actual_columns[col],\n",
    "                            'null_count': int(df[col].isnull().sum()),\n",
    "                            'unique_count': int(df[col].nunique()),\n",
    "                            'sample_values': self.get_sample_values(df[col]),\n",
    "                            'stats': self.get_column_stats(df[col])\n",
    "                        }\n",
    "                        # Store in discovered columns\n",
    "                        self.discovered_columns[file_path][col] = result['extra_columns'][col]\n",
    "                \n",
    "            except Exception as e:\n",
    "                result['error'] = str(e)\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def get_column_stats(self, series: pd.Series) -> Dict:\n",
    "        \"\"\"Get statistics for a column.\"\"\"\n",
    "        stats = {\n",
    "            'total_count': int(len(series)),\n",
    "            'non_null_count': int(series.count()),\n",
    "            'null_percentage': float((series.isnull().sum() / len(series) * 100) if len(series) > 0 else 0)\n",
    "        }\n",
    "        \n",
    "        if pd.api.types.is_numeric_dtype(series):\n",
    "            # Handle potential NaN values\n",
    "            if series.count() > 0:  # If there are non-null values\n",
    "                stats.update({\n",
    "                    'mean': self.convert_to_native_types(series.mean()),\n",
    "                    'std': self.convert_to_native_types(series.std()),\n",
    "                    'min': self.convert_to_native_types(series.min()),\n",
    "                    'max': self.convert_to_native_types(series.max()),\n",
    "                    'q25': self.convert_to_native_types(series.quantile(0.25)),\n",
    "                    'q50': self.convert_to_native_types(series.quantile(0.50)),\n",
    "                    'q75': self.convert_to_native_types(series.quantile(0.75))\n",
    "                })\n",
    "            else:\n",
    "                # All values are null\n",
    "                stats.update({\n",
    "                    'mean': None,\n",
    "                    'std': None,\n",
    "                    'min': None,\n",
    "                    'max': None,\n",
    "                    'q25': None,\n",
    "                    'q50': None,\n",
    "                    'q75': None\n",
    "                })\n",
    "        elif pd.api.types.is_string_dtype(series) or series.dtype == 'object':\n",
    "            value_counts = series.value_counts()\n",
    "            stats.update({\n",
    "                'unique_values': int(len(value_counts)),\n",
    "                'most_common': {str(k): int(v) for k, v in value_counts.head(5).items()} if len(value_counts) > 0 else {}\n",
    "            })\n",
    "        \n",
    "        return stats\n",
    "    \n",
    "    def check_all_files(self) -> pd.DataFrame:\n",
    "        \"\"\"Check all expected files and create comparison table.\"\"\"\n",
    "        print(f\"Checking surrogate data structure in: {self.job_output_dir}\\n\")\n",
    "        \n",
    "        all_results = []\n",
    "        \n",
    "        for file_path, expected_info in self.expected_structure.items():\n",
    "            print(f\"Checking: {file_path}\")\n",
    "            result = self.check_parquet_file(file_path, expected_info['columns'])\n",
    "            \n",
    "            # Create rows for expected columns\n",
    "            if result['exists']:\n",
    "                for col_name, col_info in result['columns'].items():\n",
    "                    all_results.append({\n",
    "                        'File': file_path.split('/')[-1],\n",
    "                        'Path': file_path,\n",
    "                        'File_Exists': '✓',\n",
    "                        'Rows': result['row_count'],\n",
    "                        'Column': col_name,\n",
    "                        'Column_Type': 'Expected',\n",
    "                        'Expected_Type': col_info['expected_type'],\n",
    "                        'Actual_Type': col_info['actual_type'],\n",
    "                        'Column_Exists': '✓' if col_info['exists'] else '✗',\n",
    "                        'Type_Match': '✓' if col_info['type_match'] else '✗',\n",
    "                        'Null_Count': col_info['null_count'],\n",
    "                        'Unique_Values': col_info['unique_count'],\n",
    "                        'Sample_Values': str(col_info['sample_values'][:3]) if col_info['sample_values'] else 'N/A',\n",
    "                        'Status': 'OK' if col_info['exists'] and col_info['type_match'] else 'ISSUE'\n",
    "                    })\n",
    "                \n",
    "                # Add rows for extra discovered columns\n",
    "                for col_name, col_info in result['extra_columns'].items():\n",
    "                    all_results.append({\n",
    "                        'File': file_path.split('/')[-1],\n",
    "                        'Path': file_path,\n",
    "                        'File_Exists': '✓',\n",
    "                        'Rows': result['row_count'],\n",
    "                        'Column': col_name,\n",
    "                        'Column_Type': 'DISCOVERED',\n",
    "                        'Expected_Type': 'N/A',\n",
    "                        'Actual_Type': col_info['actual_type'],\n",
    "                        'Column_Exists': '✓',\n",
    "                        'Type_Match': 'N/A',\n",
    "                        'Null_Count': col_info['null_count'],\n",
    "                        'Unique_Values': col_info['unique_count'],\n",
    "                        'Sample_Values': str(col_info['sample_values'][:3]) if col_info['sample_values'] else 'N/A',\n",
    "                        'Status': 'EXTRA'\n",
    "                    })\n",
    "            else:\n",
    "                all_results.append({\n",
    "                    'File': file_path.split('/')[-1],\n",
    "                    'Path': file_path,\n",
    "                    'File_Exists': '✗',\n",
    "                    'Rows': 0,\n",
    "                    'Column': 'N/A',\n",
    "                    'Column_Type': 'N/A',\n",
    "                    'Expected_Type': 'N/A',\n",
    "                    'Actual_Type': 'N/A',\n",
    "                    'Column_Exists': 'N/A',\n",
    "                    'Type_Match': 'N/A',\n",
    "                    'Null_Count': None,\n",
    "                    'Unique_Values': None,\n",
    "                    'Sample_Values': 'N/A',\n",
    "                    'Status': 'MISSING FILE'\n",
    "                })\n",
    "        \n",
    "        return pd.DataFrame(all_results)\n",
    "    \n",
    "    def create_discovered_columns_report(self) -> pd.DataFrame:\n",
    "        \"\"\"Create a detailed report of all discovered extra columns.\"\"\"\n",
    "        discovered_data = []\n",
    "        \n",
    "        for file_path, columns in self.discovered_columns.items():\n",
    "            for col_name, col_info in columns.items():\n",
    "                row = {\n",
    "                    'File': file_path.split('/')[-1],\n",
    "                    'Path': file_path,\n",
    "                    'Column_Name': col_name,\n",
    "                    'Data_Type': col_info['actual_type'],\n",
    "                    'Non_Null_Count': col_info['null_count'],\n",
    "                    'Unique_Count': col_info['unique_count'],\n",
    "                    'Null_Percentage': col_info['stats']['null_percentage'],\n",
    "                    'Sample_Values': ', '.join(map(str, col_info['sample_values'][:5]))\n",
    "                }\n",
    "                \n",
    "                # Add numeric stats if available\n",
    "                if 'mean' in col_info['stats']:\n",
    "                    row.update({\n",
    "                        'Mean': col_info['stats']['mean'],\n",
    "                        'Std': col_info['stats']['std'],\n",
    "                        'Min': col_info['stats']['min'],\n",
    "                        'Max': col_info['stats']['max']\n",
    "                    })\n",
    "                \n",
    "                discovered_data.append(row)\n",
    "        \n",
    "        return pd.DataFrame(discovered_data)\n",
    "    \n",
    "    def create_summary_report(self, df_results: pd.DataFrame) -> Dict:\n",
    "        \"\"\"Create a summary report of the check results.\"\"\"\n",
    "        summary = {\n",
    "            'check_timestamp': datetime.now().isoformat(),\n",
    "            'job_output_dir': str(self.job_output_dir),\n",
    "            'total_files_expected': len(self.expected_structure),\n",
    "            'files_found': len(df_results[df_results['File_Exists'] == '✓']['Path'].unique()),\n",
    "            'files_missing': len(df_results[df_results['File_Exists'] == '✗']['Path'].unique()),\n",
    "            'total_expected_columns': len(df_results[df_results['Column_Type'] == 'Expected']),\n",
    "            'expected_columns_found': len(df_results[(df_results['Column_Type'] == 'Expected') & \n",
    "                                                    (df_results['Column_Exists'] == '✓')]),\n",
    "            'total_discovered_columns': len(df_results[df_results['Column_Type'] == 'DISCOVERED']),\n",
    "            'total_issues': len(df_results[df_results['Status'].isin(['ISSUE', 'MISSING FILE'])]),\n",
    "            'missing_files': [],\n",
    "            'column_issues': [],\n",
    "            'type_mismatches': [],\n",
    "            'discovered_extras_summary': {}\n",
    "        }\n",
    "        \n",
    "        # Get missing files\n",
    "        missing_files = df_results[df_results['File_Exists'] == '✗']['Path'].unique()\n",
    "        summary['missing_files'] = list(missing_files)\n",
    "        \n",
    "        # Get column issues\n",
    "        column_issues = df_results[(df_results['Column_Exists'] == '✗') & \n",
    "                                 (df_results['File_Exists'] == '✓') & \n",
    "                                 (df_results['Column_Type'] == 'Expected')]\n",
    "        for _, row in column_issues.iterrows():\n",
    "            summary['column_issues'].append(f\"{row['File']}: {row['Column']}\")\n",
    "        \n",
    "        # Get type mismatches\n",
    "        type_issues = df_results[(df_results['Type_Match'] == '✗') & \n",
    "                               (df_results['Column_Exists'] == '✓') & \n",
    "                               (df_results['Column_Type'] == 'Expected')]\n",
    "        for _, row in type_issues.iterrows():\n",
    "            summary['type_mismatches'].append(\n",
    "                f\"{row['File']}: {row['Column']} (expected {row['Expected_Type']}, got {row['Actual_Type']})\"\n",
    "            )\n",
    "        \n",
    "        # Summary of discovered columns by file\n",
    "        discovered_by_file = df_results[df_results['Column_Type'] == 'DISCOVERED'].groupby('File')['Column'].count()\n",
    "        summary['discovered_extras_summary'] = discovered_by_file.to_dict()\n",
    "        \n",
    "        return summary\n",
    "    \n",
    "    def print_colored_results(self, df_results: pd.DataFrame, summary: Dict):\n",
    "        \"\"\"Print results with color coding (if in Jupyter/IPython).\"\"\"\n",
    "        try:\n",
    "            from IPython.display import display, HTML\n",
    "            \n",
    "            # Style the dataframe\n",
    "            def style_status(val):\n",
    "                if val == 'OK':\n",
    "                    return 'background-color: #90EE90'\n",
    "                elif val == 'ISSUE':\n",
    "                    return 'background-color: #FFB6C1'\n",
    "                elif val == 'MISSING FILE':\n",
    "                    return 'background-color: #FF6B6B'\n",
    "                elif val == 'EXTRA':\n",
    "                    return 'background-color: #87CEEB'\n",
    "                return ''\n",
    "            \n",
    "            def style_check(val):\n",
    "                if val == '✓':\n",
    "                    return 'color: green; font-weight: bold'\n",
    "                elif val == '✗':\n",
    "                    return 'color: red; font-weight: bold'\n",
    "                return ''\n",
    "            \n",
    "            def style_column_type(val):\n",
    "                if val == 'DISCOVERED':\n",
    "                    return 'color: blue; font-weight: bold'\n",
    "                return ''\n",
    "            \n",
    "            styled_df = df_results.style.applymap(style_status, subset=['Status'])\\\n",
    "                                       .applymap(style_check, subset=['File_Exists', 'Column_Exists', 'Type_Match'])\\\n",
    "                                       .applymap(style_column_type, subset=['Column_Type'])\n",
    "            \n",
    "            display(styled_df)\n",
    "            \n",
    "        except ImportError:\n",
    "            # Fallback to regular print\n",
    "            print(df_results.to_string())\n",
    "    \n",
    "    def save_results(self, df_results: pd.DataFrame, summary: Dict):\n",
    "        \"\"\"Save all results to files.\"\"\"\n",
    "        output_dir = self.job_output_dir / 'surrogate_data_check'\n",
    "        output_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        # Create timestamp for filenames\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        \n",
    "        # 1. Save detailed results\n",
    "        df_results.to_csv(output_dir / f'structure_check_details_{timestamp}.csv', index=False)\n",
    "        df_results.to_excel(output_dir / f'structure_check_details_{timestamp}.xlsx', index=False)\n",
    "        \n",
    "        # 2. Save summary\n",
    "        with open(output_dir / f'structure_check_summary_{timestamp}.json', 'w') as f:\n",
    "            json.dump(summary, f, indent=2)\n",
    "        \n",
    "        # 3. Save discovered columns detailed report\n",
    "        df_discovered = self.create_discovered_columns_report()\n",
    "        if not df_discovered.empty:\n",
    "            df_discovered.to_csv(output_dir / f'discovered_columns_{timestamp}.csv', index=False)\n",
    "            df_discovered.to_excel(output_dir / f'discovered_columns_{timestamp}.xlsx', index=False)\n",
    "        \n",
    "        # 4. Save discovered columns with full statistics - convert numpy types first\n",
    "        discovered_columns_native = self.convert_to_native_types(self.discovered_columns)\n",
    "        with open(output_dir / f'discovered_columns_full_{timestamp}.json', 'w') as f:\n",
    "            json.dump(discovered_columns_native, f, indent=2)\n",
    "        \n",
    "        # 5. Create a markdown report\n",
    "        self.create_markdown_report(output_dir / f'structure_report_{timestamp}.md', df_results, summary)\n",
    "        \n",
    "        # 6. Save only problematic entries for quick review\n",
    "        df_issues = df_results[df_results['Status'].isin(['ISSUE', 'MISSING FILE'])]\n",
    "        if not df_issues.empty:\n",
    "            df_issues.to_csv(output_dir / f'issues_only_{timestamp}.csv', index=False)\n",
    "        \n",
    "        print(f\"\\nResults saved to: {output_dir}\")\n",
    "        print(f\"Files created:\")\n",
    "        print(f\"  - structure_check_details_{timestamp}.csv/xlsx\")\n",
    "        print(f\"  - structure_check_summary_{timestamp}.json\")\n",
    "        print(f\"  - discovered_columns_{timestamp}.csv/xlsx\")\n",
    "        print(f\"  - discovered_columns_full_{timestamp}.json\")\n",
    "        print(f\"  - structure_report_{timestamp}.md\")\n",
    "        if not df_issues.empty:\n",
    "            print(f\"  - issues_only_{timestamp}.csv\")\n",
    "    \n",
    "    def create_markdown_report(self, output_path: Path, df_results: pd.DataFrame, summary: Dict):\n",
    "        \"\"\"Create a markdown report for easy reading.\"\"\"\n",
    "        with open(output_path, 'w') as f:\n",
    "            f.write(f\"# Surrogate Data Structure Check Report\\n\\n\")\n",
    "            f.write(f\"**Generated:** {summary['check_timestamp']}\\n\\n\")\n",
    "            f.write(f\"**Directory:** `{summary['job_output_dir']}`\\n\\n\")\n",
    "            \n",
    "            f.write(\"## Summary\\n\\n\")\n",
    "            f.write(f\"- **Files Expected:** {summary['total_files_expected']}\\n\")\n",
    "            f.write(f\"- **Files Found:** {summary['files_found']}\\n\")\n",
    "            f.write(f\"- **Files Missing:** {summary['files_missing']}\\n\")\n",
    "            f.write(f\"- **Expected Columns:** {summary['total_expected_columns']}\\n\")\n",
    "            f.write(f\"- **Expected Columns Found:** {summary['expected_columns_found']}\\n\")\n",
    "            f.write(f\"- **Extra Columns Discovered:** {summary['total_discovered_columns']}\\n\")\n",
    "            f.write(f\"- **Total Issues:** {summary['total_issues']}\\n\\n\")\n",
    "            \n",
    "            if summary['missing_files']:\n",
    "                f.write(\"## Missing Files\\n\\n\")\n",
    "                for file in summary['missing_files']:\n",
    "                    f.write(f\"- `{file}`\\n\")\n",
    "                f.write(\"\\n\")\n",
    "            \n",
    "            if summary['discovered_extras_summary']:\n",
    "                f.write(\"## Discovered Extra Columns Summary\\n\\n\")\n",
    "                f.write(\"| File | Extra Columns Count |\\n\")\n",
    "                f.write(\"|------|--------------------|\\n\")\n",
    "                for file, count in summary['discovered_extras_summary'].items():\n",
    "                    f.write(f\"| {file} | {count} |\\n\")\n",
    "                f.write(\"\\n\")\n",
    "            \n",
    "            # Add detailed discovered columns\n",
    "            df_discovered = self.create_discovered_columns_report()\n",
    "            if not df_discovered.empty:\n",
    "                f.write(\"## Discovered Columns Details\\n\\n\")\n",
    "                f.write(df_discovered.to_markdown(index=False))\n",
    "                f.write(\"\\n\")\n",
    "\n",
    "# Usage\n",
    "def check_surrogate_data_structure(job_output_dir: str):\n",
    "    \"\"\"Main function to check surrogate data structure.\"\"\"\n",
    "    checker = SurrogateDataChecker(job_output_dir)\n",
    "    \n",
    "    # Run checks\n",
    "    df_results = checker.check_all_files()\n",
    "    summary = checker.create_summary_report(df_results)\n",
    "    \n",
    "    # Print summary\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"SUMMARY\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Check performed at: {summary['check_timestamp']}\")\n",
    "    print(f\"Total files expected: {summary['total_files_expected']}\")\n",
    "    print(f\"Files found: {summary['files_found']}\")\n",
    "    print(f\"Files missing: {summary['files_missing']}\")\n",
    "    print(f\"Expected columns total: {summary['total_expected_columns']}\")\n",
    "    print(f\"Expected columns found: {summary['expected_columns_found']}\")\n",
    "    print(f\"Extra columns discovered: {summary['total_discovered_columns']}\")\n",
    "    print(f\"Total issues: {summary['total_issues']}\")\n",
    "    \n",
    "    if summary['missing_files']:\n",
    "        print(f\"\\nMISSING FILES: {len(summary['missing_files'])}\")\n",
    "        for f in summary['missing_files'][:5]:  # Show first 5\n",
    "            print(f\"  - {f}\")\n",
    "        if len(summary['missing_files']) > 5:\n",
    "            print(f\"  ... and {len(summary['missing_files']) - 5} more\")\n",
    "    \n",
    "    if summary['discovered_extras_summary']:\n",
    "        print(f\"\\nDISCOVERED EXTRA COLUMNS BY FILE:\")\n",
    "        for file, count in list(summary['discovered_extras_summary'].items())[:10]:\n",
    "            print(f\"  - {file}: {count} extra columns\")\n",
    "    \n",
    "    # Display results\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"DETAILED RESULTS (First 50 rows)\")\n",
    "    print(\"=\"*80)\n",
    "    checker.print_colored_results(df_results.head(50), summary)\n",
    "    \n",
    "    # Show discovered columns\n",
    "    df_discovered = checker.create_discovered_columns_report()\n",
    "    if not df_discovered.empty:\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"DISCOVERED COLUMNS DETAILS (First 20)\")\n",
    "        print(\"=\"*80)\n",
    "        print(df_discovered.head(20).to_string())\n",
    "    \n",
    "    # Save results\n",
    "    checker.save_results(df_results, summary)\n",
    "    \n",
    "    return df_results, summary, df_discovered\n",
    "\n",
    "# Run the check\n",
    "if __name__ == \"__main__\":\n",
    "    job_dir = r\"D:\\Documents\\daily\\E_Plus_2040_py\\output\\5f3924b1-189c-4c95-bf33-52d0602d79d9\"\n",
    "    df_results, summary, df_discovered = check_surrogate_data_structure(job_dir)\n",
    "    \n",
    "    # Additional analysis - show files by status\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"FILES BY STATUS\")\n",
    "    print(\"=\"*80)\n",
    "    status_summary = df_results.groupby('Status')['Path'].nunique()\n",
    "    print(status_summary)\n",
    "    \n",
    "    # Show column type distribution\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"COLUMN TYPE DISTRIBUTION\")\n",
    "    print(\"=\"*80)\n",
    "    column_dist = df_results['Column_Type'].value_counts()\n",
    "    print(column_dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa49daf",
   "metadata": {},
   "source": [
    "### V3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "caaf8fae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking surrogate data structure in: D:\\Documents\\daily\\E_Plus_2040_py\\output\\82e2b83f-5013-4270-8266-a37a67dbd4ff\n",
      "\n",
      "Checking: parsed_data/idf_data/by_category/lighting.parquet\n",
      "Checking: parsed_data/idf_data/by_category/hvac_equipment.parquet\n",
      "Checking: parsed_data/idf_data/by_category/materials_materials.parquet\n",
      "Checking: parsed_data/idf_data/by_category/infiltration.parquet\n",
      "Checking: parsed_data/idf_data/by_category/ventilation.parquet\n",
      "Checking: parsed_data/idf_data/by_category/equipment.parquet\n",
      "Checking: parsed_data/idf_data/by_category/dhw.parquet\n",
      "Checking: parsed_data/idf_data/by_category/shading.parquet\n",
      "Checking: parsed_data/sql_results/timeseries/aggregated/daily/zones_daily.parquet\n",
      "Checking: parsed_data/sql_results/timeseries/aggregated/daily/hvac_daily.parquet\n",
      "Checking: parsed_data/sql_results/timeseries/aggregated/daily/ventilation_daily.parquet\n",
      "Checking: parsed_data/sql_results/timeseries/aggregated/hourly/zones_hourly.parquet\n",
      "Checking: parsed_data/sql_results/timeseries/aggregated/monthly/zones_monthly.parquet\n",
      "Checking: parsed_data/relationships/zone_mappings.parquet\n",
      "Checking: parsed_data/relationships/equipment_assignments.parquet\n",
      "Checking: parsed_data/metadata/building_registry.parquet\n",
      "Checking: parsed_modified_results/sql_results/timeseries/aggregated/daily/zones_daily.parquet\n",
      "Checking: parsed_modified_results/sql_results/timeseries/aggregated/daily/hvac_daily.parquet\n",
      "Checking: parsed_modified_results/sql_results/timeseries/aggregated/daily/ventilation_daily.parquet\n",
      "Checking: parsed_modified_results/idf_data/by_category/lighting.parquet\n",
      "Checking: modified_idfs/modifications_detail_*.parquet\n",
      "Checking: sensitivity_results/sensitivity_for_surrogate.parquet\n",
      "Checking: sensitivity_results/modification_sensitivity_results.parquet\n",
      "Checking: validation_results/validation_summary.parquet\n",
      "Checking: surrogate_pipeline_export/*/1_inputs/extracted_modifications.parquet\n",
      "Checking: surrogate_pipeline_export/*/3_preprocessing/preprocessed_features.parquet\n",
      "Checking: surrogate_pipeline_export/*/3_preprocessing/preprocessed_targets.parquet\n",
      "\n",
      "================================================================================\n",
      "SUMMARY\n",
      "================================================================================\n",
      "Check performed at: 2025-06-27T05:19:55.802811\n",
      "Total files expected: 27\n",
      "Files found: 20\n",
      "Files missing: 7\n",
      "Expected columns total: 115\n",
      "Expected columns found: 97\n",
      "Extra columns discovered: 260\n",
      "Total issues: 43\n",
      "\n",
      "DATA PIPELINE READINESS:\n",
      "  - Base Outputs: ✓\n",
      "  - Modified Outputs: ✓\n",
      "  - Modifications: ✓\n",
      "  - Sensitivity Analysis: ✗\n",
      "  - Zone Mappings: ✓\n",
      "  - Ready for Surrogate: ✓ YES\n",
      "\n",
      "MISSING FILES: 7\n",
      "  - parsed_data/sql_results/timeseries/aggregated/hourly/zones_hourly.parquet\n",
      "  - sensitivity_results/sensitivity_for_surrogate.parquet\n",
      "  - sensitivity_results/modification_sensitivity_results.parquet\n",
      "  - validation_results/validation_summary.parquet\n",
      "  - surrogate_pipeline_export/*/1_inputs/extracted_modifications.parquet\n",
      "  ... and 2 more\n",
      "\n",
      "DISCOVERED EXTRA COLUMNS BY FILE:\n",
      "  - building_registry.parquet: 9 extra columns\n",
      "  - dhw.parquet: 50 extra columns\n",
      "  - equipment.parquet: 13 extra columns\n",
      "  - equipment_assignments.parquet: 2 extra columns\n",
      "  - hvac_daily.parquet: 5 extra columns\n",
      "  - hvac_equipment.parquet: 36 extra columns\n",
      "  - infiltration.parquet: 14 extra columns\n",
      "  - lighting.parquet: 36 extra columns\n",
      "  - materials_materials.parquet: 15 extra columns\n",
      "  - modifications_detail_*.parquet: 2 extra columns\n",
      "\n",
      "================================================================================\n",
      "DETAILED RESULTS (First 50 rows)\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_223ff_row0_col2, #T_223ff_row0_col8, #T_223ff_row0_col9, #T_223ff_row1_col2, #T_223ff_row1_col8, #T_223ff_row1_col9, #T_223ff_row2_col2, #T_223ff_row2_col8, #T_223ff_row2_col9, #T_223ff_row3_col2, #T_223ff_row3_col8, #T_223ff_row4_col2, #T_223ff_row4_col8, #T_223ff_row5_col2, #T_223ff_row5_col8, #T_223ff_row6_col2, #T_223ff_row6_col8, #T_223ff_row6_col9, #T_223ff_row7_col2, #T_223ff_row7_col8, #T_223ff_row8_col2, #T_223ff_row8_col8, #T_223ff_row9_col2, #T_223ff_row9_col8, #T_223ff_row10_col2, #T_223ff_row10_col8, #T_223ff_row11_col2, #T_223ff_row11_col8, #T_223ff_row12_col2, #T_223ff_row12_col8, #T_223ff_row13_col2, #T_223ff_row13_col8, #T_223ff_row14_col2, #T_223ff_row14_col8, #T_223ff_row15_col2, #T_223ff_row15_col8, #T_223ff_row16_col2, #T_223ff_row16_col8, #T_223ff_row17_col2, #T_223ff_row17_col8, #T_223ff_row18_col2, #T_223ff_row18_col8, #T_223ff_row19_col2, #T_223ff_row19_col8, #T_223ff_row20_col2, #T_223ff_row20_col8, #T_223ff_row21_col2, #T_223ff_row21_col8, #T_223ff_row22_col2, #T_223ff_row22_col8, #T_223ff_row23_col2, #T_223ff_row23_col8, #T_223ff_row24_col2, #T_223ff_row24_col8, #T_223ff_row25_col2, #T_223ff_row25_col8, #T_223ff_row25_col9, #T_223ff_row26_col2, #T_223ff_row26_col8, #T_223ff_row26_col9, #T_223ff_row27_col2, #T_223ff_row27_col8, #T_223ff_row27_col9, #T_223ff_row28_col2, #T_223ff_row28_col8, #T_223ff_row28_col9, #T_223ff_row29_col2, #T_223ff_row29_col8, #T_223ff_row30_col2, #T_223ff_row30_col8, #T_223ff_row31_col2, #T_223ff_row31_col8, #T_223ff_row32_col2, #T_223ff_row32_col8, #T_223ff_row33_col2, #T_223ff_row33_col8, #T_223ff_row34_col2, #T_223ff_row34_col8, #T_223ff_row35_col2, #T_223ff_row35_col8, #T_223ff_row36_col2, #T_223ff_row36_col8, #T_223ff_row37_col2, #T_223ff_row37_col8, #T_223ff_row38_col2, #T_223ff_row38_col8, #T_223ff_row39_col2, #T_223ff_row39_col8, #T_223ff_row40_col2, #T_223ff_row40_col8, #T_223ff_row41_col2, #T_223ff_row41_col8, #T_223ff_row42_col2, #T_223ff_row42_col8, #T_223ff_row43_col2, #T_223ff_row43_col8, #T_223ff_row44_col2, #T_223ff_row44_col8, #T_223ff_row45_col2, #T_223ff_row45_col8, #T_223ff_row46_col2, #T_223ff_row46_col8, #T_223ff_row47_col2, #T_223ff_row47_col8, #T_223ff_row48_col2, #T_223ff_row48_col8, #T_223ff_row49_col2, #T_223ff_row49_col8 {\n",
       "  color: green;\n",
       "  font-weight: bold;\n",
       "}\n",
       "#T_223ff_row0_col13, #T_223ff_row1_col13, #T_223ff_row2_col13, #T_223ff_row6_col13, #T_223ff_row25_col13, #T_223ff_row26_col13, #T_223ff_row27_col13, #T_223ff_row28_col13 {\n",
       "  background-color: #90EE90;\n",
       "}\n",
       "#T_223ff_row3_col9, #T_223ff_row4_col9, #T_223ff_row5_col9, #T_223ff_row29_col9, #T_223ff_row30_col9 {\n",
       "  color: red;\n",
       "  font-weight: bold;\n",
       "}\n",
       "#T_223ff_row3_col13, #T_223ff_row4_col13, #T_223ff_row5_col13, #T_223ff_row29_col13, #T_223ff_row30_col13 {\n",
       "  background-color: #FFB6C1;\n",
       "}\n",
       "#T_223ff_row7_col5, #T_223ff_row8_col5, #T_223ff_row9_col5, #T_223ff_row10_col5, #T_223ff_row11_col5, #T_223ff_row12_col5, #T_223ff_row13_col5, #T_223ff_row14_col5, #T_223ff_row15_col5, #T_223ff_row16_col5, #T_223ff_row17_col5, #T_223ff_row18_col5, #T_223ff_row19_col5, #T_223ff_row20_col5, #T_223ff_row21_col5, #T_223ff_row22_col5, #T_223ff_row23_col5, #T_223ff_row24_col5, #T_223ff_row31_col5, #T_223ff_row32_col5, #T_223ff_row33_col5, #T_223ff_row34_col5, #T_223ff_row35_col5, #T_223ff_row36_col5, #T_223ff_row37_col5, #T_223ff_row38_col5, #T_223ff_row39_col5, #T_223ff_row40_col5, #T_223ff_row41_col5, #T_223ff_row42_col5, #T_223ff_row43_col5, #T_223ff_row44_col5, #T_223ff_row45_col5, #T_223ff_row46_col5, #T_223ff_row47_col5, #T_223ff_row48_col5, #T_223ff_row49_col5 {\n",
       "  color: blue;\n",
       "  font-weight: bold;\n",
       "}\n",
       "#T_223ff_row7_col13, #T_223ff_row8_col13, #T_223ff_row9_col13, #T_223ff_row10_col13, #T_223ff_row11_col13, #T_223ff_row12_col13, #T_223ff_row13_col13, #T_223ff_row14_col13, #T_223ff_row15_col13, #T_223ff_row16_col13, #T_223ff_row17_col13, #T_223ff_row18_col13, #T_223ff_row19_col13, #T_223ff_row20_col13, #T_223ff_row21_col13, #T_223ff_row22_col13, #T_223ff_row23_col13, #T_223ff_row24_col13, #T_223ff_row31_col13, #T_223ff_row32_col13, #T_223ff_row33_col13, #T_223ff_row34_col13, #T_223ff_row35_col13, #T_223ff_row36_col13, #T_223ff_row37_col13, #T_223ff_row38_col13, #T_223ff_row39_col13, #T_223ff_row40_col13, #T_223ff_row41_col13, #T_223ff_row42_col13, #T_223ff_row43_col13, #T_223ff_row44_col13, #T_223ff_row45_col13, #T_223ff_row46_col13, #T_223ff_row47_col13, #T_223ff_row48_col13, #T_223ff_row49_col13 {\n",
       "  background-color: #87CEEB;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_223ff\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_223ff_level0_col0\" class=\"col_heading level0 col0\" >File</th>\n",
       "      <th id=\"T_223ff_level0_col1\" class=\"col_heading level0 col1\" >Path</th>\n",
       "      <th id=\"T_223ff_level0_col2\" class=\"col_heading level0 col2\" >File_Exists</th>\n",
       "      <th id=\"T_223ff_level0_col3\" class=\"col_heading level0 col3\" >Rows</th>\n",
       "      <th id=\"T_223ff_level0_col4\" class=\"col_heading level0 col4\" >Column</th>\n",
       "      <th id=\"T_223ff_level0_col5\" class=\"col_heading level0 col5\" >Column_Type</th>\n",
       "      <th id=\"T_223ff_level0_col6\" class=\"col_heading level0 col6\" >Expected_Type</th>\n",
       "      <th id=\"T_223ff_level0_col7\" class=\"col_heading level0 col7\" >Actual_Type</th>\n",
       "      <th id=\"T_223ff_level0_col8\" class=\"col_heading level0 col8\" >Column_Exists</th>\n",
       "      <th id=\"T_223ff_level0_col9\" class=\"col_heading level0 col9\" >Type_Match</th>\n",
       "      <th id=\"T_223ff_level0_col10\" class=\"col_heading level0 col10\" >Null_Count</th>\n",
       "      <th id=\"T_223ff_level0_col11\" class=\"col_heading level0 col11\" >Unique_Values</th>\n",
       "      <th id=\"T_223ff_level0_col12\" class=\"col_heading level0 col12\" >Sample_Values</th>\n",
       "      <th id=\"T_223ff_level0_col13\" class=\"col_heading level0 col13\" >Status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_223ff_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_223ff_row0_col0\" class=\"data row0 col0\" >lighting.parquet</td>\n",
       "      <td id=\"T_223ff_row0_col1\" class=\"data row0 col1\" >parsed_data/idf_data/by_category/lighting.parquet</td>\n",
       "      <td id=\"T_223ff_row0_col2\" class=\"data row0 col2\" >✓</td>\n",
       "      <td id=\"T_223ff_row0_col3\" class=\"data row0 col3\" >3</td>\n",
       "      <td id=\"T_223ff_row0_col4\" class=\"data row0 col4\" >building_id</td>\n",
       "      <td id=\"T_223ff_row0_col5\" class=\"data row0 col5\" >Expected</td>\n",
       "      <td id=\"T_223ff_row0_col6\" class=\"data row0 col6\" >str</td>\n",
       "      <td id=\"T_223ff_row0_col7\" class=\"data row0 col7\" >str</td>\n",
       "      <td id=\"T_223ff_row0_col8\" class=\"data row0 col8\" >✓</td>\n",
       "      <td id=\"T_223ff_row0_col9\" class=\"data row0 col9\" >✓</td>\n",
       "      <td id=\"T_223ff_row0_col10\" class=\"data row0 col10\" >0.000000</td>\n",
       "      <td id=\"T_223ff_row0_col11\" class=\"data row0 col11\" >3.000000</td>\n",
       "      <td id=\"T_223ff_row0_col12\" class=\"data row0 col12\" >['4136733', '4136737', '4136738']</td>\n",
       "      <td id=\"T_223ff_row0_col13\" class=\"data row0 col13\" >OK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_223ff_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_223ff_row1_col0\" class=\"data row1 col0\" >lighting.parquet</td>\n",
       "      <td id=\"T_223ff_row1_col1\" class=\"data row1 col1\" >parsed_data/idf_data/by_category/lighting.parquet</td>\n",
       "      <td id=\"T_223ff_row1_col2\" class=\"data row1 col2\" >✓</td>\n",
       "      <td id=\"T_223ff_row1_col3\" class=\"data row1 col3\" >3</td>\n",
       "      <td id=\"T_223ff_row1_col4\" class=\"data row1 col4\" >zone_name</td>\n",
       "      <td id=\"T_223ff_row1_col5\" class=\"data row1 col5\" >Expected</td>\n",
       "      <td id=\"T_223ff_row1_col6\" class=\"data row1 col6\" >str</td>\n",
       "      <td id=\"T_223ff_row1_col7\" class=\"data row1 col7\" >str</td>\n",
       "      <td id=\"T_223ff_row1_col8\" class=\"data row1 col8\" >✓</td>\n",
       "      <td id=\"T_223ff_row1_col9\" class=\"data row1 col9\" >✓</td>\n",
       "      <td id=\"T_223ff_row1_col10\" class=\"data row1 col10\" >0.000000</td>\n",
       "      <td id=\"T_223ff_row1_col11\" class=\"data row1 col11\" >1.000000</td>\n",
       "      <td id=\"T_223ff_row1_col12\" class=\"data row1 col12\" >['ALL_ZONES']</td>\n",
       "      <td id=\"T_223ff_row1_col13\" class=\"data row1 col13\" >OK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_223ff_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_223ff_row2_col0\" class=\"data row2 col0\" >lighting.parquet</td>\n",
       "      <td id=\"T_223ff_row2_col1\" class=\"data row2 col1\" >parsed_data/idf_data/by_category/lighting.parquet</td>\n",
       "      <td id=\"T_223ff_row2_col2\" class=\"data row2 col2\" >✓</td>\n",
       "      <td id=\"T_223ff_row2_col3\" class=\"data row2 col3\" >3</td>\n",
       "      <td id=\"T_223ff_row2_col4\" class=\"data row2 col4\" >object_name</td>\n",
       "      <td id=\"T_223ff_row2_col5\" class=\"data row2 col5\" >Expected</td>\n",
       "      <td id=\"T_223ff_row2_col6\" class=\"data row2 col6\" >str</td>\n",
       "      <td id=\"T_223ff_row2_col7\" class=\"data row2 col7\" >str</td>\n",
       "      <td id=\"T_223ff_row2_col8\" class=\"data row2 col8\" >✓</td>\n",
       "      <td id=\"T_223ff_row2_col9\" class=\"data row2 col9\" >✓</td>\n",
       "      <td id=\"T_223ff_row2_col10\" class=\"data row2 col10\" >0.000000</td>\n",
       "      <td id=\"T_223ff_row2_col11\" class=\"data row2 col11\" >1.000000</td>\n",
       "      <td id=\"T_223ff_row2_col12\" class=\"data row2 col12\" >['Lights_ALL_ZONES']</td>\n",
       "      <td id=\"T_223ff_row2_col13\" class=\"data row2 col13\" >OK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_223ff_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_223ff_row3_col0\" class=\"data row3 col0\" >lighting.parquet</td>\n",
       "      <td id=\"T_223ff_row3_col1\" class=\"data row3 col1\" >parsed_data/idf_data/by_category/lighting.parquet</td>\n",
       "      <td id=\"T_223ff_row3_col2\" class=\"data row3 col2\" >✓</td>\n",
       "      <td id=\"T_223ff_row3_col3\" class=\"data row3 col3\" >3</td>\n",
       "      <td id=\"T_223ff_row3_col4\" class=\"data row3 col4\" >watts_per_zone_floor_area</td>\n",
       "      <td id=\"T_223ff_row3_col5\" class=\"data row3 col5\" >Expected</td>\n",
       "      <td id=\"T_223ff_row3_col6\" class=\"data row3 col6\" >float</td>\n",
       "      <td id=\"T_223ff_row3_col7\" class=\"data row3 col7\" >str</td>\n",
       "      <td id=\"T_223ff_row3_col8\" class=\"data row3 col8\" >✓</td>\n",
       "      <td id=\"T_223ff_row3_col9\" class=\"data row3 col9\" >✗</td>\n",
       "      <td id=\"T_223ff_row3_col10\" class=\"data row3 col10\" >0.000000</td>\n",
       "      <td id=\"T_223ff_row3_col11\" class=\"data row3 col11\" >3.000000</td>\n",
       "      <td id=\"T_223ff_row3_col12\" class=\"data row3 col12\" >['0', '4.092524414409712', '4.980486110210785']</td>\n",
       "      <td id=\"T_223ff_row3_col13\" class=\"data row3 col13\" >ISSUE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_223ff_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_223ff_row4_col0\" class=\"data row4 col0\" >lighting.parquet</td>\n",
       "      <td id=\"T_223ff_row4_col1\" class=\"data row4 col1\" >parsed_data/idf_data/by_category/lighting.parquet</td>\n",
       "      <td id=\"T_223ff_row4_col2\" class=\"data row4 col2\" >✓</td>\n",
       "      <td id=\"T_223ff_row4_col3\" class=\"data row4 col3\" >3</td>\n",
       "      <td id=\"T_223ff_row4_col4\" class=\"data row4 col4\" >fraction_radiant</td>\n",
       "      <td id=\"T_223ff_row4_col5\" class=\"data row4 col5\" >Expected</td>\n",
       "      <td id=\"T_223ff_row4_col6\" class=\"data row4 col6\" >float</td>\n",
       "      <td id=\"T_223ff_row4_col7\" class=\"data row4 col7\" >str</td>\n",
       "      <td id=\"T_223ff_row4_col8\" class=\"data row4 col8\" >✓</td>\n",
       "      <td id=\"T_223ff_row4_col9\" class=\"data row4 col9\" >✗</td>\n",
       "      <td id=\"T_223ff_row4_col10\" class=\"data row4 col10\" >0.000000</td>\n",
       "      <td id=\"T_223ff_row4_col11\" class=\"data row4 col11\" >1.000000</td>\n",
       "      <td id=\"T_223ff_row4_col12\" class=\"data row4 col12\" >['0.1']</td>\n",
       "      <td id=\"T_223ff_row4_col13\" class=\"data row4 col13\" >ISSUE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_223ff_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "      <td id=\"T_223ff_row5_col0\" class=\"data row5 col0\" >lighting.parquet</td>\n",
       "      <td id=\"T_223ff_row5_col1\" class=\"data row5 col1\" >parsed_data/idf_data/by_category/lighting.parquet</td>\n",
       "      <td id=\"T_223ff_row5_col2\" class=\"data row5 col2\" >✓</td>\n",
       "      <td id=\"T_223ff_row5_col3\" class=\"data row5 col3\" >3</td>\n",
       "      <td id=\"T_223ff_row5_col4\" class=\"data row5 col4\" >fraction_visible</td>\n",
       "      <td id=\"T_223ff_row5_col5\" class=\"data row5 col5\" >Expected</td>\n",
       "      <td id=\"T_223ff_row5_col6\" class=\"data row5 col6\" >float</td>\n",
       "      <td id=\"T_223ff_row5_col7\" class=\"data row5 col7\" >str</td>\n",
       "      <td id=\"T_223ff_row5_col8\" class=\"data row5 col8\" >✓</td>\n",
       "      <td id=\"T_223ff_row5_col9\" class=\"data row5 col9\" >✗</td>\n",
       "      <td id=\"T_223ff_row5_col10\" class=\"data row5 col10\" >0.000000</td>\n",
       "      <td id=\"T_223ff_row5_col11\" class=\"data row5 col11\" >1.000000</td>\n",
       "      <td id=\"T_223ff_row5_col12\" class=\"data row5 col12\" >['0.1']</td>\n",
       "      <td id=\"T_223ff_row5_col13\" class=\"data row5 col13\" >ISSUE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_223ff_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
       "      <td id=\"T_223ff_row6_col0\" class=\"data row6 col0\" >lighting.parquet</td>\n",
       "      <td id=\"T_223ff_row6_col1\" class=\"data row6 col1\" >parsed_data/idf_data/by_category/lighting.parquet</td>\n",
       "      <td id=\"T_223ff_row6_col2\" class=\"data row6 col2\" >✓</td>\n",
       "      <td id=\"T_223ff_row6_col3\" class=\"data row6 col3\" >3</td>\n",
       "      <td id=\"T_223ff_row6_col4\" class=\"data row6 col4\" >schedule_name</td>\n",
       "      <td id=\"T_223ff_row6_col5\" class=\"data row6 col5\" >Expected</td>\n",
       "      <td id=\"T_223ff_row6_col6\" class=\"data row6 col6\" >str</td>\n",
       "      <td id=\"T_223ff_row6_col7\" class=\"data row6 col7\" >str</td>\n",
       "      <td id=\"T_223ff_row6_col8\" class=\"data row6 col8\" >✓</td>\n",
       "      <td id=\"T_223ff_row6_col9\" class=\"data row6 col9\" >✓</td>\n",
       "      <td id=\"T_223ff_row6_col10\" class=\"data row6 col10\" >0.000000</td>\n",
       "      <td id=\"T_223ff_row6_col11\" class=\"data row6 col11\" >1.000000</td>\n",
       "      <td id=\"T_223ff_row6_col12\" class=\"data row6 col12\" >['LightsSchedule']</td>\n",
       "      <td id=\"T_223ff_row6_col13\" class=\"data row6 col13\" >OK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_223ff_level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
       "      <td id=\"T_223ff_row7_col0\" class=\"data row7 col0\" >lighting.parquet</td>\n",
       "      <td id=\"T_223ff_row7_col1\" class=\"data row7 col1\" >parsed_data/idf_data/by_category/lighting.parquet</td>\n",
       "      <td id=\"T_223ff_row7_col2\" class=\"data row7 col2\" >✓</td>\n",
       "      <td id=\"T_223ff_row7_col3\" class=\"data row7 col3\" >3</td>\n",
       "      <td id=\"T_223ff_row7_col4\" class=\"data row7 col4\" >object_type</td>\n",
       "      <td id=\"T_223ff_row7_col5\" class=\"data row7 col5\" >DISCOVERED</td>\n",
       "      <td id=\"T_223ff_row7_col6\" class=\"data row7 col6\" >N/A</td>\n",
       "      <td id=\"T_223ff_row7_col7\" class=\"data row7 col7\" >str</td>\n",
       "      <td id=\"T_223ff_row7_col8\" class=\"data row7 col8\" >✓</td>\n",
       "      <td id=\"T_223ff_row7_col9\" class=\"data row7 col9\" >N/A</td>\n",
       "      <td id=\"T_223ff_row7_col10\" class=\"data row7 col10\" >0.000000</td>\n",
       "      <td id=\"T_223ff_row7_col11\" class=\"data row7 col11\" >1.000000</td>\n",
       "      <td id=\"T_223ff_row7_col12\" class=\"data row7 col12\" >['LIGHTS']</td>\n",
       "      <td id=\"T_223ff_row7_col13\" class=\"data row7 col13\" >EXTRA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_223ff_level0_row8\" class=\"row_heading level0 row8\" >8</th>\n",
       "      <td id=\"T_223ff_row8_col0\" class=\"data row8 col0\" >lighting.parquet</td>\n",
       "      <td id=\"T_223ff_row8_col1\" class=\"data row8 col1\" >parsed_data/idf_data/by_category/lighting.parquet</td>\n",
       "      <td id=\"T_223ff_row8_col2\" class=\"data row8 col2\" >✓</td>\n",
       "      <td id=\"T_223ff_row8_col3\" class=\"data row8 col3\" >3</td>\n",
       "      <td id=\"T_223ff_row8_col4\" class=\"data row8 col4\" >name</td>\n",
       "      <td id=\"T_223ff_row8_col5\" class=\"data row8 col5\" >DISCOVERED</td>\n",
       "      <td id=\"T_223ff_row8_col6\" class=\"data row8 col6\" >N/A</td>\n",
       "      <td id=\"T_223ff_row8_col7\" class=\"data row8 col7\" >str</td>\n",
       "      <td id=\"T_223ff_row8_col8\" class=\"data row8 col8\" >✓</td>\n",
       "      <td id=\"T_223ff_row8_col9\" class=\"data row8 col9\" >N/A</td>\n",
       "      <td id=\"T_223ff_row8_col10\" class=\"data row8 col10\" >0.000000</td>\n",
       "      <td id=\"T_223ff_row8_col11\" class=\"data row8 col11\" >1.000000</td>\n",
       "      <td id=\"T_223ff_row8_col12\" class=\"data row8 col12\" >['Lights_ALL_ZONES']</td>\n",
       "      <td id=\"T_223ff_row8_col13\" class=\"data row8 col13\" >EXTRA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_223ff_level0_row9\" class=\"row_heading level0 row9\" >9</th>\n",
       "      <td id=\"T_223ff_row9_col0\" class=\"data row9 col0\" >lighting.parquet</td>\n",
       "      <td id=\"T_223ff_row9_col1\" class=\"data row9 col1\" >parsed_data/idf_data/by_category/lighting.parquet</td>\n",
       "      <td id=\"T_223ff_row9_col2\" class=\"data row9 col2\" >✓</td>\n",
       "      <td id=\"T_223ff_row9_col3\" class=\"data row9 col3\" >3</td>\n",
       "      <td id=\"T_223ff_row9_col4\" class=\"data row9 col4\" >zone_or_zonelist_name</td>\n",
       "      <td id=\"T_223ff_row9_col5\" class=\"data row9 col5\" >DISCOVERED</td>\n",
       "      <td id=\"T_223ff_row9_col6\" class=\"data row9 col6\" >N/A</td>\n",
       "      <td id=\"T_223ff_row9_col7\" class=\"data row9 col7\" >str</td>\n",
       "      <td id=\"T_223ff_row9_col8\" class=\"data row9 col8\" >✓</td>\n",
       "      <td id=\"T_223ff_row9_col9\" class=\"data row9 col9\" >N/A</td>\n",
       "      <td id=\"T_223ff_row9_col10\" class=\"data row9 col10\" >0.000000</td>\n",
       "      <td id=\"T_223ff_row9_col11\" class=\"data row9 col11\" >1.000000</td>\n",
       "      <td id=\"T_223ff_row9_col12\" class=\"data row9 col12\" >['ALL_ZONES']</td>\n",
       "      <td id=\"T_223ff_row9_col13\" class=\"data row9 col13\" >EXTRA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_223ff_level0_row10\" class=\"row_heading level0 row10\" >10</th>\n",
       "      <td id=\"T_223ff_row10_col0\" class=\"data row10 col0\" >lighting.parquet</td>\n",
       "      <td id=\"T_223ff_row10_col1\" class=\"data row10 col1\" >parsed_data/idf_data/by_category/lighting.parquet</td>\n",
       "      <td id=\"T_223ff_row10_col2\" class=\"data row10 col2\" >✓</td>\n",
       "      <td id=\"T_223ff_row10_col3\" class=\"data row10 col3\" >3</td>\n",
       "      <td id=\"T_223ff_row10_col4\" class=\"data row10 col4\" >design_level_calculation_method</td>\n",
       "      <td id=\"T_223ff_row10_col5\" class=\"data row10 col5\" >DISCOVERED</td>\n",
       "      <td id=\"T_223ff_row10_col6\" class=\"data row10 col6\" >N/A</td>\n",
       "      <td id=\"T_223ff_row10_col7\" class=\"data row10 col7\" >str</td>\n",
       "      <td id=\"T_223ff_row10_col8\" class=\"data row10 col8\" >✓</td>\n",
       "      <td id=\"T_223ff_row10_col9\" class=\"data row10 col9\" >N/A</td>\n",
       "      <td id=\"T_223ff_row10_col10\" class=\"data row10 col10\" >0.000000</td>\n",
       "      <td id=\"T_223ff_row10_col11\" class=\"data row10 col11\" >1.000000</td>\n",
       "      <td id=\"T_223ff_row10_col12\" class=\"data row10 col12\" >['Watts/Area']</td>\n",
       "      <td id=\"T_223ff_row10_col13\" class=\"data row10 col13\" >EXTRA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_223ff_level0_row11\" class=\"row_heading level0 row11\" >11</th>\n",
       "      <td id=\"T_223ff_row11_col0\" class=\"data row11 col0\" >lighting.parquet</td>\n",
       "      <td id=\"T_223ff_row11_col1\" class=\"data row11 col1\" >parsed_data/idf_data/by_category/lighting.parquet</td>\n",
       "      <td id=\"T_223ff_row11_col2\" class=\"data row11 col2\" >✓</td>\n",
       "      <td id=\"T_223ff_row11_col3\" class=\"data row11 col3\" >3</td>\n",
       "      <td id=\"T_223ff_row11_col4\" class=\"data row11 col4\" >watts_per_zone_floor_area_numeric</td>\n",
       "      <td id=\"T_223ff_row11_col5\" class=\"data row11 col5\" >DISCOVERED</td>\n",
       "      <td id=\"T_223ff_row11_col6\" class=\"data row11 col6\" >N/A</td>\n",
       "      <td id=\"T_223ff_row11_col7\" class=\"data row11 col7\" >float</td>\n",
       "      <td id=\"T_223ff_row11_col8\" class=\"data row11 col8\" >✓</td>\n",
       "      <td id=\"T_223ff_row11_col9\" class=\"data row11 col9\" >N/A</td>\n",
       "      <td id=\"T_223ff_row11_col10\" class=\"data row11 col10\" >0.000000</td>\n",
       "      <td id=\"T_223ff_row11_col11\" class=\"data row11 col11\" >3.000000</td>\n",
       "      <td id=\"T_223ff_row11_col12\" class=\"data row11 col12\" >[0.0, 4.092524414409712, 4.980486110210785]</td>\n",
       "      <td id=\"T_223ff_row11_col13\" class=\"data row11 col13\" >EXTRA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_223ff_level0_row12\" class=\"row_heading level0 row12\" >12</th>\n",
       "      <td id=\"T_223ff_row12_col0\" class=\"data row12 col0\" >lighting.parquet</td>\n",
       "      <td id=\"T_223ff_row12_col1\" class=\"data row12 col1\" >parsed_data/idf_data/by_category/lighting.parquet</td>\n",
       "      <td id=\"T_223ff_row12_col2\" class=\"data row12 col2\" >✓</td>\n",
       "      <td id=\"T_223ff_row12_col3\" class=\"data row12 col3\" >3</td>\n",
       "      <td id=\"T_223ff_row12_col4\" class=\"data row12 col4\" >return_air_fraction</td>\n",
       "      <td id=\"T_223ff_row12_col5\" class=\"data row12 col5\" >DISCOVERED</td>\n",
       "      <td id=\"T_223ff_row12_col6\" class=\"data row12 col6\" >N/A</td>\n",
       "      <td id=\"T_223ff_row12_col7\" class=\"data row12 col7\" >str</td>\n",
       "      <td id=\"T_223ff_row12_col8\" class=\"data row12 col8\" >✓</td>\n",
       "      <td id=\"T_223ff_row12_col9\" class=\"data row12 col9\" >N/A</td>\n",
       "      <td id=\"T_223ff_row12_col10\" class=\"data row12 col10\" >0.000000</td>\n",
       "      <td id=\"T_223ff_row12_col11\" class=\"data row12 col11\" >1.000000</td>\n",
       "      <td id=\"T_223ff_row12_col12\" class=\"data row12 col12\" >['0.8']</td>\n",
       "      <td id=\"T_223ff_row12_col13\" class=\"data row12 col13\" >EXTRA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_223ff_level0_row13\" class=\"row_heading level0 row13\" >13</th>\n",
       "      <td id=\"T_223ff_row13_col0\" class=\"data row13 col0\" >lighting.parquet</td>\n",
       "      <td id=\"T_223ff_row13_col1\" class=\"data row13 col1\" >parsed_data/idf_data/by_category/lighting.parquet</td>\n",
       "      <td id=\"T_223ff_row13_col2\" class=\"data row13 col2\" >✓</td>\n",
       "      <td id=\"T_223ff_row13_col3\" class=\"data row13 col3\" >3</td>\n",
       "      <td id=\"T_223ff_row13_col4\" class=\"data row13 col4\" >return_air_fraction_numeric</td>\n",
       "      <td id=\"T_223ff_row13_col5\" class=\"data row13 col5\" >DISCOVERED</td>\n",
       "      <td id=\"T_223ff_row13_col6\" class=\"data row13 col6\" >N/A</td>\n",
       "      <td id=\"T_223ff_row13_col7\" class=\"data row13 col7\" >float</td>\n",
       "      <td id=\"T_223ff_row13_col8\" class=\"data row13 col8\" >✓</td>\n",
       "      <td id=\"T_223ff_row13_col9\" class=\"data row13 col9\" >N/A</td>\n",
       "      <td id=\"T_223ff_row13_col10\" class=\"data row13 col10\" >0.000000</td>\n",
       "      <td id=\"T_223ff_row13_col11\" class=\"data row13 col11\" >1.000000</td>\n",
       "      <td id=\"T_223ff_row13_col12\" class=\"data row13 col12\" >[0.8]</td>\n",
       "      <td id=\"T_223ff_row13_col13\" class=\"data row13 col13\" >EXTRA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_223ff_level0_row14\" class=\"row_heading level0 row14\" >14</th>\n",
       "      <td id=\"T_223ff_row14_col0\" class=\"data row14 col0\" >lighting.parquet</td>\n",
       "      <td id=\"T_223ff_row14_col1\" class=\"data row14 col1\" >parsed_data/idf_data/by_category/lighting.parquet</td>\n",
       "      <td id=\"T_223ff_row14_col2\" class=\"data row14 col2\" >✓</td>\n",
       "      <td id=\"T_223ff_row14_col3\" class=\"data row14 col3\" >3</td>\n",
       "      <td id=\"T_223ff_row14_col4\" class=\"data row14 col4\" >fraction_radiant_numeric</td>\n",
       "      <td id=\"T_223ff_row14_col5\" class=\"data row14 col5\" >DISCOVERED</td>\n",
       "      <td id=\"T_223ff_row14_col6\" class=\"data row14 col6\" >N/A</td>\n",
       "      <td id=\"T_223ff_row14_col7\" class=\"data row14 col7\" >float</td>\n",
       "      <td id=\"T_223ff_row14_col8\" class=\"data row14 col8\" >✓</td>\n",
       "      <td id=\"T_223ff_row14_col9\" class=\"data row14 col9\" >N/A</td>\n",
       "      <td id=\"T_223ff_row14_col10\" class=\"data row14 col10\" >0.000000</td>\n",
       "      <td id=\"T_223ff_row14_col11\" class=\"data row14 col11\" >1.000000</td>\n",
       "      <td id=\"T_223ff_row14_col12\" class=\"data row14 col12\" >[0.1]</td>\n",
       "      <td id=\"T_223ff_row14_col13\" class=\"data row14 col13\" >EXTRA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_223ff_level0_row15\" class=\"row_heading level0 row15\" >15</th>\n",
       "      <td id=\"T_223ff_row15_col0\" class=\"data row15 col0\" >lighting.parquet</td>\n",
       "      <td id=\"T_223ff_row15_col1\" class=\"data row15 col1\" >parsed_data/idf_data/by_category/lighting.parquet</td>\n",
       "      <td id=\"T_223ff_row15_col2\" class=\"data row15 col2\" >✓</td>\n",
       "      <td id=\"T_223ff_row15_col3\" class=\"data row15 col3\" >3</td>\n",
       "      <td id=\"T_223ff_row15_col4\" class=\"data row15 col4\" >fraction_visible_numeric</td>\n",
       "      <td id=\"T_223ff_row15_col5\" class=\"data row15 col5\" >DISCOVERED</td>\n",
       "      <td id=\"T_223ff_row15_col6\" class=\"data row15 col6\" >N/A</td>\n",
       "      <td id=\"T_223ff_row15_col7\" class=\"data row15 col7\" >float</td>\n",
       "      <td id=\"T_223ff_row15_col8\" class=\"data row15 col8\" >✓</td>\n",
       "      <td id=\"T_223ff_row15_col9\" class=\"data row15 col9\" >N/A</td>\n",
       "      <td id=\"T_223ff_row15_col10\" class=\"data row15 col10\" >0.000000</td>\n",
       "      <td id=\"T_223ff_row15_col11\" class=\"data row15 col11\" >1.000000</td>\n",
       "      <td id=\"T_223ff_row15_col12\" class=\"data row15 col12\" >[0.1]</td>\n",
       "      <td id=\"T_223ff_row15_col13\" class=\"data row15 col13\" >EXTRA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_223ff_level0_row16\" class=\"row_heading level0 row16\" >16</th>\n",
       "      <td id=\"T_223ff_row16_col0\" class=\"data row16 col0\" >lighting.parquet</td>\n",
       "      <td id=\"T_223ff_row16_col1\" class=\"data row16 col1\" >parsed_data/idf_data/by_category/lighting.parquet</td>\n",
       "      <td id=\"T_223ff_row16_col2\" class=\"data row16 col2\" >✓</td>\n",
       "      <td id=\"T_223ff_row16_col3\" class=\"data row16 col3\" >3</td>\n",
       "      <td id=\"T_223ff_row16_col4\" class=\"data row16 col4\" >fraction_replaceable</td>\n",
       "      <td id=\"T_223ff_row16_col5\" class=\"data row16 col5\" >DISCOVERED</td>\n",
       "      <td id=\"T_223ff_row16_col6\" class=\"data row16 col6\" >N/A</td>\n",
       "      <td id=\"T_223ff_row16_col7\" class=\"data row16 col7\" >str</td>\n",
       "      <td id=\"T_223ff_row16_col8\" class=\"data row16 col8\" >✓</td>\n",
       "      <td id=\"T_223ff_row16_col9\" class=\"data row16 col9\" >N/A</td>\n",
       "      <td id=\"T_223ff_row16_col10\" class=\"data row16 col10\" >0.000000</td>\n",
       "      <td id=\"T_223ff_row16_col11\" class=\"data row16 col11\" >1.000000</td>\n",
       "      <td id=\"T_223ff_row16_col12\" class=\"data row16 col12\" >['1']</td>\n",
       "      <td id=\"T_223ff_row16_col13\" class=\"data row16 col13\" >EXTRA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_223ff_level0_row17\" class=\"row_heading level0 row17\" >17</th>\n",
       "      <td id=\"T_223ff_row17_col0\" class=\"data row17 col0\" >lighting.parquet</td>\n",
       "      <td id=\"T_223ff_row17_col1\" class=\"data row17 col1\" >parsed_data/idf_data/by_category/lighting.parquet</td>\n",
       "      <td id=\"T_223ff_row17_col2\" class=\"data row17 col2\" >✓</td>\n",
       "      <td id=\"T_223ff_row17_col3\" class=\"data row17 col3\" >3</td>\n",
       "      <td id=\"T_223ff_row17_col4\" class=\"data row17 col4\" >fraction_replaceable_numeric</td>\n",
       "      <td id=\"T_223ff_row17_col5\" class=\"data row17 col5\" >DISCOVERED</td>\n",
       "      <td id=\"T_223ff_row17_col6\" class=\"data row17 col6\" >N/A</td>\n",
       "      <td id=\"T_223ff_row17_col7\" class=\"data row17 col7\" >float</td>\n",
       "      <td id=\"T_223ff_row17_col8\" class=\"data row17 col8\" >✓</td>\n",
       "      <td id=\"T_223ff_row17_col9\" class=\"data row17 col9\" >N/A</td>\n",
       "      <td id=\"T_223ff_row17_col10\" class=\"data row17 col10\" >0.000000</td>\n",
       "      <td id=\"T_223ff_row17_col11\" class=\"data row17 col11\" >1.000000</td>\n",
       "      <td id=\"T_223ff_row17_col12\" class=\"data row17 col12\" >[1.0]</td>\n",
       "      <td id=\"T_223ff_row17_col13\" class=\"data row17 col13\" >EXTRA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_223ff_level0_row18\" class=\"row_heading level0 row18\" >18</th>\n",
       "      <td id=\"T_223ff_row18_col0\" class=\"data row18 col0\" >lighting.parquet</td>\n",
       "      <td id=\"T_223ff_row18_col1\" class=\"data row18 col1\" >parsed_data/idf_data/by_category/lighting.parquet</td>\n",
       "      <td id=\"T_223ff_row18_col2\" class=\"data row18 col2\" >✓</td>\n",
       "      <td id=\"T_223ff_row18_col3\" class=\"data row18 col3\" >3</td>\n",
       "      <td id=\"T_223ff_row18_col4\" class=\"data row18 col4\" >end_use_subcategory</td>\n",
       "      <td id=\"T_223ff_row18_col5\" class=\"data row18 col5\" >DISCOVERED</td>\n",
       "      <td id=\"T_223ff_row18_col6\" class=\"data row18 col6\" >N/A</td>\n",
       "      <td id=\"T_223ff_row18_col7\" class=\"data row18 col7\" >str</td>\n",
       "      <td id=\"T_223ff_row18_col8\" class=\"data row18 col8\" >✓</td>\n",
       "      <td id=\"T_223ff_row18_col9\" class=\"data row18 col9\" >N/A</td>\n",
       "      <td id=\"T_223ff_row18_col10\" class=\"data row18 col10\" >0.000000</td>\n",
       "      <td id=\"T_223ff_row18_col11\" class=\"data row18 col11\" >1.000000</td>\n",
       "      <td id=\"T_223ff_row18_col12\" class=\"data row18 col12\" >['General']</td>\n",
       "      <td id=\"T_223ff_row18_col13\" class=\"data row18 col13\" >EXTRA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_223ff_level0_row19\" class=\"row_heading level0 row19\" >19</th>\n",
       "      <td id=\"T_223ff_row19_col0\" class=\"data row19 col0\" >lighting.parquet</td>\n",
       "      <td id=\"T_223ff_row19_col1\" class=\"data row19 col1\" >parsed_data/idf_data/by_category/lighting.parquet</td>\n",
       "      <td id=\"T_223ff_row19_col2\" class=\"data row19 col2\" >✓</td>\n",
       "      <td id=\"T_223ff_row19_col3\" class=\"data row19 col3\" >3</td>\n",
       "      <td id=\"T_223ff_row19_col4\" class=\"data row19 col4\" >return_air_fraction_calculated_from_plenum_temperature</td>\n",
       "      <td id=\"T_223ff_row19_col5\" class=\"data row19 col5\" >DISCOVERED</td>\n",
       "      <td id=\"T_223ff_row19_col6\" class=\"data row19 col6\" >N/A</td>\n",
       "      <td id=\"T_223ff_row19_col7\" class=\"data row19 col7\" >str</td>\n",
       "      <td id=\"T_223ff_row19_col8\" class=\"data row19 col8\" >✓</td>\n",
       "      <td id=\"T_223ff_row19_col9\" class=\"data row19 col9\" >N/A</td>\n",
       "      <td id=\"T_223ff_row19_col10\" class=\"data row19 col10\" >0.000000</td>\n",
       "      <td id=\"T_223ff_row19_col11\" class=\"data row19 col11\" >1.000000</td>\n",
       "      <td id=\"T_223ff_row19_col12\" class=\"data row19 col12\" >['No']</td>\n",
       "      <td id=\"T_223ff_row19_col13\" class=\"data row19 col13\" >EXTRA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_223ff_level0_row20\" class=\"row_heading level0 row20\" >20</th>\n",
       "      <td id=\"T_223ff_row20_col0\" class=\"data row20 col0\" >lighting.parquet</td>\n",
       "      <td id=\"T_223ff_row20_col1\" class=\"data row20 col1\" >parsed_data/idf_data/by_category/lighting.parquet</td>\n",
       "      <td id=\"T_223ff_row20_col2\" class=\"data row20 col2\" >✓</td>\n",
       "      <td id=\"T_223ff_row20_col3\" class=\"data row20 col3\" >3</td>\n",
       "      <td id=\"T_223ff_row20_col4\" class=\"data row20 col4\" >return_air_fraction_function_of_plenum_temperature_coefficient_1</td>\n",
       "      <td id=\"T_223ff_row20_col5\" class=\"data row20 col5\" >DISCOVERED</td>\n",
       "      <td id=\"T_223ff_row20_col6\" class=\"data row20 col6\" >N/A</td>\n",
       "      <td id=\"T_223ff_row20_col7\" class=\"data row20 col7\" >str</td>\n",
       "      <td id=\"T_223ff_row20_col8\" class=\"data row20 col8\" >✓</td>\n",
       "      <td id=\"T_223ff_row20_col9\" class=\"data row20 col9\" >N/A</td>\n",
       "      <td id=\"T_223ff_row20_col10\" class=\"data row20 col10\" >0.000000</td>\n",
       "      <td id=\"T_223ff_row20_col11\" class=\"data row20 col11\" >1.000000</td>\n",
       "      <td id=\"T_223ff_row20_col12\" class=\"data row20 col12\" >['0']</td>\n",
       "      <td id=\"T_223ff_row20_col13\" class=\"data row20 col13\" >EXTRA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_223ff_level0_row21\" class=\"row_heading level0 row21\" >21</th>\n",
       "      <td id=\"T_223ff_row21_col0\" class=\"data row21 col0\" >lighting.parquet</td>\n",
       "      <td id=\"T_223ff_row21_col1\" class=\"data row21 col1\" >parsed_data/idf_data/by_category/lighting.parquet</td>\n",
       "      <td id=\"T_223ff_row21_col2\" class=\"data row21 col2\" >✓</td>\n",
       "      <td id=\"T_223ff_row21_col3\" class=\"data row21 col3\" >3</td>\n",
       "      <td id=\"T_223ff_row21_col4\" class=\"data row21 col4\" >return_air_fraction_function_of_plenum_temperature_coefficient_1_numeric</td>\n",
       "      <td id=\"T_223ff_row21_col5\" class=\"data row21 col5\" >DISCOVERED</td>\n",
       "      <td id=\"T_223ff_row21_col6\" class=\"data row21 col6\" >N/A</td>\n",
       "      <td id=\"T_223ff_row21_col7\" class=\"data row21 col7\" >float</td>\n",
       "      <td id=\"T_223ff_row21_col8\" class=\"data row21 col8\" >✓</td>\n",
       "      <td id=\"T_223ff_row21_col9\" class=\"data row21 col9\" >N/A</td>\n",
       "      <td id=\"T_223ff_row21_col10\" class=\"data row21 col10\" >0.000000</td>\n",
       "      <td id=\"T_223ff_row21_col11\" class=\"data row21 col11\" >1.000000</td>\n",
       "      <td id=\"T_223ff_row21_col12\" class=\"data row21 col12\" >[0.0]</td>\n",
       "      <td id=\"T_223ff_row21_col13\" class=\"data row21 col13\" >EXTRA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_223ff_level0_row22\" class=\"row_heading level0 row22\" >22</th>\n",
       "      <td id=\"T_223ff_row22_col0\" class=\"data row22 col0\" >lighting.parquet</td>\n",
       "      <td id=\"T_223ff_row22_col1\" class=\"data row22 col1\" >parsed_data/idf_data/by_category/lighting.parquet</td>\n",
       "      <td id=\"T_223ff_row22_col2\" class=\"data row22 col2\" >✓</td>\n",
       "      <td id=\"T_223ff_row22_col3\" class=\"data row22 col3\" >3</td>\n",
       "      <td id=\"T_223ff_row22_col4\" class=\"data row22 col4\" >return_air_fraction_function_of_plenum_temperature_coefficient_2</td>\n",
       "      <td id=\"T_223ff_row22_col5\" class=\"data row22 col5\" >DISCOVERED</td>\n",
       "      <td id=\"T_223ff_row22_col6\" class=\"data row22 col6\" >N/A</td>\n",
       "      <td id=\"T_223ff_row22_col7\" class=\"data row22 col7\" >str</td>\n",
       "      <td id=\"T_223ff_row22_col8\" class=\"data row22 col8\" >✓</td>\n",
       "      <td id=\"T_223ff_row22_col9\" class=\"data row22 col9\" >N/A</td>\n",
       "      <td id=\"T_223ff_row22_col10\" class=\"data row22 col10\" >0.000000</td>\n",
       "      <td id=\"T_223ff_row22_col11\" class=\"data row22 col11\" >1.000000</td>\n",
       "      <td id=\"T_223ff_row22_col12\" class=\"data row22 col12\" >['0']</td>\n",
       "      <td id=\"T_223ff_row22_col13\" class=\"data row22 col13\" >EXTRA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_223ff_level0_row23\" class=\"row_heading level0 row23\" >23</th>\n",
       "      <td id=\"T_223ff_row23_col0\" class=\"data row23 col0\" >lighting.parquet</td>\n",
       "      <td id=\"T_223ff_row23_col1\" class=\"data row23 col1\" >parsed_data/idf_data/by_category/lighting.parquet</td>\n",
       "      <td id=\"T_223ff_row23_col2\" class=\"data row23 col2\" >✓</td>\n",
       "      <td id=\"T_223ff_row23_col3\" class=\"data row23 col3\" >3</td>\n",
       "      <td id=\"T_223ff_row23_col4\" class=\"data row23 col4\" >return_air_fraction_function_of_plenum_temperature_coefficient_2_numeric</td>\n",
       "      <td id=\"T_223ff_row23_col5\" class=\"data row23 col5\" >DISCOVERED</td>\n",
       "      <td id=\"T_223ff_row23_col6\" class=\"data row23 col6\" >N/A</td>\n",
       "      <td id=\"T_223ff_row23_col7\" class=\"data row23 col7\" >float</td>\n",
       "      <td id=\"T_223ff_row23_col8\" class=\"data row23 col8\" >✓</td>\n",
       "      <td id=\"T_223ff_row23_col9\" class=\"data row23 col9\" >N/A</td>\n",
       "      <td id=\"T_223ff_row23_col10\" class=\"data row23 col10\" >0.000000</td>\n",
       "      <td id=\"T_223ff_row23_col11\" class=\"data row23 col11\" >1.000000</td>\n",
       "      <td id=\"T_223ff_row23_col12\" class=\"data row23 col12\" >[0.0]</td>\n",
       "      <td id=\"T_223ff_row23_col13\" class=\"data row23 col13\" >EXTRA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_223ff_level0_row24\" class=\"row_heading level0 row24\" >24</th>\n",
       "      <td id=\"T_223ff_row24_col0\" class=\"data row24 col0\" >lighting.parquet</td>\n",
       "      <td id=\"T_223ff_row24_col1\" class=\"data row24 col1\" >parsed_data/idf_data/by_category/lighting.parquet</td>\n",
       "      <td id=\"T_223ff_row24_col2\" class=\"data row24 col2\" >✓</td>\n",
       "      <td id=\"T_223ff_row24_col3\" class=\"data row24 col3\" >3</td>\n",
       "      <td id=\"T_223ff_row24_col4\" class=\"data row24 col4\" >variant_id</td>\n",
       "      <td id=\"T_223ff_row24_col5\" class=\"data row24 col5\" >DISCOVERED</td>\n",
       "      <td id=\"T_223ff_row24_col6\" class=\"data row24 col6\" >N/A</td>\n",
       "      <td id=\"T_223ff_row24_col7\" class=\"data row24 col7\" >str</td>\n",
       "      <td id=\"T_223ff_row24_col8\" class=\"data row24 col8\" >✓</td>\n",
       "      <td id=\"T_223ff_row24_col9\" class=\"data row24 col9\" >N/A</td>\n",
       "      <td id=\"T_223ff_row24_col10\" class=\"data row24 col10\" >0.000000</td>\n",
       "      <td id=\"T_223ff_row24_col11\" class=\"data row24 col11\" >1.000000</td>\n",
       "      <td id=\"T_223ff_row24_col12\" class=\"data row24 col12\" >['base']</td>\n",
       "      <td id=\"T_223ff_row24_col13\" class=\"data row24 col13\" >EXTRA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_223ff_level0_row25\" class=\"row_heading level0 row25\" >25</th>\n",
       "      <td id=\"T_223ff_row25_col0\" class=\"data row25 col0\" >hvac_equipment.parquet</td>\n",
       "      <td id=\"T_223ff_row25_col1\" class=\"data row25 col1\" >parsed_data/idf_data/by_category/hvac_equipment.parquet</td>\n",
       "      <td id=\"T_223ff_row25_col2\" class=\"data row25 col2\" >✓</td>\n",
       "      <td id=\"T_223ff_row25_col3\" class=\"data row25 col3\" >48</td>\n",
       "      <td id=\"T_223ff_row25_col4\" class=\"data row25 col4\" >building_id</td>\n",
       "      <td id=\"T_223ff_row25_col5\" class=\"data row25 col5\" >Expected</td>\n",
       "      <td id=\"T_223ff_row25_col6\" class=\"data row25 col6\" >str</td>\n",
       "      <td id=\"T_223ff_row25_col7\" class=\"data row25 col7\" >str</td>\n",
       "      <td id=\"T_223ff_row25_col8\" class=\"data row25 col8\" >✓</td>\n",
       "      <td id=\"T_223ff_row25_col9\" class=\"data row25 col9\" >✓</td>\n",
       "      <td id=\"T_223ff_row25_col10\" class=\"data row25 col10\" >0.000000</td>\n",
       "      <td id=\"T_223ff_row25_col11\" class=\"data row25 col11\" >3.000000</td>\n",
       "      <td id=\"T_223ff_row25_col12\" class=\"data row25 col12\" >['4136733', '4136737', '4136738']</td>\n",
       "      <td id=\"T_223ff_row25_col13\" class=\"data row25 col13\" >OK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_223ff_level0_row26\" class=\"row_heading level0 row26\" >26</th>\n",
       "      <td id=\"T_223ff_row26_col0\" class=\"data row26 col0\" >hvac_equipment.parquet</td>\n",
       "      <td id=\"T_223ff_row26_col1\" class=\"data row26 col1\" >parsed_data/idf_data/by_category/hvac_equipment.parquet</td>\n",
       "      <td id=\"T_223ff_row26_col2\" class=\"data row26 col2\" >✓</td>\n",
       "      <td id=\"T_223ff_row26_col3\" class=\"data row26 col3\" >48</td>\n",
       "      <td id=\"T_223ff_row26_col4\" class=\"data row26 col4\" >zone_name</td>\n",
       "      <td id=\"T_223ff_row26_col5\" class=\"data row26 col5\" >Expected</td>\n",
       "      <td id=\"T_223ff_row26_col6\" class=\"data row26 col6\" >str</td>\n",
       "      <td id=\"T_223ff_row26_col7\" class=\"data row26 col7\" >str</td>\n",
       "      <td id=\"T_223ff_row26_col8\" class=\"data row26 col8\" >✓</td>\n",
       "      <td id=\"T_223ff_row26_col9\" class=\"data row26 col9\" >✓</td>\n",
       "      <td id=\"T_223ff_row26_col10\" class=\"data row26 col10\" >32.000000</td>\n",
       "      <td id=\"T_223ff_row26_col11\" class=\"data row26 col11\" >11.000000</td>\n",
       "      <td id=\"T_223ff_row26_col12\" class=\"data row26 col12\" >['Zone1_FrontPerimeter_EQUIPMENT', 'Zone1_RightPerimeter_EQUIPMENT', 'Zone1_RearPerimeter_EQUIPMENT']</td>\n",
       "      <td id=\"T_223ff_row26_col13\" class=\"data row26 col13\" >OK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_223ff_level0_row27\" class=\"row_heading level0 row27\" >27</th>\n",
       "      <td id=\"T_223ff_row27_col0\" class=\"data row27 col0\" >hvac_equipment.parquet</td>\n",
       "      <td id=\"T_223ff_row27_col1\" class=\"data row27 col1\" >parsed_data/idf_data/by_category/hvac_equipment.parquet</td>\n",
       "      <td id=\"T_223ff_row27_col2\" class=\"data row27 col2\" >✓</td>\n",
       "      <td id=\"T_223ff_row27_col3\" class=\"data row27 col3\" >48</td>\n",
       "      <td id=\"T_223ff_row27_col4\" class=\"data row27 col4\" >object_name</td>\n",
       "      <td id=\"T_223ff_row27_col5\" class=\"data row27 col5\" >Expected</td>\n",
       "      <td id=\"T_223ff_row27_col6\" class=\"data row27 col6\" >str</td>\n",
       "      <td id=\"T_223ff_row27_col7\" class=\"data row27 col7\" >str</td>\n",
       "      <td id=\"T_223ff_row27_col8\" class=\"data row27 col8\" >✓</td>\n",
       "      <td id=\"T_223ff_row27_col9\" class=\"data row27 col9\" >✓</td>\n",
       "      <td id=\"T_223ff_row27_col10\" class=\"data row27 col10\" >0.000000</td>\n",
       "      <td id=\"T_223ff_row27_col11\" class=\"data row27 col11\" >33.000000</td>\n",
       "      <td id=\"T_223ff_row27_col12\" class=\"data row27 col12\" >['Zone1_FrontPerimeter_Ideal_Loads', 'Zone1_RightPerimeter_Ideal_Loads', 'Zone1_RearPerimeter_Ideal_Loads']</td>\n",
       "      <td id=\"T_223ff_row27_col13\" class=\"data row27 col13\" >OK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_223ff_level0_row28\" class=\"row_heading level0 row28\" >28</th>\n",
       "      <td id=\"T_223ff_row28_col0\" class=\"data row28 col0\" >hvac_equipment.parquet</td>\n",
       "      <td id=\"T_223ff_row28_col1\" class=\"data row28 col1\" >parsed_data/idf_data/by_category/hvac_equipment.parquet</td>\n",
       "      <td id=\"T_223ff_row28_col2\" class=\"data row28 col2\" >✓</td>\n",
       "      <td id=\"T_223ff_row28_col3\" class=\"data row28 col3\" >48</td>\n",
       "      <td id=\"T_223ff_row28_col4\" class=\"data row28 col4\" >object_type</td>\n",
       "      <td id=\"T_223ff_row28_col5\" class=\"data row28 col5\" >Expected</td>\n",
       "      <td id=\"T_223ff_row28_col6\" class=\"data row28 col6\" >str</td>\n",
       "      <td id=\"T_223ff_row28_col7\" class=\"data row28 col7\" >str</td>\n",
       "      <td id=\"T_223ff_row28_col8\" class=\"data row28 col8\" >✓</td>\n",
       "      <td id=\"T_223ff_row28_col9\" class=\"data row28 col9\" >✓</td>\n",
       "      <td id=\"T_223ff_row28_col10\" class=\"data row28 col10\" >0.000000</td>\n",
       "      <td id=\"T_223ff_row28_col11\" class=\"data row28 col11\" >3.000000</td>\n",
       "      <td id=\"T_223ff_row28_col12\" class=\"data row28 col12\" >['ZONEHVAC:IDEALLOADSAIRSYSTEM', 'ZONEHVAC:EQUIPMENTLIST', 'ZONEHVAC:EQUIPMENTCONNECTIONS']</td>\n",
       "      <td id=\"T_223ff_row28_col13\" class=\"data row28 col13\" >OK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_223ff_level0_row29\" class=\"row_heading level0 row29\" >29</th>\n",
       "      <td id=\"T_223ff_row29_col0\" class=\"data row29 col0\" >hvac_equipment.parquet</td>\n",
       "      <td id=\"T_223ff_row29_col1\" class=\"data row29 col1\" >parsed_data/idf_data/by_category/hvac_equipment.parquet</td>\n",
       "      <td id=\"T_223ff_row29_col2\" class=\"data row29 col2\" >✓</td>\n",
       "      <td id=\"T_223ff_row29_col3\" class=\"data row29 col3\" >48</td>\n",
       "      <td id=\"T_223ff_row29_col4\" class=\"data row29 col4\" >maximum_heating_supply_air_temperature</td>\n",
       "      <td id=\"T_223ff_row29_col5\" class=\"data row29 col5\" >Expected</td>\n",
       "      <td id=\"T_223ff_row29_col6\" class=\"data row29 col6\" >float</td>\n",
       "      <td id=\"T_223ff_row29_col7\" class=\"data row29 col7\" >str</td>\n",
       "      <td id=\"T_223ff_row29_col8\" class=\"data row29 col8\" >✓</td>\n",
       "      <td id=\"T_223ff_row29_col9\" class=\"data row29 col9\" >✗</td>\n",
       "      <td id=\"T_223ff_row29_col10\" class=\"data row29 col10\" >32.000000</td>\n",
       "      <td id=\"T_223ff_row29_col11\" class=\"data row29 col11\" >3.000000</td>\n",
       "      <td id=\"T_223ff_row29_col12\" class=\"data row29 col12\" >['53.68235607082006', '66.71646764117767', '62.25159520578147']</td>\n",
       "      <td id=\"T_223ff_row29_col13\" class=\"data row29 col13\" >ISSUE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_223ff_level0_row30\" class=\"row_heading level0 row30\" >30</th>\n",
       "      <td id=\"T_223ff_row30_col0\" class=\"data row30 col0\" >hvac_equipment.parquet</td>\n",
       "      <td id=\"T_223ff_row30_col1\" class=\"data row30 col1\" >parsed_data/idf_data/by_category/hvac_equipment.parquet</td>\n",
       "      <td id=\"T_223ff_row30_col2\" class=\"data row30 col2\" >✓</td>\n",
       "      <td id=\"T_223ff_row30_col3\" class=\"data row30 col3\" >48</td>\n",
       "      <td id=\"T_223ff_row30_col4\" class=\"data row30 col4\" >minimum_cooling_supply_air_temperature</td>\n",
       "      <td id=\"T_223ff_row30_col5\" class=\"data row30 col5\" >Expected</td>\n",
       "      <td id=\"T_223ff_row30_col6\" class=\"data row30 col6\" >float</td>\n",
       "      <td id=\"T_223ff_row30_col7\" class=\"data row30 col7\" >str</td>\n",
       "      <td id=\"T_223ff_row30_col8\" class=\"data row30 col8\" >✓</td>\n",
       "      <td id=\"T_223ff_row30_col9\" class=\"data row30 col9\" >✗</td>\n",
       "      <td id=\"T_223ff_row30_col10\" class=\"data row30 col10\" >32.000000</td>\n",
       "      <td id=\"T_223ff_row30_col11\" class=\"data row30 col11\" >3.000000</td>\n",
       "      <td id=\"T_223ff_row30_col12\" class=\"data row30 col12\" >['13.353398974845822', '13.585902543310588', '12.058194506536221']</td>\n",
       "      <td id=\"T_223ff_row30_col13\" class=\"data row30 col13\" >ISSUE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_223ff_level0_row31\" class=\"row_heading level0 row31\" >31</th>\n",
       "      <td id=\"T_223ff_row31_col0\" class=\"data row31 col0\" >hvac_equipment.parquet</td>\n",
       "      <td id=\"T_223ff_row31_col1\" class=\"data row31 col1\" >parsed_data/idf_data/by_category/hvac_equipment.parquet</td>\n",
       "      <td id=\"T_223ff_row31_col2\" class=\"data row31 col2\" >✓</td>\n",
       "      <td id=\"T_223ff_row31_col3\" class=\"data row31 col3\" >48</td>\n",
       "      <td id=\"T_223ff_row31_col4\" class=\"data row31 col4\" >name</td>\n",
       "      <td id=\"T_223ff_row31_col5\" class=\"data row31 col5\" >DISCOVERED</td>\n",
       "      <td id=\"T_223ff_row31_col6\" class=\"data row31 col6\" >N/A</td>\n",
       "      <td id=\"T_223ff_row31_col7\" class=\"data row31 col7\" >str</td>\n",
       "      <td id=\"T_223ff_row31_col8\" class=\"data row31 col8\" >✓</td>\n",
       "      <td id=\"T_223ff_row31_col9\" class=\"data row31 col9\" >N/A</td>\n",
       "      <td id=\"T_223ff_row31_col10\" class=\"data row31 col10\" >32.000000</td>\n",
       "      <td id=\"T_223ff_row31_col11\" class=\"data row31 col11\" >11.000000</td>\n",
       "      <td id=\"T_223ff_row31_col12\" class=\"data row31 col12\" >['Zone1_FrontPerimeter_Ideal_Loads', 'Zone1_RightPerimeter_Ideal_Loads', 'Zone1_RearPerimeter_Ideal_Loads']</td>\n",
       "      <td id=\"T_223ff_row31_col13\" class=\"data row31 col13\" >EXTRA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_223ff_level0_row32\" class=\"row_heading level0 row32\" >32</th>\n",
       "      <td id=\"T_223ff_row32_col0\" class=\"data row32 col0\" >hvac_equipment.parquet</td>\n",
       "      <td id=\"T_223ff_row32_col1\" class=\"data row32 col1\" >parsed_data/idf_data/by_category/hvac_equipment.parquet</td>\n",
       "      <td id=\"T_223ff_row32_col2\" class=\"data row32 col2\" >✓</td>\n",
       "      <td id=\"T_223ff_row32_col3\" class=\"data row32 col3\" >48</td>\n",
       "      <td id=\"T_223ff_row32_col4\" class=\"data row32 col4\" >availability_schedule_name</td>\n",
       "      <td id=\"T_223ff_row32_col5\" class=\"data row32 col5\" >DISCOVERED</td>\n",
       "      <td id=\"T_223ff_row32_col6\" class=\"data row32 col6\" >N/A</td>\n",
       "      <td id=\"T_223ff_row32_col7\" class=\"data row32 col7\" >str</td>\n",
       "      <td id=\"T_223ff_row32_col8\" class=\"data row32 col8\" >✓</td>\n",
       "      <td id=\"T_223ff_row32_col9\" class=\"data row32 col9\" >N/A</td>\n",
       "      <td id=\"T_223ff_row32_col10\" class=\"data row32 col10\" >32.000000</td>\n",
       "      <td id=\"T_223ff_row32_col11\" class=\"data row32 col11\" >2.000000</td>\n",
       "      <td id=\"T_223ff_row32_col12\" class=\"data row32 col12\" >['VentSched_Twoandahalfstory_House', 'HVAC_Avail_Sched']</td>\n",
       "      <td id=\"T_223ff_row32_col13\" class=\"data row32 col13\" >EXTRA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_223ff_level0_row33\" class=\"row_heading level0 row33\" >33</th>\n",
       "      <td id=\"T_223ff_row33_col0\" class=\"data row33 col0\" >hvac_equipment.parquet</td>\n",
       "      <td id=\"T_223ff_row33_col1\" class=\"data row33 col1\" >parsed_data/idf_data/by_category/hvac_equipment.parquet</td>\n",
       "      <td id=\"T_223ff_row33_col2\" class=\"data row33 col2\" >✓</td>\n",
       "      <td id=\"T_223ff_row33_col3\" class=\"data row33 col3\" >48</td>\n",
       "      <td id=\"T_223ff_row33_col4\" class=\"data row33 col4\" >zone_supply_air_node_name</td>\n",
       "      <td id=\"T_223ff_row33_col5\" class=\"data row33 col5\" >DISCOVERED</td>\n",
       "      <td id=\"T_223ff_row33_col6\" class=\"data row33 col6\" >N/A</td>\n",
       "      <td id=\"T_223ff_row33_col7\" class=\"data row33 col7\" >str</td>\n",
       "      <td id=\"T_223ff_row33_col8\" class=\"data row33 col8\" >✓</td>\n",
       "      <td id=\"T_223ff_row33_col9\" class=\"data row33 col9\" >N/A</td>\n",
       "      <td id=\"T_223ff_row33_col10\" class=\"data row33 col10\" >32.000000</td>\n",
       "      <td id=\"T_223ff_row33_col11\" class=\"data row33 col11\" >11.000000</td>\n",
       "      <td id=\"T_223ff_row33_col12\" class=\"data row33 col12\" >['Zone1_FrontPerimeter_INLETS', 'Zone1_RightPerimeter_INLETS', 'Zone1_RearPerimeter_INLETS']</td>\n",
       "      <td id=\"T_223ff_row33_col13\" class=\"data row33 col13\" >EXTRA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_223ff_level0_row34\" class=\"row_heading level0 row34\" >34</th>\n",
       "      <td id=\"T_223ff_row34_col0\" class=\"data row34 col0\" >hvac_equipment.parquet</td>\n",
       "      <td id=\"T_223ff_row34_col1\" class=\"data row34 col1\" >parsed_data/idf_data/by_category/hvac_equipment.parquet</td>\n",
       "      <td id=\"T_223ff_row34_col2\" class=\"data row34 col2\" >✓</td>\n",
       "      <td id=\"T_223ff_row34_col3\" class=\"data row34 col3\" >48</td>\n",
       "      <td id=\"T_223ff_row34_col4\" class=\"data row34 col4\" >maximum_heating_supply_air_temperature_numeric</td>\n",
       "      <td id=\"T_223ff_row34_col5\" class=\"data row34 col5\" >DISCOVERED</td>\n",
       "      <td id=\"T_223ff_row34_col6\" class=\"data row34 col6\" >N/A</td>\n",
       "      <td id=\"T_223ff_row34_col7\" class=\"data row34 col7\" >float</td>\n",
       "      <td id=\"T_223ff_row34_col8\" class=\"data row34 col8\" >✓</td>\n",
       "      <td id=\"T_223ff_row34_col9\" class=\"data row34 col9\" >N/A</td>\n",
       "      <td id=\"T_223ff_row34_col10\" class=\"data row34 col10\" >32.000000</td>\n",
       "      <td id=\"T_223ff_row34_col11\" class=\"data row34 col11\" >3.000000</td>\n",
       "      <td id=\"T_223ff_row34_col12\" class=\"data row34 col12\" >[53.68235607082006, 66.71646764117767, 62.25159520578147]</td>\n",
       "      <td id=\"T_223ff_row34_col13\" class=\"data row34 col13\" >EXTRA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_223ff_level0_row35\" class=\"row_heading level0 row35\" >35</th>\n",
       "      <td id=\"T_223ff_row35_col0\" class=\"data row35 col0\" >hvac_equipment.parquet</td>\n",
       "      <td id=\"T_223ff_row35_col1\" class=\"data row35 col1\" >parsed_data/idf_data/by_category/hvac_equipment.parquet</td>\n",
       "      <td id=\"T_223ff_row35_col2\" class=\"data row35 col2\" >✓</td>\n",
       "      <td id=\"T_223ff_row35_col3\" class=\"data row35 col3\" >48</td>\n",
       "      <td id=\"T_223ff_row35_col4\" class=\"data row35 col4\" >minimum_cooling_supply_air_temperature_numeric</td>\n",
       "      <td id=\"T_223ff_row35_col5\" class=\"data row35 col5\" >DISCOVERED</td>\n",
       "      <td id=\"T_223ff_row35_col6\" class=\"data row35 col6\" >N/A</td>\n",
       "      <td id=\"T_223ff_row35_col7\" class=\"data row35 col7\" >float</td>\n",
       "      <td id=\"T_223ff_row35_col8\" class=\"data row35 col8\" >✓</td>\n",
       "      <td id=\"T_223ff_row35_col9\" class=\"data row35 col9\" >N/A</td>\n",
       "      <td id=\"T_223ff_row35_col10\" class=\"data row35 col10\" >32.000000</td>\n",
       "      <td id=\"T_223ff_row35_col11\" class=\"data row35 col11\" >3.000000</td>\n",
       "      <td id=\"T_223ff_row35_col12\" class=\"data row35 col12\" >[13.353398974845822, 13.585902543310588, 12.058194506536221]</td>\n",
       "      <td id=\"T_223ff_row35_col13\" class=\"data row35 col13\" >EXTRA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_223ff_level0_row36\" class=\"row_heading level0 row36\" >36</th>\n",
       "      <td id=\"T_223ff_row36_col0\" class=\"data row36 col0\" >hvac_equipment.parquet</td>\n",
       "      <td id=\"T_223ff_row36_col1\" class=\"data row36 col1\" >parsed_data/idf_data/by_category/hvac_equipment.parquet</td>\n",
       "      <td id=\"T_223ff_row36_col2\" class=\"data row36 col2\" >✓</td>\n",
       "      <td id=\"T_223ff_row36_col3\" class=\"data row36 col3\" >48</td>\n",
       "      <td id=\"T_223ff_row36_col4\" class=\"data row36 col4\" >maximum_heating_supply_air_humidity_ratio</td>\n",
       "      <td id=\"T_223ff_row36_col5\" class=\"data row36 col5\" >DISCOVERED</td>\n",
       "      <td id=\"T_223ff_row36_col6\" class=\"data row36 col6\" >N/A</td>\n",
       "      <td id=\"T_223ff_row36_col7\" class=\"data row36 col7\" >str</td>\n",
       "      <td id=\"T_223ff_row36_col8\" class=\"data row36 col8\" >✓</td>\n",
       "      <td id=\"T_223ff_row36_col9\" class=\"data row36 col9\" >N/A</td>\n",
       "      <td id=\"T_223ff_row36_col10\" class=\"data row36 col10\" >32.000000</td>\n",
       "      <td id=\"T_223ff_row36_col11\" class=\"data row36 col11\" >1.000000</td>\n",
       "      <td id=\"T_223ff_row36_col12\" class=\"data row36 col12\" >['0.0156']</td>\n",
       "      <td id=\"T_223ff_row36_col13\" class=\"data row36 col13\" >EXTRA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_223ff_level0_row37\" class=\"row_heading level0 row37\" >37</th>\n",
       "      <td id=\"T_223ff_row37_col0\" class=\"data row37 col0\" >hvac_equipment.parquet</td>\n",
       "      <td id=\"T_223ff_row37_col1\" class=\"data row37 col1\" >parsed_data/idf_data/by_category/hvac_equipment.parquet</td>\n",
       "      <td id=\"T_223ff_row37_col2\" class=\"data row37 col2\" >✓</td>\n",
       "      <td id=\"T_223ff_row37_col3\" class=\"data row37 col3\" >48</td>\n",
       "      <td id=\"T_223ff_row37_col4\" class=\"data row37 col4\" >maximum_heating_supply_air_humidity_ratio_numeric</td>\n",
       "      <td id=\"T_223ff_row37_col5\" class=\"data row37 col5\" >DISCOVERED</td>\n",
       "      <td id=\"T_223ff_row37_col6\" class=\"data row37 col6\" >N/A</td>\n",
       "      <td id=\"T_223ff_row37_col7\" class=\"data row37 col7\" >float</td>\n",
       "      <td id=\"T_223ff_row37_col8\" class=\"data row37 col8\" >✓</td>\n",
       "      <td id=\"T_223ff_row37_col9\" class=\"data row37 col9\" >N/A</td>\n",
       "      <td id=\"T_223ff_row37_col10\" class=\"data row37 col10\" >32.000000</td>\n",
       "      <td id=\"T_223ff_row37_col11\" class=\"data row37 col11\" >1.000000</td>\n",
       "      <td id=\"T_223ff_row37_col12\" class=\"data row37 col12\" >[0.0156]</td>\n",
       "      <td id=\"T_223ff_row37_col13\" class=\"data row37 col13\" >EXTRA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_223ff_level0_row38\" class=\"row_heading level0 row38\" >38</th>\n",
       "      <td id=\"T_223ff_row38_col0\" class=\"data row38 col0\" >hvac_equipment.parquet</td>\n",
       "      <td id=\"T_223ff_row38_col1\" class=\"data row38 col1\" >parsed_data/idf_data/by_category/hvac_equipment.parquet</td>\n",
       "      <td id=\"T_223ff_row38_col2\" class=\"data row38 col2\" >✓</td>\n",
       "      <td id=\"T_223ff_row38_col3\" class=\"data row38 col3\" >48</td>\n",
       "      <td id=\"T_223ff_row38_col4\" class=\"data row38 col4\" >minimum_cooling_supply_air_humidity_ratio</td>\n",
       "      <td id=\"T_223ff_row38_col5\" class=\"data row38 col5\" >DISCOVERED</td>\n",
       "      <td id=\"T_223ff_row38_col6\" class=\"data row38 col6\" >N/A</td>\n",
       "      <td id=\"T_223ff_row38_col7\" class=\"data row38 col7\" >str</td>\n",
       "      <td id=\"T_223ff_row38_col8\" class=\"data row38 col8\" >✓</td>\n",
       "      <td id=\"T_223ff_row38_col9\" class=\"data row38 col9\" >N/A</td>\n",
       "      <td id=\"T_223ff_row38_col10\" class=\"data row38 col10\" >32.000000</td>\n",
       "      <td id=\"T_223ff_row38_col11\" class=\"data row38 col11\" >1.000000</td>\n",
       "      <td id=\"T_223ff_row38_col12\" class=\"data row38 col12\" >['0.0077']</td>\n",
       "      <td id=\"T_223ff_row38_col13\" class=\"data row38 col13\" >EXTRA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_223ff_level0_row39\" class=\"row_heading level0 row39\" >39</th>\n",
       "      <td id=\"T_223ff_row39_col0\" class=\"data row39 col0\" >hvac_equipment.parquet</td>\n",
       "      <td id=\"T_223ff_row39_col1\" class=\"data row39 col1\" >parsed_data/idf_data/by_category/hvac_equipment.parquet</td>\n",
       "      <td id=\"T_223ff_row39_col2\" class=\"data row39 col2\" >✓</td>\n",
       "      <td id=\"T_223ff_row39_col3\" class=\"data row39 col3\" >48</td>\n",
       "      <td id=\"T_223ff_row39_col4\" class=\"data row39 col4\" >minimum_cooling_supply_air_humidity_ratio_numeric</td>\n",
       "      <td id=\"T_223ff_row39_col5\" class=\"data row39 col5\" >DISCOVERED</td>\n",
       "      <td id=\"T_223ff_row39_col6\" class=\"data row39 col6\" >N/A</td>\n",
       "      <td id=\"T_223ff_row39_col7\" class=\"data row39 col7\" >float</td>\n",
       "      <td id=\"T_223ff_row39_col8\" class=\"data row39 col8\" >✓</td>\n",
       "      <td id=\"T_223ff_row39_col9\" class=\"data row39 col9\" >N/A</td>\n",
       "      <td id=\"T_223ff_row39_col10\" class=\"data row39 col10\" >32.000000</td>\n",
       "      <td id=\"T_223ff_row39_col11\" class=\"data row39 col11\" >1.000000</td>\n",
       "      <td id=\"T_223ff_row39_col12\" class=\"data row39 col12\" >[0.0077]</td>\n",
       "      <td id=\"T_223ff_row39_col13\" class=\"data row39 col13\" >EXTRA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_223ff_level0_row40\" class=\"row_heading level0 row40\" >40</th>\n",
       "      <td id=\"T_223ff_row40_col0\" class=\"data row40 col0\" >hvac_equipment.parquet</td>\n",
       "      <td id=\"T_223ff_row40_col1\" class=\"data row40 col1\" >parsed_data/idf_data/by_category/hvac_equipment.parquet</td>\n",
       "      <td id=\"T_223ff_row40_col2\" class=\"data row40 col2\" >✓</td>\n",
       "      <td id=\"T_223ff_row40_col3\" class=\"data row40 col3\" >48</td>\n",
       "      <td id=\"T_223ff_row40_col4\" class=\"data row40 col4\" >heating_limit</td>\n",
       "      <td id=\"T_223ff_row40_col5\" class=\"data row40 col5\" >DISCOVERED</td>\n",
       "      <td id=\"T_223ff_row40_col6\" class=\"data row40 col6\" >N/A</td>\n",
       "      <td id=\"T_223ff_row40_col7\" class=\"data row40 col7\" >str</td>\n",
       "      <td id=\"T_223ff_row40_col8\" class=\"data row40 col8\" >✓</td>\n",
       "      <td id=\"T_223ff_row40_col9\" class=\"data row40 col9\" >N/A</td>\n",
       "      <td id=\"T_223ff_row40_col10\" class=\"data row40 col10\" >32.000000</td>\n",
       "      <td id=\"T_223ff_row40_col11\" class=\"data row40 col11\" >1.000000</td>\n",
       "      <td id=\"T_223ff_row40_col12\" class=\"data row40 col12\" >['LimitFlowRateAndCapacity']</td>\n",
       "      <td id=\"T_223ff_row40_col13\" class=\"data row40 col13\" >EXTRA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_223ff_level0_row41\" class=\"row_heading level0 row41\" >41</th>\n",
       "      <td id=\"T_223ff_row41_col0\" class=\"data row41 col0\" >hvac_equipment.parquet</td>\n",
       "      <td id=\"T_223ff_row41_col1\" class=\"data row41 col1\" >parsed_data/idf_data/by_category/hvac_equipment.parquet</td>\n",
       "      <td id=\"T_223ff_row41_col2\" class=\"data row41 col2\" >✓</td>\n",
       "      <td id=\"T_223ff_row41_col3\" class=\"data row41 col3\" >48</td>\n",
       "      <td id=\"T_223ff_row41_col4\" class=\"data row41 col4\" >maximum_heating_air_flow_rate</td>\n",
       "      <td id=\"T_223ff_row41_col5\" class=\"data row41 col5\" >DISCOVERED</td>\n",
       "      <td id=\"T_223ff_row41_col6\" class=\"data row41 col6\" >N/A</td>\n",
       "      <td id=\"T_223ff_row41_col7\" class=\"data row41 col7\" >str</td>\n",
       "      <td id=\"T_223ff_row41_col8\" class=\"data row41 col8\" >✓</td>\n",
       "      <td id=\"T_223ff_row41_col9\" class=\"data row41 col9\" >N/A</td>\n",
       "      <td id=\"T_223ff_row41_col10\" class=\"data row41 col10\" >32.000000</td>\n",
       "      <td id=\"T_223ff_row41_col11\" class=\"data row41 col11\" >1.000000</td>\n",
       "      <td id=\"T_223ff_row41_col12\" class=\"data row41 col12\" >['Autosize']</td>\n",
       "      <td id=\"T_223ff_row41_col13\" class=\"data row41 col13\" >EXTRA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_223ff_level0_row42\" class=\"row_heading level0 row42\" >42</th>\n",
       "      <td id=\"T_223ff_row42_col0\" class=\"data row42 col0\" >hvac_equipment.parquet</td>\n",
       "      <td id=\"T_223ff_row42_col1\" class=\"data row42 col1\" >parsed_data/idf_data/by_category/hvac_equipment.parquet</td>\n",
       "      <td id=\"T_223ff_row42_col2\" class=\"data row42 col2\" >✓</td>\n",
       "      <td id=\"T_223ff_row42_col3\" class=\"data row42 col3\" >48</td>\n",
       "      <td id=\"T_223ff_row42_col4\" class=\"data row42 col4\" >maximum_sensible_heating_capacity</td>\n",
       "      <td id=\"T_223ff_row42_col5\" class=\"data row42 col5\" >DISCOVERED</td>\n",
       "      <td id=\"T_223ff_row42_col6\" class=\"data row42 col6\" >N/A</td>\n",
       "      <td id=\"T_223ff_row42_col7\" class=\"data row42 col7\" >str</td>\n",
       "      <td id=\"T_223ff_row42_col8\" class=\"data row42 col8\" >✓</td>\n",
       "      <td id=\"T_223ff_row42_col9\" class=\"data row42 col9\" >N/A</td>\n",
       "      <td id=\"T_223ff_row42_col10\" class=\"data row42 col10\" >32.000000</td>\n",
       "      <td id=\"T_223ff_row42_col11\" class=\"data row42 col11\" >1.000000</td>\n",
       "      <td id=\"T_223ff_row42_col12\" class=\"data row42 col12\" >['Autosize']</td>\n",
       "      <td id=\"T_223ff_row42_col13\" class=\"data row42 col13\" >EXTRA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_223ff_level0_row43\" class=\"row_heading level0 row43\" >43</th>\n",
       "      <td id=\"T_223ff_row43_col0\" class=\"data row43 col0\" >hvac_equipment.parquet</td>\n",
       "      <td id=\"T_223ff_row43_col1\" class=\"data row43 col1\" >parsed_data/idf_data/by_category/hvac_equipment.parquet</td>\n",
       "      <td id=\"T_223ff_row43_col2\" class=\"data row43 col2\" >✓</td>\n",
       "      <td id=\"T_223ff_row43_col3\" class=\"data row43 col3\" >48</td>\n",
       "      <td id=\"T_223ff_row43_col4\" class=\"data row43 col4\" >cooling_limit</td>\n",
       "      <td id=\"T_223ff_row43_col5\" class=\"data row43 col5\" >DISCOVERED</td>\n",
       "      <td id=\"T_223ff_row43_col6\" class=\"data row43 col6\" >N/A</td>\n",
       "      <td id=\"T_223ff_row43_col7\" class=\"data row43 col7\" >str</td>\n",
       "      <td id=\"T_223ff_row43_col8\" class=\"data row43 col8\" >✓</td>\n",
       "      <td id=\"T_223ff_row43_col9\" class=\"data row43 col9\" >N/A</td>\n",
       "      <td id=\"T_223ff_row43_col10\" class=\"data row43 col10\" >32.000000</td>\n",
       "      <td id=\"T_223ff_row43_col11\" class=\"data row43 col11\" >1.000000</td>\n",
       "      <td id=\"T_223ff_row43_col12\" class=\"data row43 col12\" >['LimitFlowRateAndCapacity']</td>\n",
       "      <td id=\"T_223ff_row43_col13\" class=\"data row43 col13\" >EXTRA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_223ff_level0_row44\" class=\"row_heading level0 row44\" >44</th>\n",
       "      <td id=\"T_223ff_row44_col0\" class=\"data row44 col0\" >hvac_equipment.parquet</td>\n",
       "      <td id=\"T_223ff_row44_col1\" class=\"data row44 col1\" >parsed_data/idf_data/by_category/hvac_equipment.parquet</td>\n",
       "      <td id=\"T_223ff_row44_col2\" class=\"data row44 col2\" >✓</td>\n",
       "      <td id=\"T_223ff_row44_col3\" class=\"data row44 col3\" >48</td>\n",
       "      <td id=\"T_223ff_row44_col4\" class=\"data row44 col4\" >maximum_cooling_air_flow_rate</td>\n",
       "      <td id=\"T_223ff_row44_col5\" class=\"data row44 col5\" >DISCOVERED</td>\n",
       "      <td id=\"T_223ff_row44_col6\" class=\"data row44 col6\" >N/A</td>\n",
       "      <td id=\"T_223ff_row44_col7\" class=\"data row44 col7\" >str</td>\n",
       "      <td id=\"T_223ff_row44_col8\" class=\"data row44 col8\" >✓</td>\n",
       "      <td id=\"T_223ff_row44_col9\" class=\"data row44 col9\" >N/A</td>\n",
       "      <td id=\"T_223ff_row44_col10\" class=\"data row44 col10\" >32.000000</td>\n",
       "      <td id=\"T_223ff_row44_col11\" class=\"data row44 col11\" >1.000000</td>\n",
       "      <td id=\"T_223ff_row44_col12\" class=\"data row44 col12\" >['Autosize']</td>\n",
       "      <td id=\"T_223ff_row44_col13\" class=\"data row44 col13\" >EXTRA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_223ff_level0_row45\" class=\"row_heading level0 row45\" >45</th>\n",
       "      <td id=\"T_223ff_row45_col0\" class=\"data row45 col0\" >hvac_equipment.parquet</td>\n",
       "      <td id=\"T_223ff_row45_col1\" class=\"data row45 col1\" >parsed_data/idf_data/by_category/hvac_equipment.parquet</td>\n",
       "      <td id=\"T_223ff_row45_col2\" class=\"data row45 col2\" >✓</td>\n",
       "      <td id=\"T_223ff_row45_col3\" class=\"data row45 col3\" >48</td>\n",
       "      <td id=\"T_223ff_row45_col4\" class=\"data row45 col4\" >maximum_total_cooling_capacity</td>\n",
       "      <td id=\"T_223ff_row45_col5\" class=\"data row45 col5\" >DISCOVERED</td>\n",
       "      <td id=\"T_223ff_row45_col6\" class=\"data row45 col6\" >N/A</td>\n",
       "      <td id=\"T_223ff_row45_col7\" class=\"data row45 col7\" >str</td>\n",
       "      <td id=\"T_223ff_row45_col8\" class=\"data row45 col8\" >✓</td>\n",
       "      <td id=\"T_223ff_row45_col9\" class=\"data row45 col9\" >N/A</td>\n",
       "      <td id=\"T_223ff_row45_col10\" class=\"data row45 col10\" >32.000000</td>\n",
       "      <td id=\"T_223ff_row45_col11\" class=\"data row45 col11\" >1.000000</td>\n",
       "      <td id=\"T_223ff_row45_col12\" class=\"data row45 col12\" >['Autosize']</td>\n",
       "      <td id=\"T_223ff_row45_col13\" class=\"data row45 col13\" >EXTRA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_223ff_level0_row46\" class=\"row_heading level0 row46\" >46</th>\n",
       "      <td id=\"T_223ff_row46_col0\" class=\"data row46 col0\" >hvac_equipment.parquet</td>\n",
       "      <td id=\"T_223ff_row46_col1\" class=\"data row46 col1\" >parsed_data/idf_data/by_category/hvac_equipment.parquet</td>\n",
       "      <td id=\"T_223ff_row46_col2\" class=\"data row46 col2\" >✓</td>\n",
       "      <td id=\"T_223ff_row46_col3\" class=\"data row46 col3\" >48</td>\n",
       "      <td id=\"T_223ff_row46_col4\" class=\"data row46 col4\" >dehumidification_control_type</td>\n",
       "      <td id=\"T_223ff_row46_col5\" class=\"data row46 col5\" >DISCOVERED</td>\n",
       "      <td id=\"T_223ff_row46_col6\" class=\"data row46 col6\" >N/A</td>\n",
       "      <td id=\"T_223ff_row46_col7\" class=\"data row46 col7\" >str</td>\n",
       "      <td id=\"T_223ff_row46_col8\" class=\"data row46 col8\" >✓</td>\n",
       "      <td id=\"T_223ff_row46_col9\" class=\"data row46 col9\" >N/A</td>\n",
       "      <td id=\"T_223ff_row46_col10\" class=\"data row46 col10\" >32.000000</td>\n",
       "      <td id=\"T_223ff_row46_col11\" class=\"data row46 col11\" >1.000000</td>\n",
       "      <td id=\"T_223ff_row46_col12\" class=\"data row46 col12\" >['None']</td>\n",
       "      <td id=\"T_223ff_row46_col13\" class=\"data row46 col13\" >EXTRA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_223ff_level0_row47\" class=\"row_heading level0 row47\" >47</th>\n",
       "      <td id=\"T_223ff_row47_col0\" class=\"data row47 col0\" >hvac_equipment.parquet</td>\n",
       "      <td id=\"T_223ff_row47_col1\" class=\"data row47 col1\" >parsed_data/idf_data/by_category/hvac_equipment.parquet</td>\n",
       "      <td id=\"T_223ff_row47_col2\" class=\"data row47 col2\" >✓</td>\n",
       "      <td id=\"T_223ff_row47_col3\" class=\"data row47 col3\" >48</td>\n",
       "      <td id=\"T_223ff_row47_col4\" class=\"data row47 col4\" >cooling_sensible_heat_ratio</td>\n",
       "      <td id=\"T_223ff_row47_col5\" class=\"data row47 col5\" >DISCOVERED</td>\n",
       "      <td id=\"T_223ff_row47_col6\" class=\"data row47 col6\" >N/A</td>\n",
       "      <td id=\"T_223ff_row47_col7\" class=\"data row47 col7\" >str</td>\n",
       "      <td id=\"T_223ff_row47_col8\" class=\"data row47 col8\" >✓</td>\n",
       "      <td id=\"T_223ff_row47_col9\" class=\"data row47 col9\" >N/A</td>\n",
       "      <td id=\"T_223ff_row47_col10\" class=\"data row47 col10\" >32.000000</td>\n",
       "      <td id=\"T_223ff_row47_col11\" class=\"data row47 col11\" >1.000000</td>\n",
       "      <td id=\"T_223ff_row47_col12\" class=\"data row47 col12\" >['0.7']</td>\n",
       "      <td id=\"T_223ff_row47_col13\" class=\"data row47 col13\" >EXTRA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_223ff_level0_row48\" class=\"row_heading level0 row48\" >48</th>\n",
       "      <td id=\"T_223ff_row48_col0\" class=\"data row48 col0\" >hvac_equipment.parquet</td>\n",
       "      <td id=\"T_223ff_row48_col1\" class=\"data row48 col1\" >parsed_data/idf_data/by_category/hvac_equipment.parquet</td>\n",
       "      <td id=\"T_223ff_row48_col2\" class=\"data row48 col2\" >✓</td>\n",
       "      <td id=\"T_223ff_row48_col3\" class=\"data row48 col3\" >48</td>\n",
       "      <td id=\"T_223ff_row48_col4\" class=\"data row48 col4\" >cooling_sensible_heat_ratio_numeric</td>\n",
       "      <td id=\"T_223ff_row48_col5\" class=\"data row48 col5\" >DISCOVERED</td>\n",
       "      <td id=\"T_223ff_row48_col6\" class=\"data row48 col6\" >N/A</td>\n",
       "      <td id=\"T_223ff_row48_col7\" class=\"data row48 col7\" >float</td>\n",
       "      <td id=\"T_223ff_row48_col8\" class=\"data row48 col8\" >✓</td>\n",
       "      <td id=\"T_223ff_row48_col9\" class=\"data row48 col9\" >N/A</td>\n",
       "      <td id=\"T_223ff_row48_col10\" class=\"data row48 col10\" >32.000000</td>\n",
       "      <td id=\"T_223ff_row48_col11\" class=\"data row48 col11\" >1.000000</td>\n",
       "      <td id=\"T_223ff_row48_col12\" class=\"data row48 col12\" >[0.7]</td>\n",
       "      <td id=\"T_223ff_row48_col13\" class=\"data row48 col13\" >EXTRA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_223ff_level0_row49\" class=\"row_heading level0 row49\" >49</th>\n",
       "      <td id=\"T_223ff_row49_col0\" class=\"data row49 col0\" >hvac_equipment.parquet</td>\n",
       "      <td id=\"T_223ff_row49_col1\" class=\"data row49 col1\" >parsed_data/idf_data/by_category/hvac_equipment.parquet</td>\n",
       "      <td id=\"T_223ff_row49_col2\" class=\"data row49 col2\" >✓</td>\n",
       "      <td id=\"T_223ff_row49_col3\" class=\"data row49 col3\" >48</td>\n",
       "      <td id=\"T_223ff_row49_col4\" class=\"data row49 col4\" >humidification_control_type</td>\n",
       "      <td id=\"T_223ff_row49_col5\" class=\"data row49 col5\" >DISCOVERED</td>\n",
       "      <td id=\"T_223ff_row49_col6\" class=\"data row49 col6\" >N/A</td>\n",
       "      <td id=\"T_223ff_row49_col7\" class=\"data row49 col7\" >str</td>\n",
       "      <td id=\"T_223ff_row49_col8\" class=\"data row49 col8\" >✓</td>\n",
       "      <td id=\"T_223ff_row49_col9\" class=\"data row49 col9\" >N/A</td>\n",
       "      <td id=\"T_223ff_row49_col10\" class=\"data row49 col10\" >32.000000</td>\n",
       "      <td id=\"T_223ff_row49_col11\" class=\"data row49 col11\" >1.000000</td>\n",
       "      <td id=\"T_223ff_row49_col12\" class=\"data row49 col12\" >['None']</td>\n",
       "      <td id=\"T_223ff_row49_col13\" class=\"data row49 col13\" >EXTRA</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x21375039220>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "DISCOVERED COLUMNS DETAILS (First 20)\n",
      "================================================================================\n",
      "                      File                                                     Path                                                               Column_Name Data_Type  Non_Null_Count  Unique_Count  Null_Percentage                                                                                                                                                 Sample_Values      Mean           Std  Min       Max\n",
      "0         lighting.parquet        parsed_data/idf_data/by_category/lighting.parquet                                                               object_type       str               0             1         0.000000                                                                                                                                                        LIGHTS       NaN           NaN  NaN       NaN\n",
      "1         lighting.parquet        parsed_data/idf_data/by_category/lighting.parquet                                                                      name       str               0             1         0.000000                                                                                                                                              Lights_ALL_ZONES       NaN           NaN  NaN       NaN\n",
      "2         lighting.parquet        parsed_data/idf_data/by_category/lighting.parquet                                                     zone_or_zonelist_name       str               0             1         0.000000                                                                                                                                                     ALL_ZONES       NaN           NaN  NaN       NaN\n",
      "3         lighting.parquet        parsed_data/idf_data/by_category/lighting.parquet                                           design_level_calculation_method       str               0             1         0.000000                                                                                                                                                    Watts/Area       NaN           NaN  NaN       NaN\n",
      "4         lighting.parquet        parsed_data/idf_data/by_category/lighting.parquet                                         watts_per_zone_floor_area_numeric     float               0             3         0.000000                                                                                                                     0.0, 4.092524414409712, 4.980486110210785  3.024337  2.656516e+00  0.0  4.980486\n",
      "5         lighting.parquet        parsed_data/idf_data/by_category/lighting.parquet                                                       return_air_fraction       str               0             1         0.000000                                                                                                                                                           0.8       NaN           NaN  NaN       NaN\n",
      "6         lighting.parquet        parsed_data/idf_data/by_category/lighting.parquet                                               return_air_fraction_numeric     float               0             1         0.000000                                                                                                                                                           0.8  0.800000  1.359740e-16  0.8  0.800000\n",
      "7         lighting.parquet        parsed_data/idf_data/by_category/lighting.parquet                                                  fraction_radiant_numeric     float               0             1         0.000000                                                                                                                                                           0.1  0.100000  1.699675e-17  0.1  0.100000\n",
      "8         lighting.parquet        parsed_data/idf_data/by_category/lighting.parquet                                                  fraction_visible_numeric     float               0             1         0.000000                                                                                                                                                           0.1  0.100000  1.699675e-17  0.1  0.100000\n",
      "9         lighting.parquet        parsed_data/idf_data/by_category/lighting.parquet                                                      fraction_replaceable       str               0             1         0.000000                                                                                                                                                             1       NaN           NaN  NaN       NaN\n",
      "10        lighting.parquet        parsed_data/idf_data/by_category/lighting.parquet                                              fraction_replaceable_numeric     float               0             1         0.000000                                                                                                                                                           1.0  1.000000  0.000000e+00  1.0  1.000000\n",
      "11        lighting.parquet        parsed_data/idf_data/by_category/lighting.parquet                                                       end_use_subcategory       str               0             1         0.000000                                                                                                                                                       General       NaN           NaN  NaN       NaN\n",
      "12        lighting.parquet        parsed_data/idf_data/by_category/lighting.parquet                    return_air_fraction_calculated_from_plenum_temperature       str               0             1         0.000000                                                                                                                                                            No       NaN           NaN  NaN       NaN\n",
      "13        lighting.parquet        parsed_data/idf_data/by_category/lighting.parquet          return_air_fraction_function_of_plenum_temperature_coefficient_1       str               0             1         0.000000                                                                                                                                                             0       NaN           NaN  NaN       NaN\n",
      "14        lighting.parquet        parsed_data/idf_data/by_category/lighting.parquet  return_air_fraction_function_of_plenum_temperature_coefficient_1_numeric     float               0             1         0.000000                                                                                                                                                           0.0  0.000000  0.000000e+00  0.0  0.000000\n",
      "15        lighting.parquet        parsed_data/idf_data/by_category/lighting.parquet          return_air_fraction_function_of_plenum_temperature_coefficient_2       str               0             1         0.000000                                                                                                                                                             0       NaN           NaN  NaN       NaN\n",
      "16        lighting.parquet        parsed_data/idf_data/by_category/lighting.parquet  return_air_fraction_function_of_plenum_temperature_coefficient_2_numeric     float               0             1         0.000000                                                                                                                                                           0.0  0.000000  0.000000e+00  0.0  0.000000\n",
      "17        lighting.parquet        parsed_data/idf_data/by_category/lighting.parquet                                                                variant_id       str               0             1         0.000000                                                                                                                                                          base       NaN           NaN  NaN       NaN\n",
      "18  hvac_equipment.parquet  parsed_data/idf_data/by_category/hvac_equipment.parquet                                                                      name       str              32            11        66.666667  Zone1_FrontPerimeter_Ideal_Loads, Zone1_RightPerimeter_Ideal_Loads, Zone1_RearPerimeter_Ideal_Loads, Zone1_LeftPerimeter_Ideal_Loads, Zone1_Core_Ideal_Loads       NaN           NaN  NaN       NaN\n",
      "19  hvac_equipment.parquet  parsed_data/idf_data/by_category/hvac_equipment.parquet                                                availability_schedule_name       str              32             2        66.666667                                                                                                            VentSched_Twoandahalfstory_House, HVAC_Avail_Sched       NaN           NaN  NaN       NaN\n",
      "\n",
      "Results saved to: D:\\Documents\\daily\\E_Plus_2040_py\\output\\82e2b83f-5013-4270-8266-a37a67dbd4ff\\surrogate_data_check\n",
      "Files created:\n",
      "  - structure_check_details_20250627_051955.csv/xlsx\n",
      "  - structure_check_summary_20250627_051955.json\n",
      "  - discovered_columns_20250627_051955.csv/xlsx\n",
      "  - discovered_columns_full_20250627_051955.json\n",
      "  - structure_report_20250627_051955.md\n",
      "  - issues_only_20250627_051955.csv\n",
      "\n",
      "================================================================================\n",
      "FILES BY STATUS\n",
      "================================================================================\n",
      "Status\n",
      "EXTRA           20\n",
      "ISSUE           16\n",
      "MISSING FILE     7\n",
      "OK              20\n",
      "Name: Path, dtype: int64\n",
      "\n",
      "================================================================================\n",
      "COLUMN TYPE DISTRIBUTION\n",
      "================================================================================\n",
      "Column_Type\n",
      "DISCOVERED    260\n",
      "Expected      115\n",
      "N/A             7\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Optional, Any\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "class SurrogateDataChecker:\n",
    "    def __init__(self, job_output_dir: str):\n",
    "        self.job_output_dir = Path(job_output_dir)\n",
    "        self.results = []\n",
    "        self.discovered_columns = {}  # Store discovered extra columns\n",
    "        \n",
    "        # Updated expected structure based on new pipeline\n",
    "        self.expected_structure = {\n",
    "            # IDF Data - Updated column names based on _map_column_names\n",
    "            'parsed_data/idf_data/by_category/lighting.parquet': {\n",
    "                'columns': {\n",
    "                    'building_id': 'str',\n",
    "                    'zone_name': 'str',\n",
    "                    'object_name': 'str',\n",
    "                    'watts_per_zone_floor_area': 'float',  # Updated from LightingLevel\n",
    "                    'fraction_radiant': 'float',  # Updated from FractionRadiant\n",
    "                    'fraction_visible': 'float',  # Updated from FractionVisible\n",
    "                    'schedule_name': 'str'  # Updated from ScheduleName\n",
    "                }\n",
    "            },\n",
    "            'parsed_data/idf_data/by_category/hvac_equipment.parquet': {\n",
    "                'columns': {\n",
    "                    'building_id': 'str',\n",
    "                    'zone_name': 'str',\n",
    "                    'object_name': 'str',\n",
    "                    'object_type': 'str',\n",
    "                    'maximum_heating_supply_air_temperature': 'float',  # Updated\n",
    "                    'minimum_cooling_supply_air_temperature': 'float'   # Updated\n",
    "                }\n",
    "            },\n",
    "            'parsed_data/idf_data/by_category/materials_materials.parquet': {\n",
    "                'columns': {\n",
    "                    'building_id': 'str',\n",
    "                    'name': 'str',  # Updated from material_name\n",
    "                    'object_name': 'str',  # Added based on mapping\n",
    "                    'thickness': 'float',  # Updated from Thickness\n",
    "                    'conductivity': 'float',  # Updated from Conductivity\n",
    "                    'density': 'float',  # Updated from Density\n",
    "                    'specific_heat': 'float'  # Updated from SpecificHeat\n",
    "                }\n",
    "            },\n",
    "            'parsed_data/idf_data/by_category/infiltration.parquet': {\n",
    "                'columns': {\n",
    "                    'building_id': 'str',\n",
    "                    'zone_name': 'str',\n",
    "                    'object_name': 'str',\n",
    "                    'design_flow_rate_calculation_method': 'str',  # Updated\n",
    "                    'design_flow_rate': 'float'  # Updated\n",
    "                }\n",
    "            },\n",
    "            'parsed_data/idf_data/by_category/ventilation.parquet': {\n",
    "                'columns': {\n",
    "                    'building_id': 'str',\n",
    "                    'zone_name': 'str',\n",
    "                    'object_name': 'str',\n",
    "                    'outdoor_air_method': 'str',  # Updated\n",
    "                    'outdoor_air_flow_per_person': 'float'  # Updated\n",
    "                }\n",
    "            },\n",
    "            'parsed_data/idf_data/by_category/equipment.parquet': {  # Added\n",
    "                'columns': {\n",
    "                    'building_id': 'str',\n",
    "                    'zone_name': 'str',\n",
    "                    'object_name': 'str',\n",
    "                    'design_level': 'float',\n",
    "                    'watts_per_zone_floor_area': 'float',\n",
    "                    'schedule_name': 'str'\n",
    "                }\n",
    "            },\n",
    "            'parsed_data/idf_data/by_category/dhw.parquet': {  # Added\n",
    "                'columns': {\n",
    "                    'building_id': 'str',\n",
    "                    'object_name': 'str',\n",
    "                    'heater_maximum_capacity': 'float',\n",
    "                    'heater_thermal_efficiency': 'float',\n",
    "                    'tank_volume': 'float'\n",
    "                }\n",
    "            },\n",
    "            'parsed_data/idf_data/by_category/shading.parquet': {  # Added\n",
    "                'columns': {\n",
    "                    'building_id': 'str',\n",
    "                    'zone_name': 'str',\n",
    "                    'shading_control_name': 'str',\n",
    "                    'shading_type': 'str',\n",
    "                    'shading_control_type': 'str'\n",
    "                }\n",
    "            },\n",
    "            # SQL Results - Now supports multiple temporal resolutions\n",
    "            'parsed_data/sql_results/timeseries/aggregated/daily/zones_daily.parquet': {\n",
    "                'columns': {\n",
    "                    'building_id': 'str',\n",
    "                    'Zone': 'str',\n",
    "                    'Variable': 'str',\n",
    "                    'Value': 'float',\n",
    "                    'DateTime': 'datetime'  # May or may not exist\n",
    "                }\n",
    "            },\n",
    "            'parsed_data/sql_results/timeseries/aggregated/daily/hvac_daily.parquet': {\n",
    "                'columns': {\n",
    "                    'building_id': 'str',\n",
    "                    'Zone': 'str',\n",
    "                    'Variable': 'str',\n",
    "                    'Value': 'float'\n",
    "                }\n",
    "            },\n",
    "            'parsed_data/sql_results/timeseries/aggregated/daily/ventilation_daily.parquet': {\n",
    "                'columns': {\n",
    "                    'building_id': 'str',\n",
    "                    'Zone': 'str',\n",
    "                    'Variable': 'str',\n",
    "                    'Value': 'float'\n",
    "                }\n",
    "            },\n",
    "            'parsed_data/sql_results/timeseries/aggregated/hourly/zones_hourly.parquet': {  # Added\n",
    "                'columns': {\n",
    "                    'building_id': 'str',\n",
    "                    'Zone': 'str',\n",
    "                    'Variable': 'str',\n",
    "                    'Value': 'float'\n",
    "                }\n",
    "            },\n",
    "            'parsed_data/sql_results/timeseries/aggregated/monthly/zones_monthly.parquet': {  # Added\n",
    "                'columns': {\n",
    "                    'building_id': 'str',\n",
    "                    'Zone': 'str',\n",
    "                    'Variable': 'str',\n",
    "                    'Value': 'float'\n",
    "                }\n",
    "            },\n",
    "            # Relationships\n",
    "            'parsed_data/relationships/zone_mappings.parquet': {\n",
    "                'columns': {\n",
    "                    'building_id': 'str',\n",
    "                    'zone_name': 'str',\n",
    "                    'sql_zone_name': 'str',\n",
    "                    'zone_index': 'int',\n",
    "                    'floor_area': 'float',\n",
    "                    'volume': 'float'\n",
    "                }\n",
    "            },\n",
    "            'parsed_data/relationships/equipment_assignments.parquet': {  # Added\n",
    "                'columns': {\n",
    "                    'building_id': 'str',\n",
    "                    'zone_name': 'str',\n",
    "                    'equipment_type': 'str',\n",
    "                    'equipment_name': 'str',\n",
    "                    'assignment_type': 'str'\n",
    "                }\n",
    "            },\n",
    "            'parsed_data/metadata/building_registry.parquet': {\n",
    "                'columns': {\n",
    "                    'building_id': 'str',\n",
    "                    'building_type': 'str',\n",
    "                    'total_floor_area': 'float',\n",
    "                    'num_zones': 'int',\n",
    "                    'conditioned_floor_area': 'float',  # Added\n",
    "                    'total_volume': 'float'  # Added\n",
    "                }\n",
    "            },\n",
    "            # Modified results - Updated to handle variant_id better\n",
    "            'parsed_modified_results/sql_results/timeseries/aggregated/daily/zones_daily.parquet': {\n",
    "                'columns': {\n",
    "                    'building_id': 'str',\n",
    "                    'original_building_id': 'str',  # Added - extracted from composite ID\n",
    "                    'variant_id': 'str',\n",
    "                    'Zone': 'str',\n",
    "                    'Variable': 'str',\n",
    "                    'Value': 'float'\n",
    "                }\n",
    "            },\n",
    "            'parsed_modified_results/sql_results/timeseries/aggregated/daily/hvac_daily.parquet': {\n",
    "                'columns': {\n",
    "                    'building_id': 'str',\n",
    "                    'original_building_id': 'str',\n",
    "                    'variant_id': 'str',\n",
    "                    'Zone': 'str',\n",
    "                    'Variable': 'str',\n",
    "                    'Value': 'float'\n",
    "                }\n",
    "            },\n",
    "            'parsed_modified_results/sql_results/timeseries/aggregated/daily/ventilation_daily.parquet': {\n",
    "                'columns': {\n",
    "                    'building_id': 'str',\n",
    "                    'original_building_id': 'str',\n",
    "                    'variant_id': 'str',\n",
    "                    'Zone': 'str',\n",
    "                    'Variable': 'str',\n",
    "                    'Value': 'float'\n",
    "                }\n",
    "            },\n",
    "            # Modified IDF data\n",
    "            'parsed_modified_results/idf_data/by_category/lighting.parquet': {\n",
    "                'columns': {\n",
    "                    'building_id': 'str',\n",
    "                    'variant_id': 'str',\n",
    "                    'zone_name': 'str',\n",
    "                    'object_name': 'str',\n",
    "                    'watts_per_zone_floor_area': 'float',\n",
    "                    'fraction_radiant': 'float',\n",
    "                    'fraction_visible': 'float'\n",
    "                }\n",
    "            },\n",
    "            # Modifications tracking - Updated structure\n",
    "            'modified_idfs/modifications_detail_*.parquet': {\n",
    "                'columns': {\n",
    "                    'building_id': ['str', 'int'],\n",
    "                    'variant_id': 'str',\n",
    "                    'category': 'str',\n",
    "                    'object_type': 'str',\n",
    "                    'object_name': 'str',\n",
    "                    'field_name': 'str',\n",
    "                    'original_value': ['str', 'float'],\n",
    "                    'new_value': ['str', 'float'],\n",
    "                    'relative_change': 'float',\n",
    "                    'param_id': 'str'  # Added - category*object_type*object_name*field_name\n",
    "                }\n",
    "            },\n",
    "            # Sensitivity - Multiple possible files\n",
    "            'sensitivity_results/sensitivity_for_surrogate.parquet': {\n",
    "                'columns': {\n",
    "                    'parameter': 'str',\n",
    "                    'sensitivity_score': 'float',\n",
    "                    'elasticity': 'float',\n",
    "                    'p_value': 'float',\n",
    "                    'confidence_level': 'str',  # Added\n",
    "                    'rank': 'float'  # Added\n",
    "                }\n",
    "            },\n",
    "            'sensitivity_results/modification_sensitivity_results.parquet': {  # Alternative\n",
    "                'columns': {\n",
    "                    'parameter': 'str',\n",
    "                    'sensitivity_score': 'float',\n",
    "                    'elasticity': 'float',\n",
    "                    'output_variable': 'str',  # Added for multi-output\n",
    "                    'aggregation_level': 'str'  # Added - building/zone\n",
    "                }\n",
    "            },\n",
    "            # Validation results - New\n",
    "            'validation_results/validation_summary.parquet': {\n",
    "                'columns': {\n",
    "                    'building_id': 'str',\n",
    "                    'variable': 'str',\n",
    "                    'cvrmse': 'float',\n",
    "                    'nmbe': 'float',\n",
    "                    'r2': 'float',\n",
    "                    'mae': 'float'\n",
    "                }\n",
    "            },\n",
    "            # Surrogate pipeline outputs - New\n",
    "            'surrogate_pipeline_export/*/1_inputs/extracted_modifications.parquet': {\n",
    "                'columns': {\n",
    "                    'building_id': 'str',\n",
    "                    'variant_id': 'str',\n",
    "                    'param_id': 'str',\n",
    "                    'relative_change': 'float'\n",
    "                }\n",
    "            },\n",
    "            'surrogate_pipeline_export/*/3_preprocessing/preprocessed_features.parquet': {\n",
    "                'columns': {\n",
    "                    'building_id': 'str',\n",
    "                    'variant_id': 'str',\n",
    "                    # Dynamic feature columns based on modifications\n",
    "                }\n",
    "            },\n",
    "            'surrogate_pipeline_export/*/3_preprocessing/preprocessed_targets.parquet': {\n",
    "                'columns': {\n",
    "                    'building_id': 'str',\n",
    "                    'variant_id': 'str',\n",
    "                    # Target columns like:\n",
    "                    # 'Heating_EnergyTransfer_J_Hourly_total': 'float',\n",
    "                    # 'Cooling_EnergyTransfer_J_Hourly_total': 'float',\n",
    "                    # 'Electricity_Facility_J_Hourly_total': 'float'\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def convert_to_native_types(self, obj: Any) -> Any:\n",
    "        \"\"\"Convert numpy types to native Python types for JSON serialization.\"\"\"\n",
    "        if isinstance(obj, dict):\n",
    "            return {k: self.convert_to_native_types(v) for k, v in obj.items()}\n",
    "        elif isinstance(obj, list):\n",
    "            return [self.convert_to_native_types(item) for item in obj]\n",
    "        elif isinstance(obj, tuple):\n",
    "            return tuple(self.convert_to_native_types(item) for item in obj)\n",
    "        elif isinstance(obj, pd.Timestamp):\n",
    "            # Convert pandas Timestamp to ISO format string\n",
    "            return obj.isoformat()\n",
    "        elif isinstance(obj, datetime):\n",
    "            # Convert datetime to ISO format string\n",
    "            return obj.isoformat()\n",
    "        elif isinstance(obj, np.integer):\n",
    "            return int(obj)\n",
    "        elif isinstance(obj, np.floating):\n",
    "            if np.isnan(obj):\n",
    "                return None\n",
    "            return float(obj)\n",
    "        elif isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        elif pd.isna(obj):\n",
    "            return None\n",
    "        else:\n",
    "            return obj\n",
    "    \n",
    "    def check_file_exists(self, file_path: str) -> Tuple[bool, Optional[str]]:\n",
    "        \"\"\"Check if file exists and return its absolute path.\"\"\"\n",
    "        full_path = self.job_output_dir / file_path\n",
    "        \n",
    "        # Also check with wildcards for files like modifications_detail_*.parquet\n",
    "        if '*' in file_path:\n",
    "            parent = full_path.parent\n",
    "            pattern = full_path.name\n",
    "            if parent.exists():\n",
    "                matching_files = list(parent.glob(pattern))\n",
    "                if matching_files:\n",
    "                    return True, str(matching_files[0])\n",
    "            return False, None\n",
    "        \n",
    "        return full_path.exists(), str(full_path) if full_path.exists() else None\n",
    "    \n",
    "    def get_dtype_string(self, dtype) -> str:\n",
    "        \"\"\"Convert numpy/pandas dtype to simple string.\"\"\"\n",
    "        dtype_str = str(dtype)\n",
    "        \n",
    "        if 'int' in dtype_str:\n",
    "            return 'int'\n",
    "        elif 'float' in dtype_str:\n",
    "            return 'float'\n",
    "        elif 'object' in dtype_str or 'string' in dtype_str:\n",
    "            return 'str'\n",
    "        elif 'datetime' in dtype_str:\n",
    "            return 'datetime'\n",
    "        elif 'bool' in dtype_str:\n",
    "            return 'bool'\n",
    "        else:\n",
    "            return dtype_str\n",
    "    \n",
    "    def get_sample_values(self, series: pd.Series, n_samples: int = 5) -> List:\n",
    "        \"\"\"Get sample values from a series.\"\"\"\n",
    "        unique_vals = series.dropna().unique()\n",
    "        if len(unique_vals) <= n_samples:\n",
    "            return [self.convert_to_native_types(val) for val in unique_vals]\n",
    "        else:\n",
    "            # Get a mix of values\n",
    "            samples = []\n",
    "            if len(unique_vals) > 0:\n",
    "                # Add min/max for numeric\n",
    "                if pd.api.types.is_numeric_dtype(series):\n",
    "                    samples.append(self.convert_to_native_types(series.min()))\n",
    "                    samples.append(self.convert_to_native_types(series.max()))\n",
    "                    # Add some random samples\n",
    "                    remaining = n_samples - 2\n",
    "                    if remaining > 0:\n",
    "                        random_samples = series.dropna().sample(n=min(remaining, len(series))).tolist()\n",
    "                        samples.extend([self.convert_to_native_types(val) for val in random_samples[:remaining]])\n",
    "                else:\n",
    "                    # For non-numeric, just take first n_samples\n",
    "                    samples = [self.convert_to_native_types(val) for val in unique_vals[:n_samples]]\n",
    "            return samples\n",
    "    \n",
    "    def check_parquet_file(self, file_path: str, expected_columns: Dict[str, any]) -> Dict:\n",
    "        \"\"\"Check a parquet file's structure.\"\"\"\n",
    "        exists, abs_path = self.check_file_exists(file_path)\n",
    "        \n",
    "        result = {\n",
    "            'file': file_path,\n",
    "            'exists': exists,\n",
    "            'path': abs_path,\n",
    "            'row_count': 0,\n",
    "            'columns': {},\n",
    "            'extra_columns': {}  # Store discovered columns\n",
    "        }\n",
    "        \n",
    "        if exists and abs_path:\n",
    "            try:\n",
    "                # Read parquet file\n",
    "                df = pd.read_parquet(abs_path)\n",
    "                result['row_count'] = len(df)\n",
    "                \n",
    "                # Get actual columns and types\n",
    "                actual_columns = {}\n",
    "                for col in df.columns:\n",
    "                    actual_columns[col] = self.get_dtype_string(df[col].dtype)\n",
    "                \n",
    "                # Store discovered columns info for this file\n",
    "                if file_path not in self.discovered_columns:\n",
    "                    self.discovered_columns[file_path] = {}\n",
    "                \n",
    "                # Compare with expected\n",
    "                for exp_col, exp_type in expected_columns.items():\n",
    "                    if exp_col in actual_columns:\n",
    "                        actual_type = actual_columns[exp_col]\n",
    "                        \n",
    "                        # Handle multiple expected types\n",
    "                        if isinstance(exp_type, list):\n",
    "                            type_match = actual_type in exp_type\n",
    "                        else:\n",
    "                            type_match = (actual_type == exp_type) or \\\n",
    "                                       (exp_type == 'str' and actual_type == 'object')\n",
    "                        \n",
    "                        result['columns'][exp_col] = {\n",
    "                            'expected_type': exp_type,\n",
    "                            'actual_type': actual_type,\n",
    "                            'exists': True,\n",
    "                            'type_match': type_match,\n",
    "                            'null_count': int(df[exp_col].isnull().sum()),\n",
    "                            'unique_count': int(df[exp_col].nunique()),\n",
    "                            'sample_values': self.get_sample_values(df[exp_col])\n",
    "                        }\n",
    "                    else:\n",
    "                        result['columns'][exp_col] = {\n",
    "                            'expected_type': exp_type,\n",
    "                            'actual_type': None,\n",
    "                            'exists': False,\n",
    "                            'type_match': False,\n",
    "                            'null_count': None,\n",
    "                            'unique_count': None,\n",
    "                            'sample_values': []\n",
    "                        }\n",
    "                \n",
    "                # Add unexpected columns (discovered extras)\n",
    "                for col in actual_columns:\n",
    "                    if col not in expected_columns:\n",
    "                        result['extra_columns'][col] = {\n",
    "                            'actual_type': actual_columns[col],\n",
    "                            'null_count': int(df[col].isnull().sum()),\n",
    "                            'unique_count': int(df[col].nunique()),\n",
    "                            'sample_values': self.get_sample_values(df[col]),\n",
    "                            'stats': self.get_column_stats(df[col])\n",
    "                        }\n",
    "                        # Store in discovered columns\n",
    "                        self.discovered_columns[file_path][col] = result['extra_columns'][col]\n",
    "                \n",
    "            except Exception as e:\n",
    "                result['error'] = str(e)\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def get_column_stats(self, series: pd.Series) -> Dict:\n",
    "        \"\"\"Get statistics for a column.\"\"\"\n",
    "        stats = {\n",
    "            'total_count': int(len(series)),\n",
    "            'non_null_count': int(series.count()),\n",
    "            'null_percentage': float((series.isnull().sum() / len(series) * 100) if len(series) > 0 else 0)\n",
    "        }\n",
    "        \n",
    "        if pd.api.types.is_numeric_dtype(series):\n",
    "            # Handle potential NaN values\n",
    "            if series.count() > 0:  # If there are non-null values\n",
    "                stats.update({\n",
    "                    'mean': self.convert_to_native_types(series.mean()),\n",
    "                    'std': self.convert_to_native_types(series.std()),\n",
    "                    'min': self.convert_to_native_types(series.min()),\n",
    "                    'max': self.convert_to_native_types(series.max()),\n",
    "                    'q25': self.convert_to_native_types(series.quantile(0.25)),\n",
    "                    'q50': self.convert_to_native_types(series.quantile(0.50)),\n",
    "                    'q75': self.convert_to_native_types(series.quantile(0.75))\n",
    "                })\n",
    "            else:\n",
    "                # All values are null\n",
    "                stats.update({\n",
    "                    'mean': None,\n",
    "                    'std': None,\n",
    "                    'min': None,\n",
    "                    'max': None,\n",
    "                    'q25': None,\n",
    "                    'q50': None,\n",
    "                    'q75': None\n",
    "                })\n",
    "        elif pd.api.types.is_string_dtype(series) or series.dtype == 'object':\n",
    "            value_counts = series.value_counts()\n",
    "            stats.update({\n",
    "                'unique_values': int(len(value_counts)),\n",
    "                'most_common': {str(k): int(v) for k, v in value_counts.head(5).items()} if len(value_counts) > 0 else {}\n",
    "            })\n",
    "        \n",
    "        return stats\n",
    "    \n",
    "    def check_all_files(self) -> pd.DataFrame:\n",
    "        \"\"\"Check all expected files and create comparison table.\"\"\"\n",
    "        print(f\"Checking surrogate data structure in: {self.job_output_dir}\\n\")\n",
    "        \n",
    "        all_results = []\n",
    "        \n",
    "        for file_path, expected_info in self.expected_structure.items():\n",
    "            print(f\"Checking: {file_path}\")\n",
    "            result = self.check_parquet_file(file_path, expected_info['columns'])\n",
    "            \n",
    "            # Create rows for expected columns\n",
    "            if result['exists']:\n",
    "                for col_name, col_info in result['columns'].items():\n",
    "                    all_results.append({\n",
    "                        'File': file_path.split('/')[-1],\n",
    "                        'Path': file_path,\n",
    "                        'File_Exists': '✓',\n",
    "                        'Rows': result['row_count'],\n",
    "                        'Column': col_name,\n",
    "                        'Column_Type': 'Expected',\n",
    "                        'Expected_Type': col_info['expected_type'],\n",
    "                        'Actual_Type': col_info['actual_type'],\n",
    "                        'Column_Exists': '✓' if col_info['exists'] else '✗',\n",
    "                        'Type_Match': '✓' if col_info['type_match'] else '✗',\n",
    "                        'Null_Count': col_info['null_count'],\n",
    "                        'Unique_Values': col_info['unique_count'],\n",
    "                        'Sample_Values': str(col_info['sample_values'][:3]) if col_info['sample_values'] else 'N/A',\n",
    "                        'Status': 'OK' if col_info['exists'] and col_info['type_match'] else 'ISSUE'\n",
    "                    })\n",
    "                \n",
    "                # Add rows for extra discovered columns\n",
    "                for col_name, col_info in result['extra_columns'].items():\n",
    "                    all_results.append({\n",
    "                        'File': file_path.split('/')[-1],\n",
    "                        'Path': file_path,\n",
    "                        'File_Exists': '✓',\n",
    "                        'Rows': result['row_count'],\n",
    "                        'Column': col_name,\n",
    "                        'Column_Type': 'DISCOVERED',\n",
    "                        'Expected_Type': 'N/A',\n",
    "                        'Actual_Type': col_info['actual_type'],\n",
    "                        'Column_Exists': '✓',\n",
    "                        'Type_Match': 'N/A',\n",
    "                        'Null_Count': col_info['null_count'],\n",
    "                        'Unique_Values': col_info['unique_count'],\n",
    "                        'Sample_Values': str(col_info['sample_values'][:3]) if col_info['sample_values'] else 'N/A',\n",
    "                        'Status': 'EXTRA'\n",
    "                    })\n",
    "            else:\n",
    "                all_results.append({\n",
    "                    'File': file_path.split('/')[-1],\n",
    "                    'Path': file_path,\n",
    "                    'File_Exists': '✗',\n",
    "                    'Rows': 0,\n",
    "                    'Column': 'N/A',\n",
    "                    'Column_Type': 'N/A',\n",
    "                    'Expected_Type': 'N/A',\n",
    "                    'Actual_Type': 'N/A',\n",
    "                    'Column_Exists': 'N/A',\n",
    "                    'Type_Match': 'N/A',\n",
    "                    'Null_Count': None,\n",
    "                    'Unique_Values': None,\n",
    "                    'Sample_Values': 'N/A',\n",
    "                    'Status': 'MISSING FILE'\n",
    "                })\n",
    "        \n",
    "        return pd.DataFrame(all_results)\n",
    "    \n",
    "    def create_discovered_columns_report(self) -> pd.DataFrame:\n",
    "        \"\"\"Create a detailed report of all discovered extra columns.\"\"\"\n",
    "        discovered_data = []\n",
    "        \n",
    "        for file_path, columns in self.discovered_columns.items():\n",
    "            for col_name, col_info in columns.items():\n",
    "                row = {\n",
    "                    'File': file_path.split('/')[-1],\n",
    "                    'Path': file_path,\n",
    "                    'Column_Name': col_name,\n",
    "                    'Data_Type': col_info['actual_type'],\n",
    "                    'Non_Null_Count': col_info['null_count'],\n",
    "                    'Unique_Count': col_info['unique_count'],\n",
    "                    'Null_Percentage': col_info['stats']['null_percentage'],\n",
    "                    'Sample_Values': ', '.join(map(str, col_info['sample_values'][:5]))\n",
    "                }\n",
    "                \n",
    "                # Add numeric stats if available\n",
    "                if 'mean' in col_info['stats']:\n",
    "                    row.update({\n",
    "                        'Mean': col_info['stats']['mean'],\n",
    "                        'Std': col_info['stats']['std'],\n",
    "                        'Min': col_info['stats']['min'],\n",
    "                        'Max': col_info['stats']['max']\n",
    "                    })\n",
    "                \n",
    "                discovered_data.append(row)\n",
    "        \n",
    "        return pd.DataFrame(discovered_data)\n",
    "    \n",
    "    def create_summary_report(self, df_results: pd.DataFrame) -> Dict:\n",
    "        \"\"\"Create a summary report of the check results.\"\"\"\n",
    "        summary = {\n",
    "            'check_timestamp': datetime.now().isoformat(),\n",
    "            'job_output_dir': str(self.job_output_dir),\n",
    "            'total_files_expected': len(self.expected_structure),\n",
    "            'files_found': len(df_results[df_results['File_Exists'] == '✓']['Path'].unique()),\n",
    "            'files_missing': len(df_results[df_results['File_Exists'] == '✗']['Path'].unique()),\n",
    "            'total_expected_columns': len(df_results[df_results['Column_Type'] == 'Expected']),\n",
    "            'expected_columns_found': len(df_results[(df_results['Column_Type'] == 'Expected') & \n",
    "                                                    (df_results['Column_Exists'] == '✓')]),\n",
    "            'total_discovered_columns': len(df_results[df_results['Column_Type'] == 'DISCOVERED']),\n",
    "            'total_issues': len(df_results[df_results['Status'].isin(['ISSUE', 'MISSING FILE'])]),\n",
    "            'missing_files': [],\n",
    "            'column_issues': [],\n",
    "            'type_mismatches': [],\n",
    "            'discovered_extras_summary': {},\n",
    "            'data_pipeline_readiness': {  # New section\n",
    "                'has_base_outputs': False,\n",
    "                'has_modified_outputs': False,\n",
    "                'has_modifications': False,\n",
    "                'has_sensitivity': False,\n",
    "                'has_zone_mappings': False,\n",
    "                'ready_for_surrogate': False\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Get missing files\n",
    "        missing_files = df_results[df_results['File_Exists'] == '✗']['Path'].unique()\n",
    "        summary['missing_files'] = list(missing_files)\n",
    "        \n",
    "        # Get column issues\n",
    "        column_issues = df_results[(df_results['Column_Exists'] == '✗') & \n",
    "                                 (df_results['File_Exists'] == '✓') & \n",
    "                                 (df_results['Column_Type'] == 'Expected')]\n",
    "        for _, row in column_issues.iterrows():\n",
    "            summary['column_issues'].append(f\"{row['File']}: {row['Column']}\")\n",
    "        \n",
    "        # Get type mismatches\n",
    "        type_issues = df_results[(df_results['Type_Match'] == '✗') & \n",
    "                               (df_results['Column_Exists'] == '✓') & \n",
    "                               (df_results['Column_Type'] == 'Expected')]\n",
    "        for _, row in type_issues.iterrows():\n",
    "            summary['type_mismatches'].append(\n",
    "                f\"{row['File']}: {row['Column']} (expected {row['Expected_Type']}, got {row['Actual_Type']})\"\n",
    "            )\n",
    "        \n",
    "        # Summary of discovered columns by file\n",
    "        discovered_by_file = df_results[df_results['Column_Type'] == 'DISCOVERED'].groupby('File')['Column'].count()\n",
    "        summary['discovered_extras_summary'] = discovered_by_file.to_dict()\n",
    "        \n",
    "        # Check data pipeline readiness\n",
    "        files_exist = df_results[df_results['File_Exists'] == '✓']['Path'].unique()\n",
    "        summary['data_pipeline_readiness']['has_base_outputs'] = any('parsed_data/sql_results' in f for f in files_exist)\n",
    "        summary['data_pipeline_readiness']['has_modified_outputs'] = any('parsed_modified_results/sql_results' in f for f in files_exist)\n",
    "        summary['data_pipeline_readiness']['has_modifications'] = any('modifications_detail' in f for f in files_exist)\n",
    "        summary['data_pipeline_readiness']['has_sensitivity'] = any('sensitivity_results' in f for f in files_exist)\n",
    "        summary['data_pipeline_readiness']['has_zone_mappings'] = any('zone_mappings' in f for f in files_exist)\n",
    "        \n",
    "        # Check if ready for surrogate modeling\n",
    "        summary['data_pipeline_readiness']['ready_for_surrogate'] = (\n",
    "            summary['data_pipeline_readiness']['has_base_outputs'] and\n",
    "            summary['data_pipeline_readiness']['has_modified_outputs'] and\n",
    "            summary['data_pipeline_readiness']['has_modifications']\n",
    "        )\n",
    "        \n",
    "        return summary\n",
    "    \n",
    "    def print_colored_results(self, df_results: pd.DataFrame, summary: Dict):\n",
    "        \"\"\"Print results with color coding (if in Jupyter/IPython).\"\"\"\n",
    "        try:\n",
    "            from IPython.display import display, HTML\n",
    "            \n",
    "            # Style the dataframe\n",
    "            def style_status(val):\n",
    "                if val == 'OK':\n",
    "                    return 'background-color: #90EE90'\n",
    "                elif val == 'ISSUE':\n",
    "                    return 'background-color: #FFB6C1'\n",
    "                elif val == 'MISSING FILE':\n",
    "                    return 'background-color: #FF6B6B'\n",
    "                elif val == 'EXTRA':\n",
    "                    return 'background-color: #87CEEB'\n",
    "                return ''\n",
    "            \n",
    "            def style_check(val):\n",
    "                if val == '✓':\n",
    "                    return 'color: green; font-weight: bold'\n",
    "                elif val == '✗':\n",
    "                    return 'color: red; font-weight: bold'\n",
    "                return ''\n",
    "            \n",
    "            def style_column_type(val):\n",
    "                if val == 'DISCOVERED':\n",
    "                    return 'color: blue; font-weight: bold'\n",
    "                return ''\n",
    "            \n",
    "            styled_df = df_results.style.applymap(style_status, subset=['Status'])\\\n",
    "                                       .applymap(style_check, subset=['File_Exists', 'Column_Exists', 'Type_Match'])\\\n",
    "                                       .applymap(style_column_type, subset=['Column_Type'])\n",
    "            \n",
    "            display(styled_df)\n",
    "            \n",
    "        except ImportError:\n",
    "            # Fallback to regular print\n",
    "            print(df_results.to_string())\n",
    "    \n",
    "    def save_results(self, df_results: pd.DataFrame, summary: Dict):\n",
    "        \"\"\"Save all results to files.\"\"\"\n",
    "        output_dir = self.job_output_dir / 'surrogate_data_check'\n",
    "        output_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        # Create timestamp for filenames\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        \n",
    "        # 1. Save detailed results\n",
    "        df_results.to_csv(output_dir / f'structure_check_details_{timestamp}.csv', index=False)\n",
    "        df_results.to_excel(output_dir / f'structure_check_details_{timestamp}.xlsx', index=False)\n",
    "        \n",
    "        # 2. Save summary\n",
    "        with open(output_dir / f'structure_check_summary_{timestamp}.json', 'w') as f:\n",
    "            json.dump(summary, f, indent=2)\n",
    "        \n",
    "        # 3. Save discovered columns detailed report\n",
    "        df_discovered = self.create_discovered_columns_report()\n",
    "        if not df_discovered.empty:\n",
    "            df_discovered.to_csv(output_dir / f'discovered_columns_{timestamp}.csv', index=False)\n",
    "            df_discovered.to_excel(output_dir / f'discovered_columns_{timestamp}.xlsx', index=False)\n",
    "        \n",
    "        # 4. Save discovered columns with full statistics - convert numpy types first\n",
    "        discovered_columns_native = self.convert_to_native_types(self.discovered_columns)\n",
    "        with open(output_dir / f'discovered_columns_full_{timestamp}.json', 'w') as f:\n",
    "            json.dump(discovered_columns_native, f, indent=2)\n",
    "        \n",
    "        # 5. Create a markdown report\n",
    "        self.create_markdown_report(output_dir / f'structure_report_{timestamp}.md', df_results, summary)\n",
    "        \n",
    "        # 6. Save only problematic entries for quick review\n",
    "        df_issues = df_results[df_results['Status'].isin(['ISSUE', 'MISSING FILE'])]\n",
    "        if not df_issues.empty:\n",
    "            df_issues.to_csv(output_dir / f'issues_only_{timestamp}.csv', index=False)\n",
    "        \n",
    "        print(f\"\\nResults saved to: {output_dir}\")\n",
    "        print(f\"Files created:\")\n",
    "        print(f\"  - structure_check_details_{timestamp}.csv/xlsx\")\n",
    "        print(f\"  - structure_check_summary_{timestamp}.json\")\n",
    "        print(f\"  - discovered_columns_{timestamp}.csv/xlsx\")\n",
    "        print(f\"  - discovered_columns_full_{timestamp}.json\")\n",
    "        print(f\"  - structure_report_{timestamp}.md\")\n",
    "        if not df_issues.empty:\n",
    "            print(f\"  - issues_only_{timestamp}.csv\")\n",
    "    \n",
    "    def create_markdown_report(self, output_path: Path, df_results: pd.DataFrame, summary: Dict):\n",
    "        \"\"\"Create a markdown report for easy reading.\"\"\"\n",
    "        with open(output_path, 'w') as f:\n",
    "            f.write(f\"# Surrogate Data Structure Check Report\\n\\n\")\n",
    "            f.write(f\"**Generated:** {summary['check_timestamp']}\\n\\n\")\n",
    "            f.write(f\"**Directory:** `{summary['job_output_dir']}`\\n\\n\")\n",
    "            \n",
    "            f.write(\"## Summary\\n\\n\")\n",
    "            f.write(f\"- **Files Expected:** {summary['total_files_expected']}\\n\")\n",
    "            f.write(f\"- **Files Found:** {summary['files_found']}\\n\")\n",
    "            f.write(f\"- **Files Missing:** {summary['files_missing']}\\n\")\n",
    "            f.write(f\"- **Expected Columns:** {summary['total_expected_columns']}\\n\")\n",
    "            f.write(f\"- **Expected Columns Found:** {summary['expected_columns_found']}\\n\")\n",
    "            f.write(f\"- **Extra Columns Discovered:** {summary['total_discovered_columns']}\\n\")\n",
    "            f.write(f\"- **Total Issues:** {summary['total_issues']}\\n\\n\")\n",
    "            \n",
    "            # Add pipeline readiness section\n",
    "            f.write(\"## Data Pipeline Readiness\\n\\n\")\n",
    "            readiness = summary['data_pipeline_readiness']\n",
    "            f.write(f\"- **Base Outputs:** {'✓' if readiness['has_base_outputs'] else '✗'}\\n\")\n",
    "            f.write(f\"- **Modified Outputs:** {'✓' if readiness['has_modified_outputs'] else '✗'}\\n\")\n",
    "            f.write(f\"- **Modifications:** {'✓' if readiness['has_modifications'] else '✗'}\\n\")\n",
    "            f.write(f\"- **Sensitivity Analysis:** {'✓' if readiness['has_sensitivity'] else '✗'}\\n\")\n",
    "            f.write(f\"- **Zone Mappings:** {'✓' if readiness['has_zone_mappings'] else '✗'}\\n\")\n",
    "            f.write(f\"- **Ready for Surrogate Modeling:** {'✓' if readiness['ready_for_surrogate'] else '✗'}\\n\\n\")\n",
    "            \n",
    "            if summary['missing_files']:\n",
    "                f.write(\"## Missing Files\\n\\n\")\n",
    "                for file in summary['missing_files']:\n",
    "                    f.write(f\"- `{file}`\\n\")\n",
    "                f.write(\"\\n\")\n",
    "            \n",
    "            if summary['discovered_extras_summary']:\n",
    "                f.write(\"## Discovered Extra Columns Summary\\n\\n\")\n",
    "                f.write(\"| File | Extra Columns Count |\\n\")\n",
    "                f.write(\"|------|--------------------|\\n\")\n",
    "                for file, count in summary['discovered_extras_summary'].items():\n",
    "                    f.write(f\"| {file} | {count} |\\n\")\n",
    "                f.write(\"\\n\")\n",
    "            \n",
    "            # Add detailed discovered columns\n",
    "            df_discovered = self.create_discovered_columns_report()\n",
    "            if not df_discovered.empty:\n",
    "                f.write(\"## Discovered Columns Details\\n\\n\")\n",
    "                f.write(df_discovered.to_markdown(index=False))\n",
    "                f.write(\"\\n\")\n",
    "\n",
    "# Usage\n",
    "def check_surrogate_data_structure(job_output_dir: str):\n",
    "    \"\"\"Main function to check surrogate data structure.\"\"\"\n",
    "    checker = SurrogateDataChecker(job_output_dir)\n",
    "    \n",
    "    # Run checks\n",
    "    df_results = checker.check_all_files()\n",
    "    summary = checker.create_summary_report(df_results)\n",
    "    \n",
    "    # Print summary\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"SUMMARY\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Check performed at: {summary['check_timestamp']}\")\n",
    "    print(f\"Total files expected: {summary['total_files_expected']}\")\n",
    "    print(f\"Files found: {summary['files_found']}\")\n",
    "    print(f\"Files missing: {summary['files_missing']}\")\n",
    "    print(f\"Expected columns total: {summary['total_expected_columns']}\")\n",
    "    print(f\"Expected columns found: {summary['expected_columns_found']}\")\n",
    "    print(f\"Extra columns discovered: {summary['total_discovered_columns']}\")\n",
    "    print(f\"Total issues: {summary['total_issues']}\")\n",
    "    \n",
    "    # Print pipeline readiness\n",
    "    print(\"\\nDATA PIPELINE READINESS:\")\n",
    "    readiness = summary['data_pipeline_readiness']\n",
    "    print(f\"  - Base Outputs: {'✓' if readiness['has_base_outputs'] else '✗'}\")\n",
    "    print(f\"  - Modified Outputs: {'✓' if readiness['has_modified_outputs'] else '✗'}\")\n",
    "    print(f\"  - Modifications: {'✓' if readiness['has_modifications'] else '✗'}\")\n",
    "    print(f\"  - Sensitivity Analysis: {'✓' if readiness['has_sensitivity'] else '✗'}\")\n",
    "    print(f\"  - Zone Mappings: {'✓' if readiness['has_zone_mappings'] else '✗'}\")\n",
    "    print(f\"  - Ready for Surrogate: {'✓ YES' if readiness['ready_for_surrogate'] else '✗ NO'}\")\n",
    "    \n",
    "    if summary['missing_files']:\n",
    "        print(f\"\\nMISSING FILES: {len(summary['missing_files'])}\")\n",
    "        for f in summary['missing_files'][:5]:  # Show first 5\n",
    "            print(f\"  - {f}\")\n",
    "        if len(summary['missing_files']) > 5:\n",
    "            print(f\"  ... and {len(summary['missing_files']) - 5} more\")\n",
    "    \n",
    "    if summary['discovered_extras_summary']:\n",
    "        print(f\"\\nDISCOVERED EXTRA COLUMNS BY FILE:\")\n",
    "        for file, count in list(summary['discovered_extras_summary'].items())[:10]:\n",
    "            print(f\"  - {file}: {count} extra columns\")\n",
    "    \n",
    "    # Display results\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"DETAILED RESULTS (First 50 rows)\")\n",
    "    print(\"=\"*80)\n",
    "    checker.print_colored_results(df_results.head(50), summary)\n",
    "    \n",
    "    # Show discovered columns\n",
    "    df_discovered = checker.create_discovered_columns_report()\n",
    "    if not df_discovered.empty:\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"DISCOVERED COLUMNS DETAILS (First 20)\")\n",
    "        print(\"=\"*80)\n",
    "        print(df_discovered.head(20).to_string())\n",
    "    \n",
    "    # Save results\n",
    "    checker.save_results(df_results, summary)\n",
    "    \n",
    "    return df_results, summary, df_discovered\n",
    "\n",
    "# Run the check\n",
    "if __name__ == \"__main__\":\n",
    "    job_dir = r\"D:\\Documents\\daily\\E_Plus_2040_py\\output\\82e2b83f-5013-4270-8266-a37a67dbd4ff\"\n",
    "    df_results, summary, df_discovered = check_surrogate_data_structure(job_dir)\n",
    "    \n",
    "    # Additional analysis - show files by status\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"FILES BY STATUS\")\n",
    "    print(\"=\"*80)\n",
    "    status_summary = df_results.groupby('Status')['Path'].nunique()\n",
    "    print(status_summary)\n",
    "    \n",
    "    # Show column type distribution\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"COLUMN TYPE DISTRIBUTION\")\n",
    "    print(\"=\"*80)\n",
    "    column_dist = df_results['Column_Type'].value_counts()\n",
    "    print(column_dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5ee7ea",
   "metadata": {},
   "source": [
    "# Sensitivity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d6cc4b",
   "metadata": {},
   "source": [
    "## check v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "270b135f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking sensitivity data structure in: D:\\Documents\\daily\\E_Plus_2040_py\\output\\82e2b83f-5013-4270-8266-a37a67dbd4ff\n",
      "\n",
      "Checking: modified_idfs/modifications_detail_*.parquet\n",
      "Checking: parsed_data/sql_results/timeseries/aggregated/daily/hvac_daily.parquet\n",
      "Checking: parsed_data/sql_results/timeseries/aggregated/daily/energy_daily.parquet\n",
      "Checking: parsed_data/sql_results/timeseries/aggregated/daily/electricity_daily.parquet\n",
      "Checking: parsed_data/sql_results/timeseries/aggregated/daily/zones_daily.parquet\n",
      "Checking: parsed_data/sql_results/timeseries/aggregated/daily/temperature_daily.parquet\n",
      "Checking: parsed_data/sql_results/timeseries/aggregated/daily/ventilation_daily.parquet\n",
      "Checking: parsed_data/sql_results/timeseries/hourly/hvac_2013.parquet\n",
      "Checking: parsed_modified_results/sql_results/timeseries/aggregated/daily/hvac_daily.parquet\n",
      "Checking: parsed_modified_results/sql_results/timeseries/aggregated/daily/energy_daily.parquet\n",
      "Checking: parsed_modified_results/sql_results/timeseries/aggregated/daily/electricity_daily.parquet\n",
      "Checking: parsed_modified_results/sql_results/timeseries/aggregated/daily/zones_daily.parquet\n",
      "Checking: parsed_data/relationships/zone_mappings.parquet\n",
      "Checking: parsed_data/relationships/equipment_assignments.parquet\n",
      "Checking: parsed_data/idf_data/by_category/*.parquet\n",
      "Checking: parsed_data/analysis_ready/parameter_matrix.parquet\n",
      "Checking: parsed_data/metadata/building_registry.parquet\n",
      "Checking: sensitivity_results/modification_sensitivity_results.parquet\n",
      "Checking: sensitivity_results/traditional_sensitivity_results.parquet\n",
      "Checking: sensitivity_results/hybrid_sensitivity_results.parquet\n",
      "Checking: sensitivity_results/parameter_stability_across_time_slices.parquet\n",
      "Checking: sensitivity_results/*_sensitivity_results_*.parquet\n",
      "Checking: sensitivity_results/uncertainty_analysis_results.parquet\n",
      "Checking: sensitivity_results/threshold_analysis_results.parquet\n",
      "Checking: sensitivity_results/regional_sensitivity_results.parquet\n",
      "Checking: sensitivity_results/sobol_analysis_results.parquet\n",
      "Checking: sensitivity_results/temporal_pattern_results.parquet\n",
      "Checking: sensitivity_results/top_sensitive_parameters.csv\n",
      "Checking: sensitivity_results/calibration_parameters.json\n",
      "Checking: sensitivity_results/modification_sensitivity_report.json\n",
      "Checking: sensitivity_results/sensitivity_summary.json\n",
      "Checking: sensitivity_results/time_slice_sensitivity_summary.json\n",
      "Checking: sensitivity_results/advanced_sensitivity_report.json\n",
      "\n",
      "================================================================================\n",
      "CONFIGURATION DETECTED\n",
      "================================================================================\n",
      "Analysis Types: modification_based, traditional\n",
      "Multi-level: ✓\n",
      "Time Slicing: ✓\n",
      "Advanced Analyses: 2 enabled\n",
      "\n",
      "================================================================================\n",
      "SUMMARY\n",
      "================================================================================\n",
      "Total files expected: 33\n",
      "Files found: 15\n",
      "Files missing: 18\n",
      "Total issues: 29\n",
      "\n",
      "PIPELINE READINESS:\n",
      "  - modification_based: ✓ READY\n",
      "  - traditional: ✓ READY\n",
      "  - multi_level: ✓ READY\n",
      "  - time_slicing: ✓ READY\n",
      "\n",
      "Results saved to: D:\\Documents\\daily\\E_Plus_2040_py\\output\\82e2b83f-5013-4270-8266-a37a67dbd4ff\\sensitivity_data_check\n",
      "Files created:\n",
      "  - structure_check_details_20250627_045018.csv/xlsx\n",
      "  - structure_check_summary_20250627_045018.json\n",
      "  - structure_report_20250627_045018.md\n",
      "  - issues_only_20250627_045018.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Optional, Any\n",
    "import json\n",
    "from datetime import datetime\n",
    "import glob\n",
    "\n",
    "class SensitivityDataChecker:\n",
    "    def __init__(self, job_output_dir: str):\n",
    "        self.job_output_dir = Path(job_output_dir)\n",
    "        self.results = []\n",
    "        self.discovered_columns = {}\n",
    "        \n",
    "        # Expected structure for sensitivity analysis pipeline\n",
    "        self.expected_structure = {\n",
    "            # === MODIFICATION TRACKING ===\n",
    "            'modified_idfs/modifications_detail_*.parquet': {\n",
    "                'columns': {\n",
    "                    'building_id': ['str', 'int'],\n",
    "                    'category': 'str',\n",
    "                    'object_type': 'str',\n",
    "                    'object_name': 'str',\n",
    "                    'field_name': 'str',\n",
    "                    'original_value': ['str', 'float'],\n",
    "                    'new_value': ['str', 'float'],\n",
    "                    'original_value_numeric': 'float',\n",
    "                    'new_value_numeric': 'float',\n",
    "                    'param_delta': 'float',\n",
    "                    'param_pct_change': 'float',\n",
    "                    'param_key': 'str'  # category*object_type*object_name*field_name\n",
    "                },\n",
    "                'required_for': ['modification_based']\n",
    "            },\n",
    "            \n",
    "            # === BASE SIMULATION RESULTS ===\n",
    "            'parsed_data/sql_results/timeseries/aggregated/daily/hvac_daily.parquet': {\n",
    "                'columns': {\n",
    "                    'building_id': ['str', 'int'],\n",
    "                    'Variable': 'str',\n",
    "                    'Value': 'float',\n",
    "                    'DateTime': 'datetime',\n",
    "                    'Zone': 'str'  # Optional\n",
    "                },\n",
    "                'required_for': ['all']\n",
    "            },\n",
    "            'parsed_data/sql_results/timeseries/aggregated/daily/energy_daily.parquet': {\n",
    "                'columns': {\n",
    "                    'building_id': ['str', 'int'],\n",
    "                    'Variable': 'str',\n",
    "                    'Value': 'float',\n",
    "                    'DateTime': 'datetime'\n",
    "                },\n",
    "                'required_for': ['all']\n",
    "            },\n",
    "            'parsed_data/sql_results/timeseries/aggregated/daily/electricity_daily.parquet': {\n",
    "                'columns': {\n",
    "                    'building_id': ['str', 'int'],\n",
    "                    'Variable': 'str',\n",
    "                    'Value': 'float',\n",
    "                    'DateTime': 'datetime'\n",
    "                },\n",
    "                'required_for': ['all']\n",
    "            },\n",
    "            'parsed_data/sql_results/timeseries/aggregated/daily/zones_daily.parquet': {\n",
    "                'columns': {\n",
    "                    'building_id': ['str', 'int'],\n",
    "                    'Zone': 'str',\n",
    "                    'Variable': 'str',\n",
    "                    'Value': 'float',\n",
    "                    'DateTime': 'datetime'\n",
    "                },\n",
    "                'required_for': ['zone_level', 'multi_level']\n",
    "            },\n",
    "            'parsed_data/sql_results/timeseries/aggregated/daily/temperature_daily.parquet': {\n",
    "                'columns': {\n",
    "                    'building_id': ['str', 'int'],\n",
    "                    'Zone': 'str',\n",
    "                    'Variable': 'str',\n",
    "                    'Value': 'float',\n",
    "                    'DateTime': 'datetime'\n",
    "                },\n",
    "                'required_for': ['zone_level']\n",
    "            },\n",
    "            'parsed_data/sql_results/timeseries/aggregated/daily/ventilation_daily.parquet': {\n",
    "                'columns': {\n",
    "                    'building_id': ['str', 'int'],\n",
    "                    'Zone': 'str',\n",
    "                    'Variable': 'str',\n",
    "                    'Value': 'float',\n",
    "                    'DateTime': 'datetime'\n",
    "                },\n",
    "                'required_for': ['zone_level']\n",
    "            },\n",
    "            \n",
    "            # === HOURLY DATA (for time-of-day analysis) ===\n",
    "            'parsed_data/sql_results/timeseries/hourly/hvac_2013.parquet': {\n",
    "                'columns': {\n",
    "                    'building_id': ['str', 'int'],\n",
    "                    'Variable': 'str',\n",
    "                    'Value': 'float',\n",
    "                    'DateTime': 'datetime',\n",
    "                    'Zone': 'str'\n",
    "                },\n",
    "                'required_for': ['time_slicing_hourly']\n",
    "            },\n",
    "            \n",
    "            # === MODIFIED SIMULATION RESULTS ===\n",
    "            'parsed_modified_results/sql_results/timeseries/aggregated/daily/hvac_daily.parquet': {\n",
    "                'columns': {\n",
    "                    'building_id': ['str', 'int'],\n",
    "                    'Variable': 'str',\n",
    "                    'Value': 'float',\n",
    "                    'DateTime': 'datetime',\n",
    "                    'Zone': 'str'\n",
    "                },\n",
    "                'required_for': ['modification_based']\n",
    "            },\n",
    "            'parsed_modified_results/sql_results/timeseries/aggregated/daily/energy_daily.parquet': {\n",
    "                'columns': {\n",
    "                    'building_id': ['str', 'int'],\n",
    "                    'Variable': 'str',\n",
    "                    'Value': 'float',\n",
    "                    'DateTime': 'datetime'\n",
    "                },\n",
    "                'required_for': ['modification_based']\n",
    "            },\n",
    "            'parsed_modified_results/sql_results/timeseries/aggregated/daily/electricity_daily.parquet': {\n",
    "                'columns': {\n",
    "                    'building_id': ['str', 'int'],\n",
    "                    'Variable': 'str',\n",
    "                    'Value': 'float',\n",
    "                    'DateTime': 'datetime'\n",
    "                },\n",
    "                'required_for': ['modification_based']\n",
    "            },\n",
    "            'parsed_modified_results/sql_results/timeseries/aggregated/daily/zones_daily.parquet': {\n",
    "                'columns': {\n",
    "                    'building_id': ['str', 'int'],\n",
    "                    'Zone': 'str',\n",
    "                    'Variable': 'str',\n",
    "                    'Value': 'float',\n",
    "                    'DateTime': 'datetime'\n",
    "                },\n",
    "                'required_for': ['modification_based_zone_level']\n",
    "            },\n",
    "            \n",
    "            # === RELATIONSHIPS FOR MULTI-LEVEL ===\n",
    "            'parsed_data/relationships/zone_mappings.parquet': {\n",
    "                'columns': {\n",
    "                    'building_id': ['str', 'int'],\n",
    "                    'idf_zone_name': 'str',\n",
    "                    'sql_zone_name': 'str',\n",
    "                    'zone_type': 'str',\n",
    "                    'multiplier': 'float'\n",
    "                },\n",
    "                'required_for': ['zone_level', 'multi_level']\n",
    "            },\n",
    "            'parsed_data/relationships/equipment_assignments.parquet': {\n",
    "                'columns': {\n",
    "                    'building_id': ['str', 'int'],\n",
    "                    'equipment_name': 'str',\n",
    "                    'equipment_type': 'str',\n",
    "                    'assigned_zone': 'str',\n",
    "                    'schedule': 'str'\n",
    "                },\n",
    "                'required_for': ['equipment_level', 'multi_level']\n",
    "            },\n",
    "            \n",
    "            # === TRADITIONAL ANALYSIS INPUTS ===\n",
    "            'parsed_data/idf_data/by_category/*.parquet': {\n",
    "                'columns': {\n",
    "                    'building_id': ['str', 'int'],\n",
    "                    'category': 'str',\n",
    "                    'field': 'str',\n",
    "                    'value': ['str', 'float'],\n",
    "                    'value_numeric': 'float'\n",
    "                },\n",
    "                'required_for': ['traditional']\n",
    "            },\n",
    "            'parsed_data/analysis_ready/parameter_matrix.parquet': {\n",
    "                'columns': {\n",
    "                    'building_id': ['str', 'int']\n",
    "                    # Dynamic columns based on parameters\n",
    "                },\n",
    "                'required_for': ['traditional_optimized']\n",
    "            },\n",
    "            \n",
    "            # === BUILDING METADATA ===\n",
    "            'parsed_data/metadata/building_registry.parquet': {\n",
    "                'columns': {\n",
    "                    'building_id': ['str', 'int'],\n",
    "                    'building_type': 'str',\n",
    "                    'total_floor_area': 'float',\n",
    "                    'num_zones': 'int'\n",
    "                },\n",
    "                'required_for': ['all']\n",
    "            },\n",
    "            \n",
    "            # === SENSITIVITY OUTPUTS ===\n",
    "            'sensitivity_results/modification_sensitivity_results.parquet': {\n",
    "                'columns': {\n",
    "                    'parameter': 'str',\n",
    "                    'output_variable': 'str',\n",
    "                    'sensitivity_score': 'float',\n",
    "                    'method': 'str',\n",
    "                    'level': 'str',  # building/zone/equipment\n",
    "                    'p_value': 'float',\n",
    "                    'confidence_lower': 'float',\n",
    "                    'confidence_upper': 'float',\n",
    "                    'category': 'str',\n",
    "                    'elasticity': 'float',\n",
    "                    'n_samples': 'int'\n",
    "                },\n",
    "                'output_from': ['modification_based']\n",
    "            },\n",
    "            'sensitivity_results/traditional_sensitivity_results.parquet': {\n",
    "                'columns': {\n",
    "                    'parameter': 'str',\n",
    "                    'output_variable': 'str',\n",
    "                    'sensitivity_score': 'float',\n",
    "                    'method': 'str',\n",
    "                    'correlation': 'float',\n",
    "                    'p_value': 'float',\n",
    "                    'n_samples': 'int'\n",
    "                },\n",
    "                'output_from': ['traditional']\n",
    "            },\n",
    "            'sensitivity_results/hybrid_sensitivity_results.parquet': {\n",
    "                'columns': {\n",
    "                    'parameter': 'str',\n",
    "                    'output_variable': 'str',\n",
    "                    'sensitivity_score': 'float',\n",
    "                    'analysis_source': 'str',\n",
    "                    'consensus_score': 'float',\n",
    "                    'score_std': 'float',\n",
    "                    'n_methods': 'int'\n",
    "                },\n",
    "                'output_from': ['hybrid']\n",
    "            },\n",
    "            \n",
    "            # === TIME SLICE OUTPUTS ===\n",
    "            'sensitivity_results/parameter_stability_across_time_slices.parquet': {\n",
    "                'columns': {\n",
    "                    'parameter': 'str',\n",
    "                    'mean_score': 'float',\n",
    "                    'std_score': 'float',\n",
    "                    'cv': 'float',\n",
    "                    'present_in_slices': 'str',\n",
    "                    'n_slices': 'int'\n",
    "                },\n",
    "                'output_from': ['time_slicing']\n",
    "            },\n",
    "            'sensitivity_results/*_sensitivity_results_*.parquet': {\n",
    "                'columns': {\n",
    "                    'parameter': 'str',\n",
    "                    'output_variable': 'str',\n",
    "                    'sensitivity_score': 'float',\n",
    "                    'time_slice': 'str'\n",
    "                },\n",
    "                'output_from': ['time_slicing']\n",
    "            },\n",
    "            \n",
    "            # === ADVANCED ANALYSIS OUTPUTS ===\n",
    "            'sensitivity_results/uncertainty_analysis_results.parquet': {\n",
    "                'columns': {\n",
    "                    'parameter': 'str',\n",
    "                    'output_variable': 'str',\n",
    "                    'sensitivity_score': 'float',\n",
    "                    'uncertainty_lower': 'float',\n",
    "                    'uncertainty_upper': 'float',\n",
    "                    'uncertainty_std': 'float',\n",
    "                    'confidence_level': 'float'\n",
    "                },\n",
    "                'output_from': ['uncertainty']\n",
    "            },\n",
    "            'sensitivity_results/threshold_analysis_results.parquet': {\n",
    "                'columns': {\n",
    "                    'parameter': 'str',\n",
    "                    'output_variable': 'str',\n",
    "                    'breakpoint_value': 'float',\n",
    "                    'segment_index': 'int',\n",
    "                    'segment_start': 'float',\n",
    "                    'segment_end': 'float',\n",
    "                    'segment_sensitivity': 'float',\n",
    "                    'is_critical_region': 'bool'\n",
    "                },\n",
    "                'output_from': ['threshold']\n",
    "            },\n",
    "            'sensitivity_results/regional_sensitivity_results.parquet': {\n",
    "                'columns': {\n",
    "                    'parameter': 'str',\n",
    "                    'output_variable': 'str',\n",
    "                    'sensitivity_score': 'float',\n",
    "                    'region_id': 'int',\n",
    "                    'parameter_mean': 'float',\n",
    "                    'parameter_std': 'float',\n",
    "                    'local_correlation': 'float',\n",
    "                    'local_nonlinearity': 'float'\n",
    "                },\n",
    "                'output_from': ['regional']\n",
    "            },\n",
    "            'sensitivity_results/sobol_analysis_results.parquet': {\n",
    "                'columns': {\n",
    "                    'parameter': 'str',\n",
    "                    'output_variable': 'str',\n",
    "                    'sobol_index': 'float',\n",
    "                    'sobol_type': 'str',  # first_order/second_order/total\n",
    "                    'total_effect': 'float',\n",
    "                    'confidence_lower': 'float',\n",
    "                    'confidence_upper': 'float'\n",
    "                },\n",
    "                'output_from': ['sobol']\n",
    "            },\n",
    "            'sensitivity_results/temporal_pattern_results.parquet': {\n",
    "                'columns': {\n",
    "                    'parameter': 'str',\n",
    "                    'output_variable': 'str',\n",
    "                    'method': 'str',\n",
    "                    'dominant_frequency': 'float',\n",
    "                    'dominant_period': 'float',\n",
    "                    'has_seasonality': 'bool',\n",
    "                    'seasonal_period': 'float',\n",
    "                    'trend_slope': 'float'\n",
    "                },\n",
    "                'output_from': ['temporal']\n",
    "            },\n",
    "            \n",
    "            # === EXPORT FILES ===\n",
    "            'sensitivity_results/top_sensitive_parameters.csv': {\n",
    "                'columns': {\n",
    "                    'parameter': 'str',\n",
    "                    'sensitivity_score': 'float',\n",
    "                    'analysis_method': 'str',\n",
    "                    'category': 'str',\n",
    "                    'object_name': 'str',\n",
    "                    'field_name': 'str'\n",
    "                },\n",
    "                'output_from': ['export_for_surrogate']\n",
    "            },\n",
    "            'sensitivity_results/calibration_parameters.json': {\n",
    "                'columns': {},  # JSON file\n",
    "                'output_from': ['export_for_calibration']\n",
    "            },\n",
    "            \n",
    "            # === REPORTS ===\n",
    "            'sensitivity_results/modification_sensitivity_report.json': {\n",
    "                'columns': {},  # JSON report\n",
    "                'output_from': ['modification_based']\n",
    "            },\n",
    "            'sensitivity_results/sensitivity_summary.json': {\n",
    "                'columns': {},  # JSON summary\n",
    "                'output_from': ['all']\n",
    "            },\n",
    "            'sensitivity_results/time_slice_sensitivity_summary.json': {\n",
    "                'columns': {},  # JSON summary\n",
    "                'output_from': ['time_slicing']\n",
    "            },\n",
    "            'sensitivity_results/advanced_sensitivity_report.json': {\n",
    "                'columns': {},  # JSON report\n",
    "                'output_from': ['advanced']\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def convert_to_native_types(self, obj: Any) -> Any:\n",
    "        \"\"\"Convert numpy types to native Python types for JSON serialization.\"\"\"\n",
    "        if isinstance(obj, dict):\n",
    "            return {k: self.convert_to_native_types(v) for k, v in obj.items()}\n",
    "        elif isinstance(obj, list):\n",
    "            return [self.convert_to_native_types(item) for item in obj]\n",
    "        elif isinstance(obj, tuple):\n",
    "            return tuple(self.convert_to_native_types(item) for item in obj)\n",
    "        elif isinstance(obj, pd.Timestamp):\n",
    "            return obj.isoformat()\n",
    "        elif isinstance(obj, datetime):\n",
    "            return obj.isoformat()\n",
    "        elif isinstance(obj, np.integer):\n",
    "            return int(obj)\n",
    "        elif isinstance(obj, np.floating):\n",
    "            if np.isnan(obj):\n",
    "                return None\n",
    "            return float(obj)\n",
    "        elif isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        elif pd.isna(obj):\n",
    "            return None\n",
    "        else:\n",
    "            return obj\n",
    "    \n",
    "    def check_file_exists(self, file_path: str) -> Tuple[bool, Optional[str]]:\n",
    "        \"\"\"Check if file exists and return its absolute path.\"\"\"\n",
    "        full_path = self.job_output_dir / file_path\n",
    "        \n",
    "        # Handle wildcards\n",
    "        if '*' in file_path:\n",
    "            parent = full_path.parent\n",
    "            pattern = full_path.name\n",
    "            if parent.exists():\n",
    "                matching_files = list(parent.glob(pattern))\n",
    "                if matching_files:\n",
    "                    return True, str(matching_files[0])\n",
    "            return False, None\n",
    "        \n",
    "        return full_path.exists(), str(full_path) if full_path.exists() else None\n",
    "    \n",
    "    def get_dtype_string(self, dtype) -> str:\n",
    "        \"\"\"Convert numpy/pandas dtype to simple string.\"\"\"\n",
    "        dtype_str = str(dtype)\n",
    "        \n",
    "        if 'int' in dtype_str:\n",
    "            return 'int'\n",
    "        elif 'float' in dtype_str:\n",
    "            return 'float'\n",
    "        elif 'object' in dtype_str or 'string' in dtype_str:\n",
    "            return 'str'\n",
    "        elif 'datetime' in dtype_str:\n",
    "            return 'datetime'\n",
    "        elif 'bool' in dtype_str:\n",
    "            return 'bool'\n",
    "        else:\n",
    "            return dtype_str\n",
    "    \n",
    "    def get_sample_values(self, series: pd.Series, n_samples: int = 5) -> List:\n",
    "        \"\"\"Get sample values from a series.\"\"\"\n",
    "        unique_vals = series.dropna().unique()\n",
    "        if len(unique_vals) <= n_samples:\n",
    "            return [self.convert_to_native_types(val) for val in unique_vals]\n",
    "        else:\n",
    "            samples = []\n",
    "            if pd.api.types.is_numeric_dtype(series):\n",
    "                samples.append(self.convert_to_native_types(series.min()))\n",
    "                samples.append(self.convert_to_native_types(series.max()))\n",
    "                remaining = n_samples - 2\n",
    "                if remaining > 0:\n",
    "                    random_samples = series.dropna().sample(n=min(remaining, len(series))).tolist()\n",
    "                    samples.extend([self.convert_to_native_types(val) for val in random_samples[:remaining]])\n",
    "            else:\n",
    "                samples = [self.convert_to_native_types(val) for val in unique_vals[:n_samples]]\n",
    "            return samples\n",
    "    \n",
    "    def check_parquet_file(self, file_path: str, expected_info: Dict) -> Dict:\n",
    "        \"\"\"Check a parquet file's structure.\"\"\"\n",
    "        exists, abs_path = self.check_file_exists(file_path)\n",
    "        expected_columns = expected_info.get('columns', {})\n",
    "        \n",
    "        result = {\n",
    "            'file': file_path,\n",
    "            'exists': exists,\n",
    "            'path': abs_path,\n",
    "            'row_count': 0,\n",
    "            'columns': {},\n",
    "            'extra_columns': {},\n",
    "            'required_for': expected_info.get('required_for', []),\n",
    "            'output_from': expected_info.get('output_from', [])\n",
    "        }\n",
    "        \n",
    "        if exists and abs_path:\n",
    "            try:\n",
    "                df = pd.read_parquet(abs_path)\n",
    "                result['row_count'] = len(df)\n",
    "                \n",
    "                # Get actual columns and types\n",
    "                actual_columns = {}\n",
    "                for col in df.columns:\n",
    "                    actual_columns[col] = self.get_dtype_string(df[col].dtype)\n",
    "                \n",
    "                # Store discovered columns\n",
    "                if file_path not in self.discovered_columns:\n",
    "                    self.discovered_columns[file_path] = {}\n",
    "                \n",
    "                # Compare with expected\n",
    "                for exp_col, exp_type in expected_columns.items():\n",
    "                    if exp_col in actual_columns:\n",
    "                        actual_type = actual_columns[exp_col]\n",
    "                        \n",
    "                        if isinstance(exp_type, list):\n",
    "                            type_match = actual_type in exp_type\n",
    "                        else:\n",
    "                            type_match = (actual_type == exp_type) or \\\n",
    "                                       (exp_type == 'str' and actual_type == 'object')\n",
    "                        \n",
    "                        result['columns'][exp_col] = {\n",
    "                            'expected_type': exp_type,\n",
    "                            'actual_type': actual_type,\n",
    "                            'exists': True,\n",
    "                            'type_match': type_match,\n",
    "                            'null_count': int(df[exp_col].isnull().sum()),\n",
    "                            'unique_count': int(df[exp_col].nunique()),\n",
    "                            'sample_values': self.get_sample_values(df[exp_col])\n",
    "                        }\n",
    "                    else:\n",
    "                        result['columns'][exp_col] = {\n",
    "                            'expected_type': exp_type,\n",
    "                            'actual_type': None,\n",
    "                            'exists': False,\n",
    "                            'type_match': False,\n",
    "                            'null_count': None,\n",
    "                            'unique_count': None,\n",
    "                            'sample_values': []\n",
    "                        }\n",
    "                \n",
    "                # Find extra columns\n",
    "                for col in actual_columns:\n",
    "                    if col not in expected_columns:\n",
    "                        result['extra_columns'][col] = {\n",
    "                            'actual_type': actual_columns[col],\n",
    "                            'null_count': int(df[col].isnull().sum()),\n",
    "                            'unique_count': int(df[col].nunique()),\n",
    "                            'sample_values': self.get_sample_values(df[col]),\n",
    "                            'stats': self.get_column_stats(df[col])\n",
    "                        }\n",
    "                        self.discovered_columns[file_path][col] = result['extra_columns'][col]\n",
    "                \n",
    "                # Special checks for sensitivity results\n",
    "                if 'sensitivity_score' in df.columns:\n",
    "                    result['sensitivity_range'] = {\n",
    "                        'min': float(df['sensitivity_score'].min()),\n",
    "                        'max': float(df['sensitivity_score'].max()),\n",
    "                        'mean': float(df['sensitivity_score'].mean()),\n",
    "                        'std': float(df['sensitivity_score'].std())\n",
    "                    }\n",
    "                \n",
    "                if 'parameter' in df.columns:\n",
    "                    result['n_parameters'] = int(df['parameter'].nunique())\n",
    "                    result['top_parameters'] = df.nlargest(5, 'sensitivity_score')[['parameter', 'sensitivity_score']].to_dict('records') if 'sensitivity_score' in df.columns else []\n",
    "                \n",
    "            except Exception as e:\n",
    "                result['error'] = str(e)\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def check_json_file(self, file_path: str) -> Dict:\n",
    "        \"\"\"Check a JSON file's structure.\"\"\"\n",
    "        exists, abs_path = self.check_file_exists(file_path)\n",
    "        \n",
    "        result = {\n",
    "            'file': file_path,\n",
    "            'exists': exists,\n",
    "            'path': abs_path,\n",
    "            'content_type': 'json'\n",
    "        }\n",
    "        \n",
    "        if exists and abs_path:\n",
    "            try:\n",
    "                with open(abs_path, 'r') as f:\n",
    "                    content = json.load(f)\n",
    "                \n",
    "                result['keys'] = list(content.keys()) if isinstance(content, dict) else ['list_content']\n",
    "                result['size_bytes'] = os.path.getsize(abs_path)\n",
    "                \n",
    "            except Exception as e:\n",
    "                result['error'] = str(e)\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def get_column_stats(self, series: pd.Series) -> Dict:\n",
    "        \"\"\"Get statistics for a column.\"\"\"\n",
    "        stats = {\n",
    "            'total_count': int(len(series)),\n",
    "            'non_null_count': int(series.count()),\n",
    "            'null_percentage': float((series.isnull().sum() / len(series) * 100) if len(series) > 0 else 0)\n",
    "        }\n",
    "        \n",
    "        if pd.api.types.is_numeric_dtype(series):\n",
    "            if series.count() > 0:\n",
    "                stats.update({\n",
    "                    'mean': self.convert_to_native_types(series.mean()),\n",
    "                    'std': self.convert_to_native_types(series.std()),\n",
    "                    'min': self.convert_to_native_types(series.min()),\n",
    "                    'max': self.convert_to_native_types(series.max()),\n",
    "                    'q25': self.convert_to_native_types(series.quantile(0.25)),\n",
    "                    'q50': self.convert_to_native_types(series.quantile(0.50)),\n",
    "                    'q75': self.convert_to_native_types(series.quantile(0.75))\n",
    "                })\n",
    "        elif pd.api.types.is_string_dtype(series) or series.dtype == 'object':\n",
    "            value_counts = series.value_counts()\n",
    "            stats.update({\n",
    "                'unique_values': int(len(value_counts)),\n",
    "                'most_common': {str(k): int(v) for k, v in value_counts.head(5).items()} if len(value_counts) > 0 else {}\n",
    "            })\n",
    "        \n",
    "        return stats\n",
    "    \n",
    "    def check_sensitivity_configuration(self) -> Dict:\n",
    "        \"\"\"Check what type of sensitivity analysis was configured based on available files.\"\"\"\n",
    "        config_check = {\n",
    "            'analysis_types': [],\n",
    "            'multi_level': False,\n",
    "            'time_slicing': False,\n",
    "            'advanced_analysis': {\n",
    "                'uncertainty': False,\n",
    "                'threshold': False,\n",
    "                'regional': False,\n",
    "                'sobol': False,\n",
    "                'temporal': False\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Check for modification-based\n",
    "        if self.check_file_exists('modified_idfs/modifications_detail_*.parquet')[0]:\n",
    "            config_check['analysis_types'].append('modification_based')\n",
    "        \n",
    "        # Check for traditional\n",
    "        if self.check_file_exists('parsed_data/idf_data/by_category/*.parquet')[0]:\n",
    "            config_check['analysis_types'].append('traditional')\n",
    "        \n",
    "        # Check for hybrid\n",
    "        if self.check_file_exists('sensitivity_results/hybrid_sensitivity_results.parquet')[0]:\n",
    "            config_check['analysis_types'].append('hybrid')\n",
    "        \n",
    "        # Check for multi-level\n",
    "        if (self.check_file_exists('parsed_data/relationships/zone_mappings.parquet')[0] and\n",
    "            self.check_file_exists('parsed_data/sql_results/timeseries/aggregated/daily/zones_daily.parquet')[0]):\n",
    "            config_check['multi_level'] = True\n",
    "        \n",
    "        # Check for time slicing\n",
    "        if (self.check_file_exists('sensitivity_results/parameter_stability_across_time_slices.parquet')[0] or\n",
    "            glob.glob(str(self.job_output_dir / 'sensitivity_results/*_sensitivity_results_*.parquet'))):\n",
    "            config_check['time_slicing'] = True\n",
    "        \n",
    "        # Check advanced analyses\n",
    "        advanced_files = {\n",
    "            'uncertainty': 'uncertainty_analysis_results.parquet',\n",
    "            'threshold': 'threshold_analysis_results.parquet',\n",
    "            'regional': 'regional_sensitivity_results.parquet',\n",
    "            'sobol': 'sobol_analysis_results.parquet',\n",
    "            'temporal': 'temporal_pattern_results.parquet'\n",
    "        }\n",
    "        \n",
    "        for analysis, filename in advanced_files.items():\n",
    "            if self.check_file_exists(f'sensitivity_results/{filename}')[0]:\n",
    "                config_check['advanced_analysis'][analysis] = True\n",
    "        \n",
    "        return config_check\n",
    "    \n",
    "    def check_all_files(self) -> pd.DataFrame:\n",
    "        \"\"\"Check all expected files and create comparison table.\"\"\"\n",
    "        print(f\"Checking sensitivity data structure in: {self.job_output_dir}\\n\")\n",
    "        \n",
    "        all_results = []\n",
    "        \n",
    "        for file_path, expected_info in self.expected_structure.items():\n",
    "            print(f\"Checking: {file_path}\")\n",
    "            \n",
    "            if file_path.endswith('.json'):\n",
    "                result = self.check_json_file(file_path)\n",
    "                all_results.append({\n",
    "                    'File': file_path.split('/')[-1],\n",
    "                    'Path': file_path,\n",
    "                    'File_Exists': '✓' if result['exists'] else '✗',\n",
    "                    'File_Type': 'JSON',\n",
    "                    'Status': 'OK' if result['exists'] else 'MISSING',\n",
    "                    'Required_For': ', '.join(expected_info.get('required_for', [])),\n",
    "                    'Output_From': ', '.join(expected_info.get('output_from', []))\n",
    "                })\n",
    "            else:\n",
    "                result = self.check_parquet_file(file_path, expected_info)\n",
    "                \n",
    "                if result['exists']:\n",
    "                    # Add rows for expected columns\n",
    "                    for col_name, col_info in result['columns'].items():\n",
    "                        all_results.append({\n",
    "                            'File': file_path.split('/')[-1],\n",
    "                            'Path': file_path,\n",
    "                            'File_Exists': '✓',\n",
    "                            'File_Type': 'Parquet',\n",
    "                            'Rows': result['row_count'],\n",
    "                            'Column': col_name,\n",
    "                            'Column_Type': 'Expected',\n",
    "                            'Expected_Type': col_info['expected_type'],\n",
    "                            'Actual_Type': col_info['actual_type'],\n",
    "                            'Column_Exists': '✓' if col_info['exists'] else '✗',\n",
    "                            'Type_Match': '✓' if col_info['type_match'] else '✗',\n",
    "                            'Null_Count': col_info['null_count'],\n",
    "                            'Unique_Values': col_info['unique_count'],\n",
    "                            'Sample_Values': str(col_info['sample_values'][:3]) if col_info['sample_values'] else 'N/A',\n",
    "                            'Status': 'OK' if col_info['exists'] and col_info['type_match'] else 'ISSUE',\n",
    "                            'Required_For': ', '.join(result.get('required_for', [])),\n",
    "                            'Output_From': ', '.join(result.get('output_from', []))\n",
    "                        })\n",
    "                    \n",
    "                    # Add rows for extra columns\n",
    "                    for col_name, col_info in result['extra_columns'].items():\n",
    "                        all_results.append({\n",
    "                            'File': file_path.split('/')[-1],\n",
    "                            'Path': file_path,\n",
    "                            'File_Exists': '✓',\n",
    "                            'File_Type': 'Parquet',\n",
    "                            'Rows': result['row_count'],\n",
    "                            'Column': col_name,\n",
    "                            'Column_Type': 'DISCOVERED',\n",
    "                            'Expected_Type': 'N/A',\n",
    "                            'Actual_Type': col_info['actual_type'],\n",
    "                            'Column_Exists': '✓',\n",
    "                            'Type_Match': 'N/A',\n",
    "                            'Null_Count': col_info['null_count'],\n",
    "                            'Unique_Values': col_info['unique_count'],\n",
    "                            'Sample_Values': str(col_info['sample_values'][:3]) if col_info['sample_values'] else 'N/A',\n",
    "                            'Status': 'EXTRA',\n",
    "                            'Required_For': 'N/A',\n",
    "                            'Output_From': 'N/A'\n",
    "                        })\n",
    "                else:\n",
    "                    all_results.append({\n",
    "                        'File': file_path.split('/')[-1],\n",
    "                        'Path': file_path,\n",
    "                        'File_Exists': '✗',\n",
    "                        'File_Type': 'Parquet',\n",
    "                        'Rows': 0,\n",
    "                        'Column': 'N/A',\n",
    "                        'Column_Type': 'N/A',\n",
    "                        'Expected_Type': 'N/A',\n",
    "                        'Actual_Type': 'N/A',\n",
    "                        'Column_Exists': 'N/A',\n",
    "                        'Type_Match': 'N/A',\n",
    "                        'Status': 'MISSING FILE',\n",
    "                        'Required_For': ', '.join(result.get('required_for', [])),\n",
    "                        'Output_From': ', '.join(result.get('output_from', []))\n",
    "                    })\n",
    "        \n",
    "        return pd.DataFrame(all_results)\n",
    "    \n",
    "    def create_summary_report(self, df_results: pd.DataFrame) -> Dict:\n",
    "        \"\"\"Create a summary report of the check results.\"\"\"\n",
    "        config_check = self.check_sensitivity_configuration()\n",
    "        \n",
    "        summary = {\n",
    "            'check_timestamp': datetime.now().isoformat(),\n",
    "            'job_output_dir': str(self.job_output_dir),\n",
    "            'configuration_detected': config_check,\n",
    "            'total_files_expected': len(self.expected_structure),\n",
    "            'files_found': len(df_results[df_results['File_Exists'] == '✓']['Path'].unique()),\n",
    "            'files_missing': len(df_results[df_results['File_Exists'] == '✗']['Path'].unique()),\n",
    "            'total_expected_columns': len(df_results[df_results['Column_Type'] == 'Expected']),\n",
    "            'expected_columns_found': len(df_results[(df_results['Column_Type'] == 'Expected') & \n",
    "                                                    (df_results['Column_Exists'] == '✓')]),\n",
    "            'total_discovered_columns': len(df_results[df_results['Column_Type'] == 'DISCOVERED']),\n",
    "            'total_issues': len(df_results[df_results['Status'].isin(['ISSUE', 'MISSING FILE'])]),\n",
    "            'missing_files': [],\n",
    "            'column_issues': [],\n",
    "            'type_mismatches': [],\n",
    "            'pipeline_readiness': {\n",
    "                'modification_based': {\n",
    "                    'has_modifications': False,\n",
    "                    'has_base_results': False,\n",
    "                    'has_modified_results': False,\n",
    "                    'has_output_files': False,\n",
    "                    'ready': False\n",
    "                },\n",
    "                'traditional': {\n",
    "                    'has_parameters': False,\n",
    "                    'has_base_results': False,\n",
    "                    'has_output_files': False,\n",
    "                    'ready': False\n",
    "                },\n",
    "                'multi_level': {\n",
    "                    'has_zone_mappings': False,\n",
    "                    'has_zone_results': False,\n",
    "                    'has_equipment_assignments': False,\n",
    "                    'ready': False\n",
    "                },\n",
    "                'time_slicing': {\n",
    "                    'has_hourly_data': False,\n",
    "                    'has_time_slice_outputs': False,\n",
    "                    'ready': False\n",
    "                },\n",
    "                'advanced': {\n",
    "                    'uncertainty': False,\n",
    "                    'threshold': False,\n",
    "                    'regional': False,\n",
    "                    'sobol': False,\n",
    "                    'temporal': False,\n",
    "                    'any_ready': False\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Get missing files\n",
    "        missing_files = df_results[df_results['File_Exists'] == '✗']['Path'].unique()\n",
    "        summary['missing_files'] = list(missing_files)\n",
    "        \n",
    "        # Get column issues\n",
    "        column_issues = df_results[(df_results['Column_Exists'] == '✗') & \n",
    "                                 (df_results['File_Exists'] == '✓') & \n",
    "                                 (df_results['Column_Type'] == 'Expected')]\n",
    "        for _, row in column_issues.iterrows():\n",
    "            summary['column_issues'].append(f\"{row['File']}: {row['Column']}\")\n",
    "        \n",
    "        # Get type mismatches\n",
    "        type_issues = df_results[(df_results['Type_Match'] == '✗') & \n",
    "                               (df_results['Column_Exists'] == '✓') & \n",
    "                               (df_results['Column_Type'] == 'Expected')]\n",
    "        for _, row in type_issues.iterrows():\n",
    "            summary['type_mismatches'].append(\n",
    "                f\"{row['File']}: {row['Column']} (expected {row['Expected_Type']}, got {row['Actual_Type']})\"\n",
    "            )\n",
    "        \n",
    "        # Check pipeline readiness\n",
    "        files_exist = df_results[df_results['File_Exists'] == '✓']['Path'].unique()\n",
    "        \n",
    "        # Modification-based readiness\n",
    "        summary['pipeline_readiness']['modification_based']['has_modifications'] = any('modifications_detail' in f for f in files_exist)\n",
    "        summary['pipeline_readiness']['modification_based']['has_base_results'] = any('parsed_data/sql_results' in f for f in files_exist)\n",
    "        summary['pipeline_readiness']['modification_based']['has_modified_results'] = any('parsed_modified_results/sql_results' in f for f in files_exist)\n",
    "        summary['pipeline_readiness']['modification_based']['has_output_files'] = any('modification_sensitivity_results' in f for f in files_exist)\n",
    "        summary['pipeline_readiness']['modification_based']['ready'] = all([\n",
    "            summary['pipeline_readiness']['modification_based']['has_modifications'],\n",
    "            summary['pipeline_readiness']['modification_based']['has_base_results'],\n",
    "            summary['pipeline_readiness']['modification_based']['has_modified_results']\n",
    "        ])\n",
    "        \n",
    "        # Traditional readiness\n",
    "        summary['pipeline_readiness']['traditional']['has_parameters'] = any('idf_data/by_category' in f or 'parameter_matrix' in f for f in files_exist)\n",
    "        summary['pipeline_readiness']['traditional']['has_base_results'] = summary['pipeline_readiness']['modification_based']['has_base_results']\n",
    "        summary['pipeline_readiness']['traditional']['has_output_files'] = any('traditional_sensitivity_results' in f for f in files_exist)\n",
    "        summary['pipeline_readiness']['traditional']['ready'] = all([\n",
    "            summary['pipeline_readiness']['traditional']['has_parameters'],\n",
    "            summary['pipeline_readiness']['traditional']['has_base_results']\n",
    "        ])\n",
    "        \n",
    "        # Multi-level readiness\n",
    "        summary['pipeline_readiness']['multi_level']['has_zone_mappings'] = any('zone_mappings' in f for f in files_exist)\n",
    "        summary['pipeline_readiness']['multi_level']['has_zone_results'] = any('zones_daily' in f for f in files_exist)\n",
    "        summary['pipeline_readiness']['multi_level']['has_equipment_assignments'] = any('equipment_assignments' in f for f in files_exist)\n",
    "        summary['pipeline_readiness']['multi_level']['ready'] = all([\n",
    "            summary['pipeline_readiness']['multi_level']['has_zone_mappings'],\n",
    "            summary['pipeline_readiness']['multi_level']['has_zone_results']\n",
    "        ])\n",
    "        \n",
    "        # Time slicing readiness\n",
    "        summary['pipeline_readiness']['time_slicing']['has_hourly_data'] = any('hourly' in f for f in files_exist)\n",
    "        summary['pipeline_readiness']['time_slicing']['has_time_slice_outputs'] = any('time_slice' in f or '_sensitivity_results_' in f for f in files_exist)\n",
    "        summary['pipeline_readiness']['time_slicing']['ready'] = summary['pipeline_readiness']['time_slicing']['has_hourly_data']\n",
    "        \n",
    "        # Advanced analyses\n",
    "        for analysis in ['uncertainty', 'threshold', 'regional', 'sobol', 'temporal']:\n",
    "            summary['pipeline_readiness']['advanced'][analysis] = any(f'{analysis}_' in f for f in files_exist)\n",
    "        summary['pipeline_readiness']['advanced']['any_ready'] = any(\n",
    "            summary['pipeline_readiness']['advanced'][a] for a in ['uncertainty', 'threshold', 'regional', 'sobol', 'temporal']\n",
    "        )\n",
    "        \n",
    "        return summary\n",
    "    \n",
    "    def create_markdown_report(self, output_path: Path, df_results: pd.DataFrame, summary: Dict):\n",
    "        \"\"\"Create a markdown report for easy reading.\"\"\"\n",
    "        with open(output_path, 'w') as f:\n",
    "            f.write(f\"# Sensitivity Analysis Data Structure Check Report\\n\\n\")\n",
    "            f.write(f\"**Generated:** {summary['check_timestamp']}\\n\\n\")\n",
    "            f.write(f\"**Directory:** `{summary['job_output_dir']}`\\n\\n\")\n",
    "            \n",
    "            # Configuration detected\n",
    "            f.write(\"## Configuration Detected\\n\\n\")\n",
    "            config = summary['configuration_detected']\n",
    "            f.write(f\"- **Analysis Types:** {', '.join(config['analysis_types']) if config['analysis_types'] else 'None detected'}\\n\")\n",
    "            f.write(f\"- **Multi-level Analysis:** {'✓' if config['multi_level'] else '✗'}\\n\")\n",
    "            f.write(f\"- **Time Slicing:** {'✓' if config['time_slicing'] else '✗'}\\n\")\n",
    "            f.write(f\"- **Advanced Analyses:**\\n\")\n",
    "            for analysis, enabled in config['advanced_analysis'].items():\n",
    "                f.write(f\"  - {analysis.capitalize()}: {'✓' if enabled else '✗'}\\n\")\n",
    "            f.write(\"\\n\")\n",
    "            \n",
    "            # Summary statistics\n",
    "            f.write(\"## Summary\\n\\n\")\n",
    "            f.write(f\"- **Files Expected:** {summary['total_files_expected']}\\n\")\n",
    "            f.write(f\"- **Files Found:** {summary['files_found']}\\n\")\n",
    "            f.write(f\"- **Files Missing:** {summary['files_missing']}\\n\")\n",
    "            f.write(f\"- **Expected Columns:** {summary['total_expected_columns']}\\n\")\n",
    "            f.write(f\"- **Expected Columns Found:** {summary['expected_columns_found']}\\n\")\n",
    "            f.write(f\"- **Extra Columns Discovered:** {summary['total_discovered_columns']}\\n\")\n",
    "            f.write(f\"- **Total Issues:** {summary['total_issues']}\\n\\n\")\n",
    "            \n",
    "            # Pipeline readiness\n",
    "            f.write(\"## Pipeline Readiness\\n\\n\")\n",
    "            \n",
    "            f.write(\"### Modification-Based Analysis\\n\")\n",
    "            mod_ready = summary['pipeline_readiness']['modification_based']\n",
    "            f.write(f\"- **Modifications:** {'✓' if mod_ready['has_modifications'] else '✗'}\\n\")\n",
    "            f.write(f\"- **Base Results:** {'✓' if mod_ready['has_base_results'] else '✗'}\\n\")\n",
    "            f.write(f\"- **Modified Results:** {'✓' if mod_ready['has_modified_results'] else '✗'}\\n\")\n",
    "            f.write(f\"- **Output Files:** {'✓' if mod_ready['has_output_files'] else '✗'}\\n\")\n",
    "            f.write(f\"- **Ready:** {'✓ YES' if mod_ready['ready'] else '✗ NO'}\\n\\n\")\n",
    "            \n",
    "            f.write(\"### Traditional Analysis\\n\")\n",
    "            trad_ready = summary['pipeline_readiness']['traditional']\n",
    "            f.write(f\"- **Parameters:** {'✓' if trad_ready['has_parameters'] else '✗'}\\n\")\n",
    "            f.write(f\"- **Base Results:** {'✓' if trad_ready['has_base_results'] else '✗'}\\n\")\n",
    "            f.write(f\"- **Output Files:** {'✓' if trad_ready['has_output_files'] else '✗'}\\n\")\n",
    "            f.write(f\"- **Ready:** {'✓ YES' if trad_ready['ready'] else '✗ NO'}\\n\\n\")\n",
    "            \n",
    "            f.write(\"### Multi-Level Analysis\\n\")\n",
    "            multi_ready = summary['pipeline_readiness']['multi_level']\n",
    "            f.write(f\"- **Zone Mappings:** {'✓' if multi_ready['has_zone_mappings'] else '✗'}\\n\")\n",
    "            f.write(f\"- **Zone Results:** {'✓' if multi_ready['has_zone_results'] else '✗'}\\n\")\n",
    "            f.write(f\"- **Equipment Assignments:** {'✓' if multi_ready['has_equipment_assignments'] else '✗'}\\n\")\n",
    "            f.write(f\"- **Ready:** {'✓ YES' if multi_ready['ready'] else '✗ NO'}\\n\\n\")\n",
    "            \n",
    "            if summary['missing_files']:\n",
    "                f.write(\"## Missing Files\\n\\n\")\n",
    "                for file in summary['missing_files']:\n",
    "                    f.write(f\"- `{file}`\\n\")\n",
    "                f.write(\"\\n\")\n",
    "    \n",
    "    def save_results(self, df_results: pd.DataFrame, summary: Dict):\n",
    "        \"\"\"Save all results to files.\"\"\"\n",
    "        output_dir = self.job_output_dir / 'sensitivity_data_check'\n",
    "        output_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        \n",
    "        # Save detailed results\n",
    "        df_results.to_csv(output_dir / f'structure_check_details_{timestamp}.csv', index=False)\n",
    "        df_results.to_excel(output_dir / f'structure_check_details_{timestamp}.xlsx', index=False)\n",
    "        \n",
    "        # Save summary\n",
    "        with open(output_dir / f'structure_check_summary_{timestamp}.json', 'w') as f:\n",
    "            json.dump(summary, f, indent=2)\n",
    "        \n",
    "        # Create markdown report\n",
    "        self.create_markdown_report(output_dir / f'structure_report_{timestamp}.md', df_results, summary)\n",
    "        \n",
    "        # Save issues only\n",
    "        df_issues = df_results[df_results['Status'].isin(['ISSUE', 'MISSING FILE'])]\n",
    "        if not df_issues.empty:\n",
    "            df_issues.to_csv(output_dir / f'issues_only_{timestamp}.csv', index=False)\n",
    "        \n",
    "        print(f\"\\nResults saved to: {output_dir}\")\n",
    "        print(f\"Files created:\")\n",
    "        print(f\"  - structure_check_details_{timestamp}.csv/xlsx\")\n",
    "        print(f\"  - structure_check_summary_{timestamp}.json\")\n",
    "        print(f\"  - structure_report_{timestamp}.md\")\n",
    "        if not df_issues.empty:\n",
    "            print(f\"  - issues_only_{timestamp}.csv\")\n",
    "\n",
    "# Usage function\n",
    "def check_sensitivity_data_structure(job_output_dir: str):\n",
    "    \"\"\"Main function to check sensitivity data structure.\"\"\"\n",
    "    checker = SensitivityDataChecker(job_output_dir)\n",
    "    \n",
    "    # Run checks\n",
    "    df_results = checker.check_all_files()\n",
    "    summary = checker.create_summary_report(df_results)\n",
    "    \n",
    "    # Print summary\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"CONFIGURATION DETECTED\")\n",
    "    print(\"=\"*80)\n",
    "    config = summary['configuration_detected']\n",
    "    print(f\"Analysis Types: {', '.join(config['analysis_types']) if config['analysis_types'] else 'None'}\")\n",
    "    print(f\"Multi-level: {'✓' if config['multi_level'] else '✗'}\")\n",
    "    print(f\"Time Slicing: {'✓' if config['time_slicing'] else '✗'}\")\n",
    "    print(f\"Advanced Analyses: {sum(config['advanced_analysis'].values())} enabled\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"SUMMARY\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Total files expected: {summary['total_files_expected']}\")\n",
    "    print(f\"Files found: {summary['files_found']}\")\n",
    "    print(f\"Files missing: {summary['files_missing']}\")\n",
    "    print(f\"Total issues: {summary['total_issues']}\")\n",
    "    \n",
    "    print(\"\\nPIPELINE READINESS:\")\n",
    "    for pipeline, status in summary['pipeline_readiness'].items():\n",
    "        if isinstance(status, dict) and 'ready' in status:\n",
    "            print(f\"  - {pipeline}: {'✓ READY' if status['ready'] else '✗ NOT READY'}\")\n",
    "    \n",
    "    # Save results\n",
    "    checker.save_results(df_results, summary)\n",
    "    \n",
    "    return df_results, summary\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Replace with your actual job directory\n",
    "    job_dir = r\"D:\\Documents\\daily\\E_Plus_2040_py\\output\\82e2b83f-5013-4270-8266-a37a67dbd4ff\"\n",
    "    df_results, summary = check_sensitivity_data_structure(job_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c85fb28",
   "metadata": {},
   "source": [
    "## check v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0542353d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================================================================================================\n",
      "                         \u001b[1m\u001b[96mSENSITIVITY DATA STRUCTURE DASHBOARD\u001b[0m                          \n",
      "====================================================================================================\n",
      "\n",
      "\u001b[1m\u001b[94m📊 OVERVIEW\u001b[0m\n",
      "  Total Files Expected: 33\n",
      "  Total Files Found: \u001b[92m14\u001b[0m\n",
      "  Overall Completeness: \u001b[91m████████░░░░░░░░░░░░\u001b[0m 42.4%\n",
      "  Total Data Size: 1.46 MB\n",
      "  Total Data Rows: 320,510\n",
      "\n",
      "\u001b[1m\u001b[94m📁 CATEGORY BREAKDOWN\u001b[0m\n",
      "\n",
      "  \u001b[1m\u001b[93mInput Data Files\u001b[0m\n",
      "  \u001b[93m████████████░░░░░░░░\u001b[0m 62.5%\n",
      "  Files: 10/16\n",
      "\n",
      "    ✅ Modification Tracking\n",
      "       Files: 1/1 | Size: 0.03 MB | Rows: 893\n",
      "\n",
      "    ⚠️ Base Simulation Results\n",
      "       Files: 3/6 | Size: 0.11 MB | Rows: 213,060\n",
      "       \u001b[91m❌ Missing:\u001b[0m energy_daily.parquet\n",
      "       \u001b[91m❌ Missing:\u001b[0m electricity_daily.parquet\n",
      "       \u001b[91m❌ Missing:\u001b[0m temperature_daily.parquet\n",
      "\n",
      "    ⚠️ Modified Simulation Results\n",
      "       Files: 2/4 | Size: 0.07 MB | Rows: 105,190\n",
      "       \u001b[91m❌ Missing:\u001b[0m energy_daily.parquet\n",
      "       \u001b[91m❌ Missing:\u001b[0m electricity_daily.parquet\n",
      "\n",
      "    ✅ Zone/Equipment Relationships\n",
      "       Files: 2/2 | Size: 0.00 MB | Rows: 89\n",
      "\n",
      "    ⚠️ Traditional Analysis Parameters\n",
      "       Files: 1/2 | Size: 0.43 MB | Rows: 3\n",
      "       \u001b[91m❌ Missing:\u001b[0m parameter_matrix.parquet\n",
      "\n",
      "    ✅ Building Metadata\n",
      "       Files: 1/1 | Size: 0.01 MB | Rows: 3\n",
      "\n",
      "  \u001b[1m\u001b[93mSensitivity Analysis Outputs\u001b[0m\n",
      "  \u001b[91m████░░░░░░░░░░░░░░░░\u001b[0m 23.5%\n",
      "  Files: 4/17\n",
      "\n",
      "    ❌ Main Sensitivity Results\n",
      "       Files: 0/3 | Size: 0.00 MB | Rows: 0\n",
      "       \u001b[91m❌ Missing:\u001b[0m modification_sensitivity_results.parquet\n",
      "       \u001b[91m❌ Missing:\u001b[0m traditional_sensitivity_results.parquet\n",
      "       \u001b[91m❌ Missing:\u001b[0m hybrid_sensitivity_results.parquet\n",
      "\n",
      "    ⚠️ Time Slice Results\n",
      "       Files: 1/2 | Size: 0.02 MB | Rows: 904\n",
      "       \u001b[91m❌ Missing:\u001b[0m parameter_stability_across_time_slices.parquet\n",
      "\n",
      "    ⚠️ Advanced Analysis Results\n",
      "       Files: 2/5 | Size: 0.03 MB | Rows: 368\n",
      "       \u001b[91m❌ Missing:\u001b[0m regional_sensitivity_results.parquet\n",
      "       \u001b[91m❌ Missing:\u001b[0m sobol_analysis_results.parquet\n",
      "       \u001b[91m❌ Missing:\u001b[0m temporal_pattern_results.parquet\n",
      "\n",
      "    ❌ Export Files\n",
      "       Files: 0/3 | Size: 0.00 MB | Rows: 0\n",
      "       \u001b[91m❌ Missing:\u001b[0m top_sensitive_parameters.csv\n",
      "       \u001b[91m❌ Missing:\u001b[0m calibration_parameters.json\n",
      "       \u001b[91m❌ Missing:\u001b[0m sensitive_parameters_for_surrogate.json\n",
      "\n",
      "    ⚠️ Analysis Reports\n",
      "       Files: 1/4 | Size: 0.76 MB | Rows: 0\n",
      "       \u001b[91m❌ Missing:\u001b[0m sensitivity_summary.json\n",
      "       \u001b[91m❌ Missing:\u001b[0m time_slice_sensitivity_summary.json\n",
      "       \u001b[91m❌ Missing:\u001b[0m advanced_sensitivity_report.json\n",
      "\n",
      "\u001b[1m\u001b[91m🚨 CRITICAL MISSING FILES\u001b[0m\n",
      "  - parsed_data/sql_results/timeseries/aggregated/daily/energy_daily.parquet\n",
      "  - parsed_modified_results/sql_results/timeseries/aggregated/daily/energy_daily.parquet\n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "\u001b[1m\u001b[95m🔧 DETECTED CONFIGURATION\u001b[0m\n",
      "  Analysis Types: Modification-based, Traditional\n",
      "  Features Enabled: Multi-level Analysis, Advanced Analysis\n",
      "\n",
      "\u001b[1m\u001b[92m💾 Results saved to:\u001b[0m D:\\Documents\\daily\\E_Plus_2040_py\\output\\82e2b83f-5013-4270-8266-a37a67dbd4ff\\sensitivity_structure_check\n",
      "  - Visual HTML Report: visual_report_20250627_045211.html\n",
      "  - File Comparison Matrix: file_comparison_matrix_20250627_045211.csv\n",
      "  - Structure Summary: sensitivity_structure_summary_20250627_045211.json\n",
      "  - Missing Files List: missing_files_20250627_045211.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Optional, Any\n",
    "import json\n",
    "from datetime import datetime\n",
    "import glob\n",
    "from collections import defaultdict\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class EnhancedSensitivityDataChecker:\n",
    "    def __init__(self, job_output_dir: str):\n",
    "        self.job_output_dir = Path(job_output_dir)\n",
    "        self.results = []\n",
    "        self.discovered_columns = {}\n",
    "        \n",
    "        # Color codes for terminal output\n",
    "        self.colors = {\n",
    "            'green': '\\033[92m',\n",
    "            'red': '\\033[91m',\n",
    "            'yellow': '\\033[93m',\n",
    "            'blue': '\\033[94m',\n",
    "            'purple': '\\033[95m',\n",
    "            'cyan': '\\033[96m',\n",
    "            'bold': '\\033[1m',\n",
    "            'underline': '\\033[4m',\n",
    "            'end': '\\033[0m'\n",
    "        }\n",
    "        \n",
    "        # Categorized structure for better organization\n",
    "        self.file_categories = {\n",
    "            'INPUT_DATA': {\n",
    "                'name': 'Input Data Files',\n",
    "                'subcategories': {\n",
    "                    'MODIFICATIONS': {\n",
    "                        'name': 'Modification Tracking',\n",
    "                        'files': ['modified_idfs/modifications_detail_*.parquet']\n",
    "                    },\n",
    "                    'BASE_RESULTS': {\n",
    "                        'name': 'Base Simulation Results',\n",
    "                        'files': [\n",
    "                            'parsed_data/sql_results/timeseries/aggregated/daily/hvac_daily.parquet',\n",
    "                            'parsed_data/sql_results/timeseries/aggregated/daily/energy_daily.parquet',\n",
    "                            'parsed_data/sql_results/timeseries/aggregated/daily/electricity_daily.parquet',\n",
    "                            'parsed_data/sql_results/timeseries/aggregated/daily/zones_daily.parquet',\n",
    "                            'parsed_data/sql_results/timeseries/aggregated/daily/temperature_daily.parquet',\n",
    "                            'parsed_data/sql_results/timeseries/aggregated/daily/ventilation_daily.parquet'\n",
    "                        ]\n",
    "                    },\n",
    "                    'MODIFIED_RESULTS': {\n",
    "                        'name': 'Modified Simulation Results',\n",
    "                        'files': [\n",
    "                            'parsed_modified_results/sql_results/timeseries/aggregated/daily/hvac_daily.parquet',\n",
    "                            'parsed_modified_results/sql_results/timeseries/aggregated/daily/energy_daily.parquet',\n",
    "                            'parsed_modified_results/sql_results/timeseries/aggregated/daily/electricity_daily.parquet',\n",
    "                            'parsed_modified_results/sql_results/timeseries/aggregated/daily/zones_daily.parquet'\n",
    "                        ]\n",
    "                    },\n",
    "                    'RELATIONSHIPS': {\n",
    "                        'name': 'Zone/Equipment Relationships',\n",
    "                        'files': [\n",
    "                            'parsed_data/relationships/zone_mappings.parquet',\n",
    "                            'parsed_data/relationships/equipment_assignments.parquet'\n",
    "                        ]\n",
    "                    },\n",
    "                    'PARAMETERS': {\n",
    "                        'name': 'Traditional Analysis Parameters',\n",
    "                        'files': [\n",
    "                            'parsed_data/idf_data/by_category/*.parquet',\n",
    "                            'parsed_data/analysis_ready/parameter_matrix.parquet'\n",
    "                        ]\n",
    "                    },\n",
    "                    'METADATA': {\n",
    "                        'name': 'Building Metadata',\n",
    "                        'files': ['parsed_data/metadata/building_registry.parquet']\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            'OUTPUT_DATA': {\n",
    "                'name': 'Sensitivity Analysis Outputs',\n",
    "                'subcategories': {\n",
    "                    'MAIN_RESULTS': {\n",
    "                        'name': 'Main Sensitivity Results',\n",
    "                        'files': [\n",
    "                            'sensitivity_results/modification_sensitivity_results.parquet',\n",
    "                            'sensitivity_results/traditional_sensitivity_results.parquet',\n",
    "                            'sensitivity_results/hybrid_sensitivity_results.parquet'\n",
    "                        ]\n",
    "                    },\n",
    "                    'TIME_SLICE_RESULTS': {\n",
    "                        'name': 'Time Slice Results',\n",
    "                        'files': [\n",
    "                            'sensitivity_results/parameter_stability_across_time_slices.parquet',\n",
    "                            'sensitivity_results/*_sensitivity_results_*.parquet'\n",
    "                        ]\n",
    "                    },\n",
    "                    'ADVANCED_RESULTS': {\n",
    "                        'name': 'Advanced Analysis Results',\n",
    "                        'files': [\n",
    "                            'sensitivity_results/uncertainty_analysis_results.parquet',\n",
    "                            'sensitivity_results/threshold_analysis_results.parquet',\n",
    "                            'sensitivity_results/regional_sensitivity_results.parquet',\n",
    "                            'sensitivity_results/sobol_analysis_results.parquet',\n",
    "                            'sensitivity_results/temporal_pattern_results.parquet'\n",
    "                        ]\n",
    "                    },\n",
    "                    'EXPORTS': {\n",
    "                        'name': 'Export Files',\n",
    "                        'files': [\n",
    "                            'sensitivity_results/top_sensitive_parameters.csv',\n",
    "                            'sensitivity_results/calibration_parameters.json',\n",
    "                            'sensitivity_results/sensitive_parameters_for_surrogate.json'\n",
    "                        ]\n",
    "                    },\n",
    "                    'REPORTS': {\n",
    "                        'name': 'Analysis Reports',\n",
    "                        'files': [\n",
    "                            'sensitivity_results/modification_sensitivity_report.json',\n",
    "                            'sensitivity_results/sensitivity_summary.json',\n",
    "                            'sensitivity_results/time_slice_sensitivity_summary.json',\n",
    "                            'sensitivity_results/advanced_sensitivity_report.json'\n",
    "                        ]\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Expected columns structure (simplified for reference)\n",
    "        self.expected_columns = {\n",
    "            'modifications': {\n",
    "                'building_id': ['str', 'int'],\n",
    "                'category': 'str',\n",
    "                'object_type': 'str',\n",
    "                'object_name': 'str',\n",
    "                'field_name': 'str',\n",
    "                'param_key': 'str',\n",
    "                'param_delta': 'float',\n",
    "                'param_pct_change': 'float'\n",
    "            },\n",
    "            'simulation_results': {\n",
    "                'building_id': ['str', 'int'],\n",
    "                'Variable': 'str',\n",
    "                'Value': 'float',\n",
    "                'DateTime': 'datetime',\n",
    "                'Zone': 'str'\n",
    "            },\n",
    "            'sensitivity_results': {\n",
    "                'parameter': 'str',\n",
    "                'output_variable': 'str',\n",
    "                'sensitivity_score': 'float',\n",
    "                'method': 'str',\n",
    "                'p_value': 'float',\n",
    "                'category': 'str'\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def colored_text(self, text: str, color: str = 'end', bold: bool = False) -> str:\n",
    "        \"\"\"Return colored text for terminal output.\"\"\"\n",
    "        color_code = self.colors.get(color, '')\n",
    "        bold_code = self.colors['bold'] if bold else ''\n",
    "        return f\"{bold_code}{color_code}{text}{self.colors['end']}\"\n",
    "    \n",
    "    def check_file_exists(self, file_path: str) -> Tuple[bool, Optional[str], int]:\n",
    "        \"\"\"Check if file exists and return path and size.\"\"\"\n",
    "        full_path = self.job_output_dir / file_path\n",
    "        \n",
    "        if '*' in file_path:\n",
    "            parent = full_path.parent\n",
    "            pattern = full_path.name\n",
    "            if parent.exists():\n",
    "                matching_files = list(parent.glob(pattern))\n",
    "                if matching_files:\n",
    "                    file_size = sum(f.stat().st_size for f in matching_files)\n",
    "                    return True, str(matching_files[0]), file_size\n",
    "            return False, None, 0\n",
    "        \n",
    "        if full_path.exists():\n",
    "            file_size = full_path.stat().st_size\n",
    "            return True, str(full_path), file_size\n",
    "        return False, None, 0\n",
    "    \n",
    "    def get_file_info(self, file_path: str) -> Dict:\n",
    "        \"\"\"Get comprehensive file information.\"\"\"\n",
    "        exists, abs_path, file_size = self.check_file_exists(file_path)\n",
    "        \n",
    "        info = {\n",
    "            'path': file_path,\n",
    "            'exists': exists,\n",
    "            'abs_path': abs_path,\n",
    "            'size_bytes': file_size,\n",
    "            'size_mb': round(file_size / (1024 * 1024), 2) if file_size > 0 else 0\n",
    "        }\n",
    "        \n",
    "        if exists and abs_path:\n",
    "            if file_path.endswith('.parquet'):\n",
    "                try:\n",
    "                    df = pd.read_parquet(abs_path)\n",
    "                    info.update({\n",
    "                        'type': 'parquet',\n",
    "                        'rows': len(df),\n",
    "                        'columns': list(df.columns),\n",
    "                        'column_count': len(df.columns),\n",
    "                        'memory_usage_mb': round(df.memory_usage(deep=True).sum() / (1024 * 1024), 2),\n",
    "                        'dtypes': {col: str(df[col].dtype) for col in df.columns}\n",
    "                    })\n",
    "                    \n",
    "                    # Sample data\n",
    "                    if len(df) > 0:\n",
    "                        info['sample_data'] = df.head(3).to_dict('records')\n",
    "                    \n",
    "                    # Key column analysis\n",
    "                    if 'sensitivity_score' in df.columns and len(df) > 0:\n",
    "                        info['sensitivity_stats'] = {\n",
    "                            'min': float(df['sensitivity_score'].min()),\n",
    "                            'max': float(df['sensitivity_score'].max()),\n",
    "                            'mean': float(df['sensitivity_score'].mean()),\n",
    "                            'std': float(df['sensitivity_score'].std()),\n",
    "                            'top_5_scores': df.nlargest(5, 'sensitivity_score')['sensitivity_score'].tolist()\n",
    "                        }\n",
    "                    \n",
    "                    if 'parameter' in df.columns:\n",
    "                        info['n_unique_parameters'] = df['parameter'].nunique()\n",
    "                        info['top_5_parameters'] = df['parameter'].value_counts().head(5).to_dict()\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    info['error'] = str(e)\n",
    "                    \n",
    "            elif file_path.endswith('.json'):\n",
    "                try:\n",
    "                    with open(abs_path, 'r') as f:\n",
    "                        content = json.load(f)\n",
    "                    info.update({\n",
    "                        'type': 'json',\n",
    "                        'keys': list(content.keys()) if isinstance(content, dict) else ['list_content'],\n",
    "                        'n_keys': len(content) if isinstance(content, dict) else 1\n",
    "                    })\n",
    "                except Exception as e:\n",
    "                    info['error'] = str(e)\n",
    "        \n",
    "        return info\n",
    "    \n",
    "    def create_visual_summary(self) -> Dict:\n",
    "        \"\"\"Create a visual summary of the data structure.\"\"\"\n",
    "        summary = {\n",
    "            'overview': {\n",
    "                'total_categories': 0,\n",
    "                'total_subcategories': 0,\n",
    "                'total_files_expected': 0,\n",
    "                'total_files_found': 0,\n",
    "                'total_size_mb': 0,\n",
    "                'total_rows': 0\n",
    "            },\n",
    "            'by_category': {},\n",
    "            'missing_critical': [],\n",
    "            'extra_files': [],\n",
    "            'issues': []\n",
    "        }\n",
    "        \n",
    "        # Check each category\n",
    "        for cat_key, category in self.file_categories.items():\n",
    "            cat_summary = {\n",
    "                'name': category['name'],\n",
    "                'subcategories': {},\n",
    "                'total_expected': 0,\n",
    "                'total_found': 0,\n",
    "                'completeness': 0\n",
    "            }\n",
    "            \n",
    "            for subcat_key, subcategory in category['subcategories'].items():\n",
    "                subcat_summary = {\n",
    "                    'name': subcategory['name'],\n",
    "                    'files': [],\n",
    "                    'expected': len(subcategory['files']),\n",
    "                    'found': 0,\n",
    "                    'missing': 0,\n",
    "                    'total_size_mb': 0,\n",
    "                    'total_rows': 0\n",
    "                }\n",
    "                \n",
    "                for file_path in subcategory['files']:\n",
    "                    file_info = self.get_file_info(file_path)\n",
    "                    \n",
    "                    file_summary = {\n",
    "                        'path': file_path,\n",
    "                        'exists': file_info['exists'],\n",
    "                        'size_mb': file_info.get('size_mb', 0),\n",
    "                        'rows': file_info.get('rows', 0),\n",
    "                        'columns': file_info.get('column_count', 0)\n",
    "                    }\n",
    "                    \n",
    "                    if file_info['exists']:\n",
    "                        subcat_summary['found'] += 1\n",
    "                        subcat_summary['total_size_mb'] += file_info.get('size_mb', 0)\n",
    "                        subcat_summary['total_rows'] += file_info.get('rows', 0)\n",
    "                        summary['overview']['total_files_found'] += 1\n",
    "                        summary['overview']['total_size_mb'] += file_info.get('size_mb', 0)\n",
    "                        summary['overview']['total_rows'] += file_info.get('rows', 0)\n",
    "                    else:\n",
    "                        subcat_summary['missing'] += 1\n",
    "                        # Check if it's critical\n",
    "                        if any(critical in file_path for critical in ['modifications_detail', 'hvac_daily', 'energy_daily']):\n",
    "                            summary['missing_critical'].append(file_path)\n",
    "                    \n",
    "                    subcat_summary['files'].append(file_summary)\n",
    "                \n",
    "                subcat_summary['completeness'] = (subcat_summary['found'] / subcat_summary['expected'] * 100) if subcat_summary['expected'] > 0 else 0\n",
    "                cat_summary['subcategories'][subcat_key] = subcat_summary\n",
    "                cat_summary['total_expected'] += subcat_summary['expected']\n",
    "                cat_summary['total_found'] += subcat_summary['found']\n",
    "                \n",
    "                summary['overview']['total_files_expected'] += subcat_summary['expected']\n",
    "            \n",
    "            cat_summary['completeness'] = (cat_summary['total_found'] / cat_summary['total_expected'] * 100) if cat_summary['total_expected'] > 0 else 0\n",
    "            summary['by_category'][cat_key] = cat_summary\n",
    "            summary['overview']['total_categories'] += 1\n",
    "            summary['overview']['total_subcategories'] += len(category['subcategories'])\n",
    "        \n",
    "        summary['overview']['overall_completeness'] = (summary['overview']['total_files_found'] / summary['overview']['total_files_expected'] * 100) if summary['overview']['total_files_expected'] > 0 else 0\n",
    "        \n",
    "        return summary\n",
    "    \n",
    "    def print_visual_dashboard(self, summary: Dict):\n",
    "        \"\"\"Print a visual dashboard to the console.\"\"\"\n",
    "        print(\"\\n\" + \"=\"*100)\n",
    "        print(self.colored_text(\"SENSITIVITY DATA STRUCTURE DASHBOARD\", 'cyan', bold=True).center(100))\n",
    "        print(\"=\"*100)\n",
    "        \n",
    "        # Overview section\n",
    "        overview = summary['overview']\n",
    "        print(f\"\\n{self.colored_text('📊 OVERVIEW', 'blue', bold=True)}\")\n",
    "        print(f\"  Total Files Expected: {overview['total_files_expected']}\")\n",
    "        print(f\"  Total Files Found: {self.colored_text(str(overview['total_files_found']), 'green' if overview['total_files_found'] > 0 else 'red')}\")\n",
    "        print(f\"  Overall Completeness: {self._get_completeness_bar(overview['overall_completeness'])}\")\n",
    "        print(f\"  Total Data Size: {overview['total_size_mb']:.2f} MB\")\n",
    "        print(f\"  Total Data Rows: {overview['total_rows']:,}\")\n",
    "        \n",
    "        # Category breakdown\n",
    "        print(f\"\\n{self.colored_text('📁 CATEGORY BREAKDOWN', 'blue', bold=True)}\")\n",
    "        \n",
    "        for cat_key, cat_data in summary['by_category'].items():\n",
    "            print(f\"\\n  {self.colored_text(cat_data['name'], 'yellow', bold=True)}\")\n",
    "            print(f\"  {self._get_completeness_bar(cat_data['completeness'])}\")\n",
    "            print(f\"  Files: {cat_data['total_found']}/{cat_data['total_expected']}\")\n",
    "            \n",
    "            # Subcategories\n",
    "            for subcat_key, subcat_data in cat_data['subcategories'].items():\n",
    "                status_icon = \"✅\" if subcat_data['completeness'] == 100 else \"⚠️\" if subcat_data['completeness'] > 0 else \"❌\"\n",
    "                print(f\"\\n    {status_icon} {subcat_data['name']}\")\n",
    "                print(f\"       Files: {subcat_data['found']}/{subcat_data['expected']} | Size: {subcat_data['total_size_mb']:.2f} MB | Rows: {subcat_data['total_rows']:,}\")\n",
    "                \n",
    "                # Show missing files\n",
    "                for file_info in subcat_data['files']:\n",
    "                    if not file_info['exists']:\n",
    "                        print(f\"       {self.colored_text('❌ Missing:', 'red')} {file_info['path'].split('/')[-1]}\")\n",
    "        \n",
    "        # Critical missing files\n",
    "        if summary['missing_critical']:\n",
    "            print(f\"\\n{self.colored_text('🚨 CRITICAL MISSING FILES', 'red', bold=True)}\")\n",
    "            for file in summary['missing_critical']:\n",
    "                print(f\"  - {file}\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*100)\n",
    "    \n",
    "    def _get_completeness_bar(self, percentage: float, width: int = 20) -> str:\n",
    "        \"\"\"Create a visual progress bar.\"\"\"\n",
    "        filled = int(width * percentage / 100)\n",
    "        bar = \"█\" * filled + \"░\" * (width - filled)\n",
    "        \n",
    "        if percentage >= 80:\n",
    "            color = 'green'\n",
    "        elif percentage >= 50:\n",
    "            color = 'yellow'\n",
    "        else:\n",
    "            color = 'red'\n",
    "        \n",
    "        return f\"{self.colored_text(bar, color)} {percentage:.1f}%\"\n",
    "    \n",
    "    def create_detailed_column_report(self) -> pd.DataFrame:\n",
    "        \"\"\"Create a detailed report of all columns across all files.\"\"\"\n",
    "        column_data = []\n",
    "        \n",
    "        for cat_key, category in self.file_categories.items():\n",
    "            for subcat_key, subcategory in category['subcategories'].items():\n",
    "                for file_path in subcategory['files']:\n",
    "                    file_info = self.get_file_info(file_path)\n",
    "                    \n",
    "                    if file_info.get('exists') and file_info.get('type') == 'parquet':\n",
    "                        for col in file_info.get('columns', []):\n",
    "                            column_data.append({\n",
    "                                'Category': category['name'],\n",
    "                                'Subcategory': subcategory['name'],\n",
    "                                'File': file_path.split('/')[-1],\n",
    "                                'Column': col,\n",
    "                                'Type': file_info['dtypes'].get(col, 'unknown'),\n",
    "                                'Status': self._check_column_status(file_path, col)\n",
    "                            })\n",
    "        \n",
    "        return pd.DataFrame(column_data)\n",
    "    \n",
    "    def _check_column_status(self, file_path: str, column: str) -> str:\n",
    "        \"\"\"Check if a column is expected, extra, or missing.\"\"\"\n",
    "        # Simplified logic - would need full expected columns mapping\n",
    "        if 'sensitivity_score' in column or 'parameter' in column:\n",
    "            return 'Expected'\n",
    "        elif column in ['building_id', 'Variable', 'Value', 'DateTime', 'Zone']:\n",
    "            return 'Expected'\n",
    "        else:\n",
    "            return 'Extra'\n",
    "    \n",
    "    def create_html_report(self, summary: Dict, output_path: Path):\n",
    "        \"\"\"Create an interactive HTML report.\"\"\"\n",
    "        html_content = \"\"\"\n",
    "<!DOCTYPE html>\n",
    "<html>\n",
    "<head>\n",
    "    <title>Sensitivity Data Structure Report</title>\n",
    "    <style>\n",
    "        body {\n",
    "            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;\n",
    "            margin: 0;\n",
    "            padding: 20px;\n",
    "            background-color: #f5f5f5;\n",
    "        }\n",
    "        .container {\n",
    "            max-width: 1400px;\n",
    "            margin: 0 auto;\n",
    "            background-color: white;\n",
    "            padding: 30px;\n",
    "            box-shadow: 0 0 20px rgba(0,0,0,0.1);\n",
    "            border-radius: 10px;\n",
    "        }\n",
    "        h1 {\n",
    "            color: #2c3e50;\n",
    "            text-align: center;\n",
    "            margin-bottom: 30px;\n",
    "        }\n",
    "        .overview-grid {\n",
    "            display: grid;\n",
    "            grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));\n",
    "            gap: 20px;\n",
    "            margin-bottom: 40px;\n",
    "        }\n",
    "        .metric-card {\n",
    "            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);\n",
    "            color: white;\n",
    "            padding: 20px;\n",
    "            border-radius: 10px;\n",
    "            text-align: center;\n",
    "            box-shadow: 0 5px 15px rgba(0,0,0,0.1);\n",
    "        }\n",
    "        .metric-value {\n",
    "            font-size: 2.5em;\n",
    "            font-weight: bold;\n",
    "            margin: 10px 0;\n",
    "        }\n",
    "        .metric-label {\n",
    "            font-size: 0.9em;\n",
    "            opacity: 0.9;\n",
    "        }\n",
    "        .category-section {\n",
    "            margin-bottom: 30px;\n",
    "            background: #f8f9fa;\n",
    "            padding: 20px;\n",
    "            border-radius: 8px;\n",
    "            border-left: 4px solid #667eea;\n",
    "        }\n",
    "        .category-header {\n",
    "            font-size: 1.3em;\n",
    "            font-weight: bold;\n",
    "            color: #2c3e50;\n",
    "            margin-bottom: 15px;\n",
    "        }\n",
    "        .subcategory {\n",
    "            background: white;\n",
    "            padding: 15px;\n",
    "            margin: 10px 0;\n",
    "            border-radius: 5px;\n",
    "            box-shadow: 0 2px 5px rgba(0,0,0,0.05);\n",
    "        }\n",
    "        .progress-bar {\n",
    "            width: 100%;\n",
    "            height: 20px;\n",
    "            background: #e0e0e0;\n",
    "            border-radius: 10px;\n",
    "            overflow: hidden;\n",
    "            margin: 10px 0;\n",
    "        }\n",
    "        .progress-fill {\n",
    "            height: 100%;\n",
    "            background: linear-gradient(90deg, #4CAF50 0%, #45a049 100%);\n",
    "            transition: width 0.3s ease;\n",
    "        }\n",
    "        .file-status {\n",
    "            display: inline-block;\n",
    "            padding: 3px 8px;\n",
    "            border-radius: 3px;\n",
    "            font-size: 0.85em;\n",
    "            margin: 2px;\n",
    "        }\n",
    "        .status-found {\n",
    "            background: #4CAF50;\n",
    "            color: white;\n",
    "        }\n",
    "        .status-missing {\n",
    "            background: #f44336;\n",
    "            color: white;\n",
    "        }\n",
    "        .file-grid {\n",
    "            display: grid;\n",
    "            grid-template-columns: repeat(auto-fill, minmax(300px, 1fr));\n",
    "            gap: 10px;\n",
    "            margin-top: 10px;\n",
    "        }\n",
    "        .file-card {\n",
    "            background: #f5f5f5;\n",
    "            padding: 10px;\n",
    "            border-radius: 5px;\n",
    "            font-size: 0.9em;\n",
    "        }\n",
    "        .critical-missing {\n",
    "            background: #ffebee;\n",
    "            border-left: 4px solid #f44336;\n",
    "            padding: 20px;\n",
    "            margin: 20px 0;\n",
    "            border-radius: 5px;\n",
    "        }\n",
    "        .legend {\n",
    "            display: flex;\n",
    "            justify-content: center;\n",
    "            gap: 20px;\n",
    "            margin: 20px 0;\n",
    "            flex-wrap: wrap;\n",
    "        }\n",
    "        .legend-item {\n",
    "            display: flex;\n",
    "            align-items: center;\n",
    "            gap: 5px;\n",
    "        }\n",
    "        .legend-color {\n",
    "            width: 20px;\n",
    "            height: 20px;\n",
    "            border-radius: 3px;\n",
    "        }\n",
    "    </style>\n",
    "</head>\n",
    "<body>\n",
    "    <div class=\"container\">\n",
    "        <h1>🔍 Sensitivity Data Structure Analysis</h1>\n",
    "        <p style=\"text-align: center; color: #666;\">Generated: \"\"\" + datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\") + \"\"\"</p>\n",
    "        \n",
    "        <div class=\"overview-grid\">\n",
    "            <div class=\"metric-card\">\n",
    "                <div class=\"metric-label\">Total Files Expected</div>\n",
    "                <div class=\"metric-value\">\"\"\" + str(summary['overview']['total_files_expected']) + \"\"\"</div>\n",
    "            </div>\n",
    "            <div class=\"metric-card\" style=\"background: linear-gradient(135deg, #4CAF50 0%, #45a049 100%);\">\n",
    "                <div class=\"metric-label\">Files Found</div>\n",
    "                <div class=\"metric-value\">\"\"\" + str(summary['overview']['total_files_found']) + \"\"\"</div>\n",
    "            </div>\n",
    "            <div class=\"metric-card\" style=\"background: linear-gradient(135deg, #ff9800 0%, #ff5722 100%);\">\n",
    "                <div class=\"metric-label\">Overall Completeness</div>\n",
    "                <div class=\"metric-value\">\"\"\" + f\"{summary['overview']['overall_completeness']:.1f}%\" + \"\"\"</div>\n",
    "            </div>\n",
    "            <div class=\"metric-card\" style=\"background: linear-gradient(135deg, #2196F3 0%, #1976D2 100%);\">\n",
    "                <div class=\"metric-label\">Total Data Size</div>\n",
    "                <div class=\"metric-value\">\"\"\" + f\"{summary['overview']['total_size_mb']:.1f} MB\" + \"\"\"</div>\n",
    "            </div>\n",
    "        </div>\n",
    "        \n",
    "        <div class=\"legend\">\n",
    "            <div class=\"legend-item\">\n",
    "                <div class=\"legend-color\" style=\"background: #4CAF50;\"></div>\n",
    "                <span>Found</span>\n",
    "            </div>\n",
    "            <div class=\"legend-item\">\n",
    "                <div class=\"legend-color\" style=\"background: #f44336;\"></div>\n",
    "                <span>Missing</span>\n",
    "            </div>\n",
    "            <div class=\"legend-item\">\n",
    "                <div class=\"legend-color\" style=\"background: #ff9800;\"></div>\n",
    "                <span>Partial</span>\n",
    "            </div>\n",
    "        </div>\n",
    "\"\"\"\n",
    "        \n",
    "        # Add category sections\n",
    "        for cat_key, cat_data in summary['by_category'].items():\n",
    "            html_content += f\"\"\"\n",
    "        <div class=\"category-section\">\n",
    "            <div class=\"category-header\">{cat_data['name']}</div>\n",
    "            <div class=\"progress-bar\">\n",
    "                <div class=\"progress-fill\" style=\"width: {cat_data['completeness']}%;\"></div>\n",
    "            </div>\n",
    "            <p>Files: {cat_data['total_found']}/{cat_data['total_expected']} ({cat_data['completeness']:.1f}% complete)</p>\n",
    "\"\"\"\n",
    "            \n",
    "            for subcat_key, subcat_data in cat_data['subcategories'].items():\n",
    "                status_color = '#4CAF50' if subcat_data['completeness'] == 100 else '#ff9800' if subcat_data['completeness'] > 0 else '#f44336'\n",
    "                html_content += f\"\"\"\n",
    "            <div class=\"subcategory\">\n",
    "                <h4 style=\"color: {status_color};\">{subcat_data['name']}</h4>\n",
    "                <div class=\"progress-bar\">\n",
    "                    <div class=\"progress-fill\" style=\"width: {subcat_data['completeness']}%; background: {status_color};\"></div>\n",
    "                </div>\n",
    "                <p>Files: {subcat_data['found']}/{subcat_data['expected']} | Size: {subcat_data['total_size_mb']:.2f} MB | Rows: {subcat_data['total_rows']:,}</p>\n",
    "                <div class=\"file-grid\">\n",
    "\"\"\"\n",
    "                \n",
    "                for file_info in subcat_data['files']:\n",
    "                    status = 'found' if file_info['exists'] else 'missing'\n",
    "                    html_content += f\"\"\"\n",
    "                    <div class=\"file-card\">\n",
    "                        <span class=\"file-status status-{status}\">{'✓' if file_info['exists'] else '✗'}</span>\n",
    "                        <strong>{file_info['path'].split('/')[-1]}</strong>\n",
    "                        {f\"<br>Size: {file_info['size_mb']:.2f} MB | Rows: {file_info['rows']:,}\" if file_info['exists'] else \"\"}\n",
    "                    </div>\n",
    "\"\"\"\n",
    "                \n",
    "                html_content += \"\"\"\n",
    "                </div>\n",
    "            </div>\n",
    "\"\"\"\n",
    "            \n",
    "            html_content += \"\"\"\n",
    "        </div>\n",
    "\"\"\"\n",
    "        \n",
    "        # Add critical missing files section if any\n",
    "        if summary['missing_critical']:\n",
    "            html_content += \"\"\"\n",
    "        <div class=\"critical-missing\">\n",
    "            <h3 style=\"color: #f44336; margin-top: 0;\">🚨 Critical Missing Files</h3>\n",
    "            <ul>\n",
    "\"\"\"\n",
    "            for file in summary['missing_critical']:\n",
    "                html_content += f\"            <li>{file}</li>\\n\"\n",
    "            \n",
    "            html_content += \"\"\"\n",
    "            </ul>\n",
    "        </div>\n",
    "\"\"\"\n",
    "        \n",
    "        html_content += \"\"\"\n",
    "    </div>\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "        \n",
    "        with open(output_path, 'w') as f:\n",
    "            f.write(html_content)\n",
    "    \n",
    "    def create_comparison_matrix(self) -> pd.DataFrame:\n",
    "        \"\"\"Create a comparison matrix showing file availability across categories.\"\"\"\n",
    "        matrix_data = []\n",
    "        \n",
    "        for cat_key, category in self.file_categories.items():\n",
    "            for subcat_key, subcategory in category['subcategories'].items():\n",
    "                for file_path in subcategory['files']:\n",
    "                    file_info = self.get_file_info(file_path)\n",
    "                    \n",
    "                    matrix_data.append({\n",
    "                        'Category': category['name'],\n",
    "                        'Subcategory': subcategory['name'],\n",
    "                        'File': file_path.split('/')[-1],\n",
    "                        'Status': '✅' if file_info['exists'] else '❌',\n",
    "                        'Size (MB)': file_info.get('size_mb', 0) if file_info['exists'] else 'N/A',\n",
    "                        'Rows': f\"{file_info.get('rows', 0):,}\" if file_info.get('rows') else 'N/A',\n",
    "                        'Columns': file_info.get('column_count', 'N/A') if file_info.get('column_count') else 'N/A'\n",
    "                    })\n",
    "        \n",
    "        return pd.DataFrame(matrix_data)\n",
    "    \n",
    "    def save_enhanced_results(self, summary: Dict, output_dir: Path):\n",
    "        \"\"\"Save enhanced results with multiple views.\"\"\"\n",
    "        output_dir.mkdir(exist_ok=True)\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        \n",
    "        # 1. Save summary JSON\n",
    "        with open(output_dir / f'sensitivity_structure_summary_{timestamp}.json', 'w') as f:\n",
    "            json.dump(summary, f, indent=2, default=str)\n",
    "        \n",
    "        # 2. Create and save comparison matrix\n",
    "        matrix_df = self.create_comparison_matrix()\n",
    "        matrix_df.to_csv(output_dir / f'file_comparison_matrix_{timestamp}.csv', index=False)\n",
    "        matrix_df.to_excel(output_dir / f'file_comparison_matrix_{timestamp}.xlsx', index=False)\n",
    "        \n",
    "        # 3. Create and save detailed column report\n",
    "        column_df = self.create_detailed_column_report()\n",
    "        if not column_df.empty:\n",
    "            column_df.to_csv(output_dir / f'column_analysis_{timestamp}.csv', index=False)\n",
    "        \n",
    "        # 4. Create HTML report\n",
    "        self.create_html_report(summary, output_dir / f'visual_report_{timestamp}.html')\n",
    "        \n",
    "        # 5. Create missing files report\n",
    "        missing_files = []\n",
    "        for cat_key, cat_data in summary['by_category'].items():\n",
    "            for subcat_key, subcat_data in cat_data['subcategories'].items():\n",
    "                for file_info in subcat_data['files']:\n",
    "                    if not file_info['exists']:\n",
    "                        missing_files.append({\n",
    "                            'Category': cat_data['name'],\n",
    "                            'Subcategory': subcat_data['name'],\n",
    "                            'File': file_info['path'],\n",
    "                            'Priority': 'Critical' if any(critical in file_info['path'] for critical in ['modifications_detail', 'hvac_daily', 'energy_daily']) else 'Normal'\n",
    "                        })\n",
    "        \n",
    "        if missing_files:\n",
    "            pd.DataFrame(missing_files).to_csv(output_dir / f'missing_files_{timestamp}.csv', index=False)\n",
    "        \n",
    "        print(f\"\\n{self.colored_text('💾 Results saved to:', 'green', bold=True)} {output_dir}\")\n",
    "        print(f\"  - Visual HTML Report: visual_report_{timestamp}.html\")\n",
    "        print(f\"  - File Comparison Matrix: file_comparison_matrix_{timestamp}.csv\")\n",
    "        print(f\"  - Structure Summary: sensitivity_structure_summary_{timestamp}.json\")\n",
    "        if missing_files:\n",
    "            print(f\"  - Missing Files List: missing_files_{timestamp}.csv\")\n",
    "\n",
    "# Enhanced usage function\n",
    "def check_sensitivity_structure_enhanced(job_output_dir: str):\n",
    "    \"\"\"Enhanced sensitivity data structure checker with visual output.\"\"\"\n",
    "    checker = EnhancedSensitivityDataChecker(job_output_dir)\n",
    "    \n",
    "    # Create visual summary\n",
    "    summary = checker.create_visual_summary()\n",
    "    \n",
    "    # Print visual dashboard\n",
    "    checker.print_visual_dashboard(summary)\n",
    "    \n",
    "    # Detect configuration\n",
    "    print(f\"\\n{checker.colored_text('🔧 DETECTED CONFIGURATION', 'purple', bold=True)}\")\n",
    "    \n",
    "    # Analysis type detection\n",
    "    analysis_types = []\n",
    "    if any('modification' in str(cat) for cat in summary['by_category'].values()):\n",
    "        if summary['by_category']['INPUT_DATA']['subcategories']['MODIFICATIONS']['found'] > 0:\n",
    "            analysis_types.append('Modification-based')\n",
    "    if summary['by_category']['INPUT_DATA']['subcategories']['PARAMETERS']['found'] > 0:\n",
    "        analysis_types.append('Traditional')\n",
    "    \n",
    "    print(f\"  Analysis Types: {', '.join(analysis_types) if analysis_types else 'None detected'}\")\n",
    "    \n",
    "    # Feature detection\n",
    "    features = []\n",
    "    if summary['by_category']['INPUT_DATA']['subcategories']['RELATIONSHIPS']['found'] > 0:\n",
    "        features.append('Multi-level Analysis')\n",
    "    if any('time_slice' in str(f) for f in summary.get('extra_files', [])):\n",
    "        features.append('Time Slicing')\n",
    "    if summary['by_category']['OUTPUT_DATA']['subcategories']['ADVANCED_RESULTS']['found'] > 0:\n",
    "        features.append('Advanced Analysis')\n",
    "    \n",
    "    print(f\"  Features Enabled: {', '.join(features) if features else 'None'}\")\n",
    "    \n",
    "    # Save results\n",
    "    output_dir = Path(job_output_dir) / 'sensitivity_structure_check'\n",
    "    checker.save_enhanced_results(summary, output_dir)\n",
    "    \n",
    "    return summary\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Replace with your actual job directory\n",
    "    job_dir = r\"D:\\Documents\\daily\\E_Plus_2040_py\\output\\82e2b83f-5013-4270-8266-a37a67dbd4ff\"\n",
    "    summary = check_sensitivity_structure_enhanced(job_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "657294bd",
   "metadata": {},
   "source": [
    "# PArsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5c957696",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking parser data structure in: D:\\Documents\\daily\\E_Plus_2040_py\\output\\82e2b83f-5013-4270-8266-a37a67dbd4ff\\parsed_modified_results\n",
      "\n",
      "Checking: metadata/project_manifest.json\n",
      "Checking: metadata/building_registry.parquet\n",
      "Checking: metadata/category_schemas.json\n",
      "Checking: metadata/output_documentation.json\n",
      "Checking: idf_data/by_category/simulation_control.parquet\n",
      "Checking: idf_data/by_category/site_location.parquet\n",
      "Checking: idf_data/by_category/geometry_zones.parquet\n",
      "Checking: idf_data/by_category/geometry_surfaces.parquet\n",
      "Checking: idf_data/by_category/materials_constructions.parquet\n",
      "Checking: idf_data/by_category/materials_materials.parquet\n",
      "Checking: idf_data/by_category/hvac_equipment.parquet\n",
      "Checking: idf_data/by_category/hvac_thermostats.parquet\n",
      "Checking: idf_data/by_category/outputs_all.parquet\n",
      "Checking: idf_data/by_category/ventilation.parquet\n",
      "Checking: idf_data/by_category/infiltration.parquet\n",
      "Checking: idf_data/by_category/lighting.parquet\n",
      "Checking: idf_data/by_category/equipment.parquet\n",
      "Checking: idf_data/by_category/dhw.parquet\n",
      "Checking: idf_data/by_category/schedules.parquet\n",
      "Checking: sql_results/timeseries/hourly/*_2020.parquet\n",
      "Checking: sql_results/timeseries/aggregated/daily/*_daily.parquet\n",
      "Checking: sql_results/timeseries/aggregated/monthly/*_monthly.parquet\n",
      "Checking: sql_results/schedules/all_schedules.parquet\n",
      "Checking: sql_results/summary_metrics/building_metrics.parquet\n",
      "Checking: sql_results/summary_metrics/zone_metrics.parquet\n",
      "Checking: sql_results/output_validation/validation_results.parquet\n",
      "Checking: sql_results/output_validation/missing_outputs.parquet\n",
      "Checking: sql_results/output_validation/existing_outputs.parquet\n",
      "Checking: sql_results/output_validation/available_outputs.parquet\n",
      "Checking: relationships/zone_mappings.parquet\n",
      "Checking: relationships/equipment_assignments.parquet\n",
      "Checking: analysis_ready/parameter_matrix.parquet\n",
      "Checking: analysis_ready/output_analysis/coverage_summary.json\n",
      "\n",
      "================================================================================\n",
      "PARSER DATA STRUCTURE CHECK SUMMARY\n",
      "================================================================================\n",
      "Directory: D:\\Documents\\daily\\E_Plus_2040_py\\output\\82e2b83f-5013-4270-8266-a37a67dbd4ff\\parsed_modified_results\n",
      "Files found: 29/33\n",
      "Parser completeness: 100.0%\n",
      "\n",
      "Data Categories:\n",
      "  - metadata: ✓\n",
      "  - idf_data: ✓\n",
      "  - sql_results: ✓\n",
      "  - relationships: ✓\n",
      "  - analysis_ready: ✓\n",
      "  - output_validation: ✓\n",
      "\n",
      "Results saved to: D:\\Documents\\daily\\E_Plus_2040_py\\output\\82e2b83f-5013-4270-8266-a37a67dbd4ff\\parsed_modified_results\\parser_data_check\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Optional, Any\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "class ParserDataChecker:\n",
    "    def __init__(self, parsed_data_dir: str):\n",
    "        self.parsed_data_dir = Path(parsed_data_dir)\n",
    "        self.results = []\n",
    "        self.discovered_columns = {}\n",
    "        \n",
    "        # Expected structure based on parser implementation\n",
    "        self.expected_structure = {\n",
    "            # ===== METADATA FILES =====\n",
    "            'metadata/project_manifest.json': {\n",
    "                'type': 'json',\n",
    "                'fields': {\n",
    "                    'project_id': 'str',\n",
    "                    'created': 'str',\n",
    "                    'total_buildings': 'int',\n",
    "                    'categories_tracked': 'list',\n",
    "                    'last_updated': 'str',\n",
    "                    'data_version': 'str',\n",
    "                    'file_structure': 'dict'\n",
    "                }\n",
    "            },\n",
    "            'metadata/building_registry.parquet': {\n",
    "                'type': 'parquet',\n",
    "                'columns': {\n",
    "                    'building_id': 'str',\n",
    "                    'ogc_fid': ['str', 'int'],\n",
    "                    'idf_path': 'str',\n",
    "                    'sql_path': 'str',\n",
    "                    'zone_count': 'int',\n",
    "                    'output_variables': 'int',\n",
    "                    'output_meters': 'int',\n",
    "                    'status': 'str',\n",
    "                    'last_modified': 'datetime',\n",
    "                    'variant_id': 'str'\n",
    "                }\n",
    "            },\n",
    "            'metadata/category_schemas.json': {\n",
    "                'type': 'json',\n",
    "                'fields': {\n",
    "                    'columns': 'list',\n",
    "                    'dtypes': 'dict',\n",
    "                    'row_count': 'int',\n",
    "                    'building_count': 'int'\n",
    "                }\n",
    "            },\n",
    "            'metadata/output_documentation.json': {\n",
    "                'type': 'json',\n",
    "                'fields': {\n",
    "                    'project': 'str',\n",
    "                    'timestamp': 'str',\n",
    "                    'buildings': 'dict',\n",
    "                    'summary': 'dict',\n",
    "                    'output_categories': 'dict'\n",
    "                }\n",
    "            },\n",
    "            \n",
    "            # ===== IDF DATA BY CATEGORY =====\n",
    "            'idf_data/by_category/simulation_control.parquet': {\n",
    "                'type': 'parquet',\n",
    "                'columns': {\n",
    "                    'building_id': 'str',\n",
    "                    'object_type': 'str',\n",
    "                    'object_name': 'str',\n",
    "                    'variant_id': 'str',\n",
    "                    'do_zone_sizing_calculation': 'str',\n",
    "                    'do_system_sizing_calculation': 'str',\n",
    "                    'do_plant_sizing_calculation': 'str',\n",
    "                    'run_simulation_for_sizing_periods': 'str',\n",
    "                    'run_simulation_for_weather_file_run_periods': 'str'\n",
    "                }\n",
    "            },\n",
    "            'idf_data/by_category/site_location.parquet': {\n",
    "                'type': 'parquet',\n",
    "                'columns': {\n",
    "                    'building_id': 'str',\n",
    "                    'object_type': 'str',\n",
    "                    'object_name': 'str',\n",
    "                    'latitude': ['str', 'float'],\n",
    "                    'longitude': ['str', 'float'],\n",
    "                    'time_zone': ['str', 'float'],\n",
    "                    'elevation': ['str', 'float']\n",
    "                }\n",
    "            },\n",
    "            'idf_data/by_category/geometry_zones.parquet': {\n",
    "                'type': 'parquet',\n",
    "                'columns': {\n",
    "                    'building_id': 'str',\n",
    "                    'object_type': 'str',\n",
    "                    'object_name': 'str',\n",
    "                    'zone_name': 'str',\n",
    "                    'volume': ['str', 'float'],\n",
    "                    'floor_area': ['str', 'float'],\n",
    "                    'ceiling_height': ['str', 'float'],\n",
    "                    'multiplier': ['str', 'float'],\n",
    "                    'volume_numeric': 'float',\n",
    "                    'floor_area_numeric': 'float',\n",
    "                    'ceiling_height_numeric': 'float'\n",
    "                }\n",
    "            },\n",
    "            'idf_data/by_category/geometry_surfaces.parquet': {\n",
    "                'type': 'parquet',\n",
    "                'columns': {\n",
    "                    'building_id': 'str',\n",
    "                    'object_type': 'str',\n",
    "                    'object_name': 'str',\n",
    "                    'zone_name': 'str',\n",
    "                    'surface_type': 'str',\n",
    "                    'construction_name': 'str',\n",
    "                    'outside_boundary_condition': 'str',\n",
    "                    'sun_exposure': 'str',\n",
    "                    'wind_exposure': 'str',\n",
    "                    'number_of_vertices': ['str', 'float']\n",
    "                }\n",
    "            },\n",
    "            'idf_data/by_category/materials_constructions.parquet': {\n",
    "                'type': 'parquet',\n",
    "                'columns': {\n",
    "                    'building_id': 'str',\n",
    "                    'object_type': 'str',\n",
    "                    'object_name': 'str',\n",
    "                    'outside_layer': 'str',\n",
    "                    'layer_2': 'str',\n",
    "                    'layer_3': 'str',\n",
    "                    'layer_4': 'str',\n",
    "                    'layer_5': 'str'\n",
    "                }\n",
    "            },\n",
    "            'idf_data/by_category/materials_materials.parquet': {\n",
    "                'type': 'parquet',\n",
    "                'columns': {\n",
    "                    'building_id': 'str',\n",
    "                    'object_type': 'str',\n",
    "                    'object_name': 'str',\n",
    "                    'roughness': 'str',\n",
    "                    'thickness': ['str', 'float'],\n",
    "                    'conductivity': ['str', 'float'],\n",
    "                    'density': ['str', 'float'],\n",
    "                    'specific_heat': ['str', 'float'],\n",
    "                    'thickness_numeric': 'float',\n",
    "                    'conductivity_numeric': 'float',\n",
    "                    'density_numeric': 'float',\n",
    "                    'specific_heat_numeric': 'float'\n",
    "                }\n",
    "            },\n",
    "            'idf_data/by_category/hvac_equipment.parquet': {\n",
    "                'type': 'parquet',\n",
    "                'columns': {\n",
    "                    'building_id': 'str',\n",
    "                    'object_type': 'str',\n",
    "                    'object_name': 'str',\n",
    "                    'zone_name': 'str',\n",
    "                    'availability_schedule_name': 'str',\n",
    "                    'maximum_heating_supply_air_temperature': ['str', 'float'],\n",
    "                    'minimum_cooling_supply_air_temperature': ['str', 'float'],\n",
    "                    'maximum_heating_air_flow_rate': ['str', 'float'],\n",
    "                    'maximum_cooling_air_flow_rate': ['str', 'float']\n",
    "                }\n",
    "            },\n",
    "            'idf_data/by_category/hvac_thermostats.parquet': {\n",
    "                'type': 'parquet',\n",
    "                'columns': {\n",
    "                    'building_id': 'str',\n",
    "                    'object_type': 'str',\n",
    "                    'object_name': 'str',\n",
    "                    'heating_setpoint_temperature_schedule_name': 'str',\n",
    "                    'cooling_setpoint_temperature_schedule_name': 'str'\n",
    "                }\n",
    "            },\n",
    "            'idf_data/by_category/outputs_all.parquet': {\n",
    "                'type': 'parquet',\n",
    "                'columns': {\n",
    "                    'building_id': 'str',\n",
    "                    'output_type': 'str',\n",
    "                    'output_subtype': 'str',\n",
    "                    'key_value': 'str',\n",
    "                    'name': 'str',\n",
    "                    'reporting_frequency': 'str',\n",
    "                    'schedule_name': 'str'\n",
    "                }\n",
    "            },\n",
    "            'idf_data/by_category/ventilation.parquet': {\n",
    "                'type': 'parquet',\n",
    "                'columns': {\n",
    "                    'building_id': 'str',\n",
    "                    'object_type': 'str',\n",
    "                    'object_name': 'str',\n",
    "                    'zone_name': 'str',\n",
    "                    'design_flow_rate': ['str', 'float'],\n",
    "                    'flow_rate_per_zone_floor_area': ['str', 'float'],\n",
    "                    'air_changes_per_hour': ['str', 'float'],\n",
    "                    'schedule_name': 'str',\n",
    "                    'design_flow_rate_numeric': 'float'\n",
    "                }\n",
    "            },\n",
    "            'idf_data/by_category/infiltration.parquet': {\n",
    "                'type': 'parquet',\n",
    "                'columns': {\n",
    "                    'building_id': 'str',\n",
    "                    'object_type': 'str',\n",
    "                    'object_name': 'str',\n",
    "                    'zone_name': 'str',\n",
    "                    'design_flow_rate': ['str', 'float'],\n",
    "                    'air_changes_per_hour': ['str', 'float'],\n",
    "                    'constant_term_coefficient': ['str', 'float'],\n",
    "                    'temperature_term_coefficient': ['str', 'float'],\n",
    "                    'design_flow_rate_numeric': 'float'\n",
    "                }\n",
    "            },\n",
    "            'idf_data/by_category/lighting.parquet': {\n",
    "                'type': 'parquet',\n",
    "                'columns': {\n",
    "                    'building_id': 'str',\n",
    "                    'object_type': 'str',\n",
    "                    'object_name': 'str',\n",
    "                    'zone_name': 'str',\n",
    "                    'lighting_level': ['str', 'float'],\n",
    "                    'watts_per_zone_floor_area': ['str', 'float'],\n",
    "                    'schedule_name': 'str',\n",
    "                    'fraction_radiant': ['str', 'float'],\n",
    "                    'fraction_visible': ['str', 'float'],\n",
    "                    'lighting_level_numeric': 'float',\n",
    "                    'watts_per_zone_floor_area_numeric': 'float'\n",
    "                }\n",
    "            },\n",
    "            'idf_data/by_category/equipment.parquet': {\n",
    "                'type': 'parquet',\n",
    "                'columns': {\n",
    "                    'building_id': 'str',\n",
    "                    'object_type': 'str',\n",
    "                    'object_name': 'str',\n",
    "                    'zone_name': 'str',\n",
    "                    'design_level': ['str', 'float'],\n",
    "                    'watts_per_zone_floor_area': ['str', 'float'],\n",
    "                    'schedule_name': 'str',\n",
    "                    'fraction_latent': ['str', 'float'],\n",
    "                    'fraction_radiant': ['str', 'float'],\n",
    "                    'design_level_numeric': 'float'\n",
    "                }\n",
    "            },\n",
    "            'idf_data/by_category/dhw.parquet': {\n",
    "                'type': 'parquet',\n",
    "                'columns': {\n",
    "                    'building_id': 'str',\n",
    "                    'object_type': 'str',\n",
    "                    'object_name': 'str',\n",
    "                    'tank_volume': ['str', 'float'],\n",
    "                    'heater_maximum_capacity': ['str', 'float'],\n",
    "                    'setpoint_temperature_schedule_name': 'str',\n",
    "                    'heater_thermal_efficiency': ['str', 'float'],\n",
    "                    'heater_fuel_type': 'str',\n",
    "                    'tank_volume_numeric': 'float',\n",
    "                    'heater_maximum_capacity_numeric': 'float'\n",
    "                }\n",
    "            },\n",
    "            'idf_data/by_category/schedules.parquet': {\n",
    "                'type': 'parquet',\n",
    "                'columns': {\n",
    "                    'building_id': 'str',\n",
    "                    'object_type': 'str',\n",
    "                    'object_name': 'str',\n",
    "                    'schedule_type_limits_name': 'str'\n",
    "                }\n",
    "            },\n",
    "            \n",
    "            # ===== SQL RESULTS =====\n",
    "            'sql_results/timeseries/hourly/*_2020.parquet': {\n",
    "                'type': 'parquet',\n",
    "                'columns': {\n",
    "                    'building_id': 'str',\n",
    "                    'variant_id': 'str',\n",
    "                    'DateTime': 'datetime',\n",
    "                    'Zone': 'str',\n",
    "                    'Variable': 'str',\n",
    "                    'Value': 'float',\n",
    "                    'Units': 'str',\n",
    "                    'ReportingFrequency': 'str'\n",
    "                }\n",
    "            },\n",
    "            'sql_results/timeseries/aggregated/daily/*_daily.parquet': {\n",
    "                'type': 'parquet',\n",
    "                'columns': {\n",
    "                    'building_id': 'str',\n",
    "                    'variant_id': 'str',\n",
    "                    'DateTime': 'datetime',\n",
    "                    'Zone': 'str',\n",
    "                    'Variable': 'str',\n",
    "                    'Value': 'float'\n",
    "                }\n",
    "            },\n",
    "            'sql_results/timeseries/aggregated/monthly/*_monthly.parquet': {\n",
    "                'type': 'parquet',\n",
    "                'columns': {\n",
    "                    'building_id': 'str',\n",
    "                    'variant_id': 'str',\n",
    "                    'DateTime': 'datetime',\n",
    "                    'Zone': 'str',\n",
    "                    'Variable': 'str',\n",
    "                    'Value': 'float'\n",
    "                }\n",
    "            },\n",
    "            'sql_results/schedules/all_schedules.parquet': {\n",
    "                'type': 'parquet',\n",
    "                'columns': {\n",
    "                    'building_id': 'str',\n",
    "                    'variant_id': 'str',\n",
    "                    'ScheduleIndex': 'int',\n",
    "                    'ScheduleName': 'str',\n",
    "                    'ScheduleType': 'str',\n",
    "                    'ScheduleMinimum': 'float',\n",
    "                    'ScheduleMaximum': 'float'\n",
    "                }\n",
    "            },\n",
    "            'sql_results/summary_metrics/building_metrics.parquet': {\n",
    "                'type': 'parquet',\n",
    "                'columns': {\n",
    "                    'building_id': 'str',\n",
    "                    'variant_id': 'str',\n",
    "                    'sql_file': 'str',\n",
    "                    'extraction_date': 'str',\n",
    "                    'zone_count': 'int',\n",
    "                    'total_floor_area': 'float',\n",
    "                    'total_volume': 'float',\n",
    "                    'energyplus_version': 'str',\n",
    "                    'simulation_timestamp': 'str',\n",
    "                    'timesteps_per_hour': 'int',\n",
    "                    'total_output_variables': 'int',\n",
    "                    'outputs_with_data': 'int',\n",
    "                    'output_coverage_percent': 'float'\n",
    "                }\n",
    "            },\n",
    "            'sql_results/summary_metrics/zone_metrics.parquet': {\n",
    "                'type': 'parquet',\n",
    "                'columns': {\n",
    "                    'building_id': 'str',\n",
    "                    'variant_id': 'str',\n",
    "                    'zone_name': 'str',\n",
    "                    'floor_area': 'float',\n",
    "                    'volume': 'float'\n",
    "                }\n",
    "            },\n",
    "            'sql_results/output_validation/validation_results.parquet': {\n",
    "                'type': 'parquet',\n",
    "                'columns': {\n",
    "                    'building_id': 'str',\n",
    "                    'variant_id': 'str',\n",
    "                    'total_requested': 'int',\n",
    "                    'found': 'int',\n",
    "                    'coverage': 'float'\n",
    "                }\n",
    "            },\n",
    "            'sql_results/output_validation/missing_outputs.parquet': {\n",
    "                'type': 'parquet',\n",
    "                'columns': {\n",
    "                    'building_id': 'str',\n",
    "                    'variant_id': 'str',\n",
    "                    'variable': 'str',\n",
    "                    'key': 'str',\n",
    "                    'frequency': 'str'\n",
    "                }\n",
    "            },\n",
    "            'sql_results/output_validation/existing_outputs.parquet': {\n",
    "                'type': 'parquet',\n",
    "                'columns': {\n",
    "                    'building_id': 'str',\n",
    "                    'variant_id': 'str',\n",
    "                    'variable': 'str',\n",
    "                    'key': 'str',\n",
    "                    'frequency': 'str',\n",
    "                    'found_in_sql': 'bool',\n",
    "                    'has_data': 'bool'\n",
    "                }\n",
    "            },\n",
    "            'sql_results/output_validation/available_outputs.parquet': {\n",
    "                'type': 'parquet',\n",
    "                'columns': {\n",
    "                    'building_id': 'str',\n",
    "                    'variant_id': 'str',\n",
    "                    'ReportDataDictionaryIndex': 'int',\n",
    "                    'VariableName': 'str',\n",
    "                    'KeyValue': 'str',\n",
    "                    'Units': 'str',\n",
    "                    'ReportingFrequency': 'str',\n",
    "                    'DataPoints': 'int',\n",
    "                    'HasData': 'bool',\n",
    "                    'Category': 'str'\n",
    "                }\n",
    "            },\n",
    "            \n",
    "            # ===== RELATIONSHIPS =====\n",
    "            'relationships/zone_mappings.parquet': {\n",
    "                'type': 'parquet',\n",
    "                'columns': {\n",
    "                    'building_id': 'str',\n",
    "                    'idf_zone_name': 'str',\n",
    "                    'sql_zone_name': 'str',\n",
    "                    'zone_type': 'str',\n",
    "                    'multiplier': 'int',\n",
    "                    'mapping_confidence': 'float'\n",
    "                }\n",
    "            },\n",
    "            'relationships/equipment_assignments.parquet': {\n",
    "                'type': 'parquet',\n",
    "                'columns': {\n",
    "                    'building_id': 'str',\n",
    "                    'equipment_name': 'str',\n",
    "                    'equipment_type': 'str',\n",
    "                    'assigned_zone': 'str',\n",
    "                    'schedule': 'str'\n",
    "                }\n",
    "            },\n",
    "            \n",
    "            # ===== ANALYSIS READY =====\n",
    "            'analysis_ready/parameter_matrix.parquet': {\n",
    "                'type': 'parquet',\n",
    "                'columns': {\n",
    "                    'building_id': 'str',\n",
    "                    # Dynamic columns based on categories\n",
    "                    # Examples:\n",
    "                    'geometry_volume': 'float',\n",
    "                    'geometry_floor_area': 'float',\n",
    "                    'geometry_ceiling_height': 'float',\n",
    "                    'materials_thickness': 'float',\n",
    "                    'materials_conductivity': 'float',\n",
    "                    'hvac_cooling_capacity': 'float',\n",
    "                    'hvac_heating_capacity': 'float'\n",
    "                }\n",
    "            },\n",
    "            'analysis_ready/output_analysis/coverage_summary.json': {\n",
    "                'type': 'json',\n",
    "                'fields': {\n",
    "                    'total_buildings': 'int',\n",
    "                    'average_coverage': 'float',\n",
    "                    'min_coverage': 'float',\n",
    "                    'max_coverage': 'float',\n",
    "                    'perfect_coverage_count': 'int',\n",
    "                    'buildings_with_issues': 'int',\n",
    "                    'top_missing_outputs': 'dict',\n",
    "                    'top_existing_outputs': 'dict'\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def convert_to_native_types(self, obj: Any) -> Any:\n",
    "        \"\"\"Convert numpy/pandas types to native Python types.\"\"\"\n",
    "        if isinstance(obj, dict):\n",
    "            return {k: self.convert_to_native_types(v) for k, v in obj.items()}\n",
    "        elif isinstance(obj, list):\n",
    "            return [self.convert_to_native_types(item) for item in obj]\n",
    "        elif isinstance(obj, pd.Timestamp):\n",
    "            return obj.isoformat()\n",
    "        elif isinstance(obj, datetime):\n",
    "            return obj.isoformat()\n",
    "        elif isinstance(obj, np.integer):\n",
    "            return int(obj)\n",
    "        elif isinstance(obj, np.floating):\n",
    "            if np.isnan(obj):\n",
    "                return None\n",
    "            return float(obj)\n",
    "        elif isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        elif pd.isna(obj):\n",
    "            return None\n",
    "        else:\n",
    "            return obj\n",
    "    \n",
    "    def check_file_exists(self, file_path: str) -> Tuple[bool, Optional[str]]:\n",
    "        \"\"\"Check if file exists, handling wildcards.\"\"\"\n",
    "        full_path = self.parsed_data_dir / file_path\n",
    "        \n",
    "        if '*' in file_path:\n",
    "            parent = full_path.parent\n",
    "            pattern = full_path.name\n",
    "            if parent.exists():\n",
    "                matching_files = list(parent.glob(pattern))\n",
    "                if matching_files:\n",
    "                    return True, str(matching_files[0])\n",
    "            return False, None\n",
    "        \n",
    "        return full_path.exists(), str(full_path) if full_path.exists() else None\n",
    "    \n",
    "    def check_json_file(self, file_path: str, expected_fields: Dict[str, str]) -> Dict:\n",
    "        \"\"\"Check a JSON file's structure.\"\"\"\n",
    "        exists, abs_path = self.check_file_exists(file_path)\n",
    "        \n",
    "        result = {\n",
    "            'file': file_path,\n",
    "            'exists': exists,\n",
    "            'path': abs_path,\n",
    "            'fields': {},\n",
    "            'extra_fields': {}\n",
    "        }\n",
    "        \n",
    "        if exists and abs_path:\n",
    "            try:\n",
    "                with open(abs_path, 'r') as f:\n",
    "                    data = json.load(f)\n",
    "                \n",
    "                # For nested JSON, check top-level keys\n",
    "                if file_path == 'metadata/category_schemas.json':\n",
    "                    # This has category names as keys\n",
    "                    result['categories'] = list(data.keys())\n",
    "                    result['category_count'] = len(data)\n",
    "                    # Check first category structure\n",
    "                    if data:\n",
    "                        first_cat = next(iter(data.values()))\n",
    "                        for field, expected_type in expected_fields.items():\n",
    "                            if field in first_cat:\n",
    "                                result['fields'][field] = {\n",
    "                                    'exists': True,\n",
    "                                    'type': type(first_cat[field]).__name__\n",
    "                                }\n",
    "                            else:\n",
    "                                result['fields'][field] = {\n",
    "                                    'exists': False,\n",
    "                                    'type': None\n",
    "                                }\n",
    "                else:\n",
    "                    # Regular field checking\n",
    "                    for field, expected_type in expected_fields.items():\n",
    "                        if field in data:\n",
    "                            result['fields'][field] = {\n",
    "                                'exists': True,\n",
    "                                'type': type(data[field]).__name__,\n",
    "                                'value': self.convert_to_native_types(data[field]) if not isinstance(data[field], (dict, list)) else f\"{expected_type} with {len(data[field])} items\"\n",
    "                            }\n",
    "                        else:\n",
    "                            result['fields'][field] = {\n",
    "                                'exists': False,\n",
    "                                'type': None\n",
    "                            }\n",
    "                    \n",
    "                    # Check for extra fields\n",
    "                    for field in data:\n",
    "                        if field not in expected_fields:\n",
    "                            result['extra_fields'][field] = {\n",
    "                                'type': type(data[field]).__name__,\n",
    "                                'value': self.convert_to_native_types(data[field]) if not isinstance(data[field], (dict, list)) else f\"{type(data[field]).__name__} with {len(data[field])} items\"\n",
    "                            }\n",
    "                            \n",
    "            except Exception as e:\n",
    "                result['error'] = str(e)\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def check_parquet_file(self, file_path: str, expected_columns: Dict[str, any]) -> Dict:\n",
    "        \"\"\"Check a parquet file's structure.\"\"\"\n",
    "        exists, abs_path = self.check_file_exists(file_path)\n",
    "        \n",
    "        result = {\n",
    "            'file': file_path,\n",
    "            'exists': exists,\n",
    "            'path': abs_path,\n",
    "            'row_count': 0,\n",
    "            'columns': {},\n",
    "            'extra_columns': {}\n",
    "        }\n",
    "        \n",
    "        if exists and abs_path:\n",
    "            try:\n",
    "                df = pd.read_parquet(abs_path)\n",
    "                result['row_count'] = len(df)\n",
    "                \n",
    "                # Store discovered columns\n",
    "                if file_path not in self.discovered_columns:\n",
    "                    self.discovered_columns[file_path] = {}\n",
    "                \n",
    "                # Get actual columns\n",
    "                actual_columns = {col: self.get_dtype_string(df[col].dtype) for col in df.columns}\n",
    "                \n",
    "                # Check expected columns\n",
    "                for exp_col, exp_type in expected_columns.items():\n",
    "                    if exp_col in actual_columns:\n",
    "                        actual_type = actual_columns[exp_col]\n",
    "                        \n",
    "                        # Handle multiple expected types\n",
    "                        if isinstance(exp_type, list):\n",
    "                            type_match = actual_type in exp_type\n",
    "                        else:\n",
    "                            type_match = (actual_type == exp_type) or \\\n",
    "                                       (exp_type == 'str' and actual_type in ['object', 'str'])\n",
    "                        \n",
    "                        result['columns'][exp_col] = {\n",
    "                            'expected_type': exp_type,\n",
    "                            'actual_type': actual_type,\n",
    "                            'exists': True,\n",
    "                            'type_match': type_match,\n",
    "                            'null_count': int(df[exp_col].isnull().sum()),\n",
    "                            'unique_count': int(df[exp_col].nunique()),\n",
    "                            'sample_values': self.get_sample_values(df[exp_col])\n",
    "                        }\n",
    "                    else:\n",
    "                        result['columns'][exp_col] = {\n",
    "                            'expected_type': exp_type,\n",
    "                            'actual_type': None,\n",
    "                            'exists': False,\n",
    "                            'type_match': False\n",
    "                        }\n",
    "                \n",
    "                # Check for extra columns\n",
    "                for col in actual_columns:\n",
    "                    if col not in expected_columns:\n",
    "                        result['extra_columns'][col] = {\n",
    "                            'actual_type': actual_columns[col],\n",
    "                            'null_count': int(df[col].isnull().sum()),\n",
    "                            'unique_count': int(df[col].nunique()),\n",
    "                            'sample_values': self.get_sample_values(df[col], 3),\n",
    "                            'stats': self.get_column_stats(df[col])\n",
    "                        }\n",
    "                        self.discovered_columns[file_path][col] = result['extra_columns'][col]\n",
    "                        \n",
    "            except Exception as e:\n",
    "                result['error'] = str(e)\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def get_dtype_string(self, dtype) -> str:\n",
    "        \"\"\"Convert numpy/pandas dtype to string.\"\"\"\n",
    "        dtype_str = str(dtype)\n",
    "        \n",
    "        if 'int' in dtype_str:\n",
    "            return 'int'\n",
    "        elif 'float' in dtype_str:\n",
    "            return 'float'\n",
    "        elif 'object' in dtype_str or 'string' in dtype_str:\n",
    "            return 'str'\n",
    "        elif 'datetime' in dtype_str:\n",
    "            return 'datetime'\n",
    "        elif 'bool' in dtype_str:\n",
    "            return 'bool'\n",
    "        else:\n",
    "            return dtype_str\n",
    "    \n",
    "    def get_sample_values(self, series: pd.Series, n_samples: int = 5) -> List:\n",
    "        \"\"\"Get sample values from a series.\"\"\"\n",
    "        unique_vals = series.dropna().unique()\n",
    "        if len(unique_vals) <= n_samples:\n",
    "            return [self.convert_to_native_types(val) for val in unique_vals]\n",
    "        else:\n",
    "            samples = []\n",
    "            if pd.api.types.is_numeric_dtype(series):\n",
    "                samples.append(self.convert_to_native_types(series.min()))\n",
    "                samples.append(self.convert_to_native_types(series.max()))\n",
    "                remaining = n_samples - 2\n",
    "                if remaining > 0 and len(series) > 0:\n",
    "                    random_samples = series.dropna().sample(n=min(remaining, len(series))).tolist()\n",
    "                    samples.extend([self.convert_to_native_types(val) for val in random_samples[:remaining]])\n",
    "            else:\n",
    "                samples = [self.convert_to_native_types(val) for val in unique_vals[:n_samples]]\n",
    "            return samples\n",
    "    \n",
    "    def get_column_stats(self, series: pd.Series) -> Dict:\n",
    "        \"\"\"Get statistics for a column.\"\"\"\n",
    "        stats = {\n",
    "            'total_count': int(len(series)),\n",
    "            'non_null_count': int(series.count()),\n",
    "            'null_percentage': float((series.isnull().sum() / len(series) * 100) if len(series) > 0 else 0)\n",
    "        }\n",
    "        \n",
    "        if pd.api.types.is_numeric_dtype(series) and series.count() > 0:\n",
    "            stats.update({\n",
    "                'mean': self.convert_to_native_types(series.mean()),\n",
    "                'std': self.convert_to_native_types(series.std()),\n",
    "                'min': self.convert_to_native_types(series.min()),\n",
    "                'max': self.convert_to_native_types(series.max()),\n",
    "                'q25': self.convert_to_native_types(series.quantile(0.25)),\n",
    "                'q50': self.convert_to_native_types(series.quantile(0.50)),\n",
    "                'q75': self.convert_to_native_types(series.quantile(0.75))\n",
    "            })\n",
    "        elif pd.api.types.is_string_dtype(series) or series.dtype == 'object':\n",
    "            value_counts = series.value_counts()\n",
    "            stats['unique_values'] = int(len(value_counts))\n",
    "            if len(value_counts) > 0:\n",
    "                stats['most_common'] = {str(k): int(v) for k, v in value_counts.head(5).items()}\n",
    "        \n",
    "        return stats\n",
    "    \n",
    "    def check_all_files(self) -> pd.DataFrame:\n",
    "        \"\"\"Check all expected files.\"\"\"\n",
    "        print(f\"Checking parser data structure in: {self.parsed_data_dir}\\n\")\n",
    "        \n",
    "        all_results = []\n",
    "        \n",
    "        for file_path, expected_info in self.expected_structure.items():\n",
    "            print(f\"Checking: {file_path}\")\n",
    "            \n",
    "            if expected_info['type'] == 'json':\n",
    "                result = self.check_json_file(file_path, expected_info.get('fields', {}))\n",
    "                \n",
    "                if result['exists']:\n",
    "                    # Add JSON field results\n",
    "                    for field_name, field_info in result['fields'].items():\n",
    "                        all_results.append({\n",
    "                            'File': file_path.split('/')[-1],\n",
    "                            'Path': file_path,\n",
    "                            'File_Type': 'JSON',\n",
    "                            'File_Exists': '✓',\n",
    "                            'Rows': 'N/A',\n",
    "                            'Column': field_name,\n",
    "                            'Column_Type': 'Expected',\n",
    "                            'Expected_Type': expected_info['fields'][field_name],\n",
    "                            'Actual_Type': field_info['type'],\n",
    "                            'Column_Exists': '✓' if field_info['exists'] else '✗',\n",
    "                            'Type_Match': '✓' if field_info['exists'] else '✗',\n",
    "                            'Null_Count': 'N/A',\n",
    "                            'Unique_Values': 'N/A',\n",
    "                            'Sample_Values': str(field_info.get('value', 'N/A'))[:50],\n",
    "                            'Status': 'OK' if field_info['exists'] else 'ISSUE'\n",
    "                        })\n",
    "                    \n",
    "                    # Add extra fields\n",
    "                    for field_name, field_info in result.get('extra_fields', {}).items():\n",
    "                        all_results.append({\n",
    "                            'File': file_path.split('/')[-1],\n",
    "                            'Path': file_path,\n",
    "                            'File_Type': 'JSON',\n",
    "                            'File_Exists': '✓',\n",
    "                            'Rows': 'N/A',\n",
    "                            'Column': field_name,\n",
    "                            'Column_Type': 'DISCOVERED',\n",
    "                            'Expected_Type': 'N/A',\n",
    "                            'Actual_Type': field_info['type'],\n",
    "                            'Column_Exists': '✓',\n",
    "                            'Type_Match': 'N/A',\n",
    "                            'Null_Count': 'N/A',\n",
    "                            'Unique_Values': 'N/A',\n",
    "                            'Sample_Values': str(field_info.get('value', 'N/A'))[:50],\n",
    "                            'Status': 'EXTRA'\n",
    "                        })\n",
    "                else:\n",
    "                    all_results.append({\n",
    "                        'File': file_path.split('/')[-1],\n",
    "                        'Path': file_path,\n",
    "                        'File_Type': 'JSON',\n",
    "                        'File_Exists': '✗',\n",
    "                        'Rows': 0,\n",
    "                        'Column': 'N/A',\n",
    "                        'Column_Type': 'N/A',\n",
    "                        'Expected_Type': 'N/A',\n",
    "                        'Actual_Type': 'N/A',\n",
    "                        'Column_Exists': 'N/A',\n",
    "                        'Type_Match': 'N/A',\n",
    "                        'Null_Count': None,\n",
    "                        'Unique_Values': None,\n",
    "                        'Sample_Values': 'N/A',\n",
    "                        'Status': 'MISSING FILE'\n",
    "                    })\n",
    "                    \n",
    "            else:  # Parquet files\n",
    "                result = self.check_parquet_file(file_path, expected_info.get('columns', {}))\n",
    "                \n",
    "                if result['exists']:\n",
    "                    # Add expected columns\n",
    "                    for col_name, col_info in result['columns'].items():\n",
    "                        all_results.append({\n",
    "                            'File': file_path.split('/')[-1],\n",
    "                            'Path': file_path,\n",
    "                            'File_Type': 'Parquet',\n",
    "                            'File_Exists': '✓',\n",
    "                            'Rows': result['row_count'],\n",
    "                            'Column': col_name,\n",
    "                            'Column_Type': 'Expected',\n",
    "                            'Expected_Type': col_info['expected_type'],\n",
    "                            'Actual_Type': col_info.get('actual_type', 'N/A'),\n",
    "                            'Column_Exists': '✓' if col_info['exists'] else '✗',\n",
    "                            'Type_Match': '✓' if col_info.get('type_match', False) else '✗',\n",
    "                            'Null_Count': col_info.get('null_count', 'N/A'),\n",
    "                            'Unique_Values': col_info.get('unique_count', 'N/A'),\n",
    "                            'Sample_Values': str(col_info.get('sample_values', [])[:3]),\n",
    "                            'Status': 'OK' if col_info['exists'] and col_info.get('type_match', False) else 'ISSUE'\n",
    "                        })\n",
    "                    \n",
    "                    # Add extra columns\n",
    "                    for col_name, col_info in result['extra_columns'].items():\n",
    "                        all_results.append({\n",
    "                            'File': file_path.split('/')[-1],\n",
    "                            'Path': file_path,\n",
    "                            'File_Type': 'Parquet',\n",
    "                            'File_Exists': '✓',\n",
    "                            'Rows': result['row_count'],\n",
    "                            'Column': col_name,\n",
    "                            'Column_Type': 'DISCOVERED',\n",
    "                            'Expected_Type': 'N/A',\n",
    "                            'Actual_Type': col_info['actual_type'],\n",
    "                            'Column_Exists': '✓',\n",
    "                            'Type_Match': 'N/A',\n",
    "                            'Null_Count': col_info['null_count'],\n",
    "                            'Unique_Values': col_info['unique_count'],\n",
    "                            'Sample_Values': str(col_info['sample_values'][:3]),\n",
    "                            'Status': 'EXTRA'\n",
    "                        })\n",
    "                else:\n",
    "                    all_results.append({\n",
    "                        'File': file_path.split('/')[-1],\n",
    "                        'Path': file_path,\n",
    "                        'File_Type': 'Parquet',\n",
    "                        'File_Exists': '✗',\n",
    "                        'Rows': 0,\n",
    "                        'Column': 'N/A',\n",
    "                        'Column_Type': 'N/A',\n",
    "                        'Expected_Type': 'N/A',\n",
    "                        'Actual_Type': 'N/A',\n",
    "                        'Column_Exists': 'N/A',\n",
    "                        'Type_Match': 'N/A',\n",
    "                        'Null_Count': None,\n",
    "                        'Unique_Values': None,\n",
    "                        'Sample_Values': 'N/A',\n",
    "                        'Status': 'MISSING FILE'\n",
    "                    })\n",
    "        \n",
    "        return pd.DataFrame(all_results)\n",
    "    \n",
    "    def create_summary_report(self, df_results: pd.DataFrame) -> Dict:\n",
    "        \"\"\"Create summary report.\"\"\"\n",
    "        summary = {\n",
    "            'check_timestamp': datetime.now().isoformat(),\n",
    "            'parsed_data_dir': str(self.parsed_data_dir),\n",
    "            'total_files_expected': len(self.expected_structure),\n",
    "            'files_found': len(df_results[df_results['File_Exists'] == '✓']['Path'].unique()),\n",
    "            'files_missing': len(df_results[df_results['File_Exists'] == '✗']['Path'].unique()),\n",
    "            'total_expected_columns': len(df_results[df_results['Column_Type'] == 'Expected']),\n",
    "            'expected_columns_found': len(df_results[(df_results['Column_Type'] == 'Expected') & \n",
    "                                                    (df_results['Column_Exists'] == '✓')]),\n",
    "            'total_discovered_columns': len(df_results[df_results['Column_Type'] == 'DISCOVERED']),\n",
    "            'total_issues': len(df_results[df_results['Status'].isin(['ISSUE', 'MISSING FILE'])]),\n",
    "            'missing_files': list(df_results[df_results['File_Exists'] == '✗']['Path'].unique()),\n",
    "            'data_categories': {\n",
    "                'metadata': False,\n",
    "                'idf_data': False,\n",
    "                'sql_results': False,\n",
    "                'relationships': False,\n",
    "                'analysis_ready': False,\n",
    "                'output_validation': False\n",
    "            },\n",
    "            'parser_completeness': 0.0\n",
    "        }\n",
    "        \n",
    "        # Check data categories\n",
    "        files_exist = df_results[df_results['File_Exists'] == '✓']['Path'].unique()\n",
    "        summary['data_categories']['metadata'] = any('metadata/' in f for f in files_exist)\n",
    "        summary['data_categories']['idf_data'] = any('idf_data/' in f for f in files_exist)\n",
    "        summary['data_categories']['sql_results'] = any('sql_results/' in f for f in files_exist)\n",
    "        summary['data_categories']['relationships'] = any('relationships/' in f for f in files_exist)\n",
    "        summary['data_categories']['analysis_ready'] = any('analysis_ready/' in f for f in files_exist)\n",
    "        summary['data_categories']['output_validation'] = any('output_validation/' in f for f in files_exist)\n",
    "        \n",
    "        # Calculate completeness\n",
    "        categories_complete = sum(summary['data_categories'].values())\n",
    "        summary['parser_completeness'] = (categories_complete / len(summary['data_categories'])) * 100\n",
    "        \n",
    "        return summary\n",
    "    \n",
    "    def save_results(self, df_results: pd.DataFrame, summary: Dict):\n",
    "        \"\"\"Save results to files.\"\"\"\n",
    "        output_dir = self.parsed_data_dir / 'parser_data_check'\n",
    "        output_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        \n",
    "        # Save detailed results\n",
    "        df_results.to_csv(output_dir / f'parser_check_details_{timestamp}.csv', index=False)\n",
    "        df_results.to_excel(output_dir / f'parser_check_details_{timestamp}.xlsx', index=False)\n",
    "        \n",
    "        # Save summary\n",
    "        with open(output_dir / f'parser_check_summary_{timestamp}.json', 'w') as f:\n",
    "            json.dump(summary, f, indent=2)\n",
    "        \n",
    "        # Save discovered columns\n",
    "        if self.discovered_columns:\n",
    "            discovered_native = self.convert_to_native_types(self.discovered_columns)\n",
    "            with open(output_dir / f'discovered_columns_{timestamp}.json', 'w') as f:\n",
    "                json.dump(discovered_native, f, indent=2)\n",
    "        \n",
    "        # Create markdown report\n",
    "        self.create_markdown_report(output_dir / f'parser_report_{timestamp}.md', df_results, summary)\n",
    "        \n",
    "        print(f\"\\nResults saved to: {output_dir}\")\n",
    "\n",
    "    def create_markdown_report(self, output_path: Path, df_results: pd.DataFrame, summary: Dict):\n",
    "        \"\"\"Create markdown report.\"\"\"\n",
    "        with open(output_path, 'w') as f:\n",
    "            f.write(\"# Parser Data Structure Check Report\\n\\n\")\n",
    "            f.write(f\"**Generated:** {summary['check_timestamp']}\\n\")\n",
    "            f.write(f\"**Directory:** `{summary['parsed_data_dir']}`\\n\\n\")\n",
    "            \n",
    "            f.write(\"## Summary\\n\\n\")\n",
    "            f.write(f\"- **Files Expected:** {summary['total_files_expected']}\\n\")\n",
    "            f.write(f\"- **Files Found:** {summary['files_found']}\\n\")\n",
    "            f.write(f\"- **Files Missing:** {summary['files_missing']}\\n\")\n",
    "            f.write(f\"- **Parser Completeness:** {summary['parser_completeness']:.1f}%\\n\\n\")\n",
    "            \n",
    "            f.write(\"## Data Categories Status\\n\\n\")\n",
    "            for category, status in summary['data_categories'].items():\n",
    "                f.write(f\"- **{category}:** {'✓' if status else '✗'}\\n\")\n",
    "            f.write(\"\\n\")\n",
    "            \n",
    "            if summary['missing_files']:\n",
    "                f.write(\"## Missing Files\\n\\n\")\n",
    "                for file in summary['missing_files']:\n",
    "                    f.write(f\"- `{file}`\\n\")\n",
    "                f.write(\"\\n\")\n",
    "\n",
    "# Main function\n",
    "def check_parser_data_structure(parsed_data_dir: str):\n",
    "    \"\"\"Check parser data structure.\"\"\"\n",
    "    checker = ParserDataChecker(parsed_data_dir)\n",
    "    \n",
    "    # Run checks\n",
    "    df_results = checker.check_all_files()\n",
    "    summary = checker.create_summary_report(df_results)\n",
    "    \n",
    "    # Print summary\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"PARSER DATA STRUCTURE CHECK SUMMARY\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Directory: {parsed_data_dir}\")\n",
    "    print(f\"Files found: {summary['files_found']}/{summary['total_files_expected']}\")\n",
    "    print(f\"Parser completeness: {summary['parser_completeness']:.1f}%\")\n",
    "    print(\"\\nData Categories:\")\n",
    "    for category, status in summary['data_categories'].items():\n",
    "        print(f\"  - {category}: {'✓' if status else '✗'}\")\n",
    "    \n",
    "    # Save results\n",
    "    checker.save_results(df_results, summary)\n",
    "    \n",
    "    return df_results, summary\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Example path - update with your actual parsed data directory\n",
    "    parsed_dir = r\"D:\\Documents\\daily\\E_Plus_2040_py\\output\\82e2b83f-5013-4270-8266-a37a67dbd4ff\\parsed_modified_results\"\n",
    "    df_results, summary = check_parser_data_structure(parsed_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd34fe8e",
   "metadata": {},
   "source": [
    "## v2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b4024a5",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a4ed334c",
   "metadata": {},
   "source": [
    "## v3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da2e319b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a2e4a04c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Checking both base and modified parsed data...\n",
      "\n",
      "====================================================================================================\n",
      "\u001b[1mPARSER DATA STRUCTURE CHECK - VISUAL SUMMARY\u001b[0m\n",
      "====================================================================================================\n",
      "\n",
      "\u001b[1mBASE PARSED DATA\u001b[0m\n",
      "Path: D:\\Documents\\daily\\E_Plus_2040_py\\output\\82e2b83f-5013-4270-8266-a37a67dbd4ff\\parsed_data\n",
      "\n",
      "Overall: 10/11 files found\n",
      "[\u001b[92m█████████████████████████████████████████████\u001b[0m\u001b[91m▓▓▓▓\u001b[0m\u001b[93m░\u001b[0m]\n",
      "Legend: \u001b[92m█ Found\u001b[0m | \u001b[91m▓ Critical Missing\u001b[0m | \u001b[93m░ Optional Missing\u001b[0m\n",
      "\n",
      "Category Status:\n",
      "  \u001b[92m✓\u001b[0m metadata             3/3 files\n",
      "  \u001b[92m✓\u001b[0m idf_data             4/4 files\n",
      "  \u001b[91m✗\u001b[0m sql_results          1/2 files\n",
      "      \u001b[91m↳ Missing: *_2020.parquet\u001b[0m\n",
      "  \u001b[92m✓\u001b[0m relationships        1/1 files\n",
      "  \u001b[92m✓\u001b[0m output_validation    1/1 files\n",
      "\n",
      "\u001b[95mExtra files found: 35\u001b[0m\n",
      "  + parsing_summary.json\n",
      "  + analysis_ready/feature_sets/extraction_statistics.parquet\n",
      "  + analysis_ready/feature_sets/missing_variables_detail.parquet\n",
      "  + analysis_ready/output_analysis/coverage_summary.json\n",
      "  + idf_data/by_building/4136733_snapshot.parquet\n",
      "  ... and 30 more\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "\u001b[1mMODIFIED PARSED DATA\u001b[0m\n",
      "Path: D:\\Documents\\daily\\E_Plus_2040_py\\output\\82e2b83f-5013-4270-8266-a37a67dbd4ff\\parsed_modified_results\n",
      "\n",
      "Overall: 10/11 files found\n",
      "[\u001b[92m█████████████████████████████████████████████\u001b[0m\u001b[91m▓▓▓▓\u001b[0m\u001b[93m░\u001b[0m]\n",
      "Legend: \u001b[92m█ Found\u001b[0m | \u001b[91m▓ Critical Missing\u001b[0m | \u001b[93m░ Optional Missing\u001b[0m\n",
      "\n",
      "Category Status:\n",
      "  \u001b[92m✓\u001b[0m metadata             3/3 files\n",
      "  \u001b[92m✓\u001b[0m idf_data             4/4 files\n",
      "  \u001b[91m✗\u001b[0m sql_results          1/2 files\n",
      "      \u001b[91m↳ Missing: *_2020.parquet\u001b[0m\n",
      "  \u001b[92m✓\u001b[0m relationships        1/1 files\n",
      "  \u001b[92m✓\u001b[0m output_validation    1/1 files\n",
      "\n",
      "\u001b[95mExtra files found: 33\u001b[0m\n",
      "  + parsing_summary.json\n",
      "  + analysis_ready/feature_sets/extraction_statistics.parquet\n",
      "  + analysis_ready/feature_sets/missing_variables_detail.parquet\n",
      "  + analysis_ready/output_analysis/coverage_summary.json\n",
      "  + idf_data/by_building/4136733_snapshot.parquet\n",
      "  ... and 28 more\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "\u001b[1mCOMPARISON SUMMARY\u001b[0m\n",
      "--------------------------------------------------\n",
      "\n",
      "\u001b[96mSQL_RESULTS\u001b[0m\n",
      "  \u001b[91mBoth missing:\u001b[0m 1 files\n",
      "\n",
      "\u001b[1mVARIANT TRACKING ANALYSIS\u001b[0m\n",
      "--------------------------------------------------\n",
      "Unique variants found: 20\n",
      "Files with variant tracking: 7\n",
      "\n",
      "Buildings with variants: 1\n",
      "  4136733: variant_14, variant_18, variant_10, variant_13, variant_12, variant_2, variant_7, variant_1, variant_3, variant_15, variant_11, variant_6, variant_0, variant_8, variant_17, base, variant_16, variant_4, variant_5, variant_9\n",
      "\n",
      "\u001b[92mResults saved to: D:\\Documents\\daily\\E_Plus_2040_py\\output\\82e2b83f-5013-4270-8266-a37a67dbd4ff\\parsed_data\\data_structure_check\u001b[0m\n",
      "\n",
      "====================================================================================================\n",
      "CRITICAL ISSUES SUMMARY\n",
      "====================================================================================================\n",
      " Dataset    Category           File             Description\n",
      "    BASE sql_results *_2020.parquet Hourly time series data\n",
      "MODIFIED sql_results *_2020.parquet Hourly time series data\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Optional, Any, Set\n",
    "import json\n",
    "from datetime import datetime\n",
    "from collections import defaultdict\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class EnhancedParserDataChecker:\n",
    "    def __init__(self, base_parsed_dir: str, modified_parsed_dir: str = None):\n",
    "        self.base_parsed_dir = Path(base_parsed_dir)\n",
    "        self.modified_parsed_dir = Path(modified_parsed_dir) if modified_parsed_dir else None\n",
    "        self.results = {}\n",
    "        self.discovered_columns = {}\n",
    "        \n",
    "        # Color codes for terminal output\n",
    "        self.COLORS = {\n",
    "            'GREEN': '\\033[92m',\n",
    "            'RED': '\\033[91m',\n",
    "            'YELLOW': '\\033[93m',\n",
    "            'BLUE': '\\033[94m',\n",
    "            'PURPLE': '\\033[95m',\n",
    "            'CYAN': '\\033[96m',\n",
    "            'BOLD': '\\033[1m',\n",
    "            'END': '\\033[0m'\n",
    "        }\n",
    "        \n",
    "        # Expected structure (same as before but reorganized)\n",
    "        self.expected_structure = self._get_expected_structure()\n",
    "        \n",
    "    def _get_expected_structure(self) -> Dict:\n",
    "        \"\"\"Get expected file structure organized by category.\"\"\"\n",
    "        return {\n",
    "            'metadata': {\n",
    "                'files': {\n",
    "                    'metadata/project_manifest.json': {\n",
    "                        'type': 'json',\n",
    "                        'description': 'Project overview and configuration',\n",
    "                        'critical': True,\n",
    "                        'fields': {\n",
    "                            'project_id': 'str',\n",
    "                            'created': 'str',\n",
    "                            'total_buildings': 'int',\n",
    "                            'categories_tracked': 'list',\n",
    "                            'data_version': 'str'\n",
    "                        }\n",
    "                    },\n",
    "                    'metadata/building_registry.parquet': {\n",
    "                        'type': 'parquet',\n",
    "                        'description': 'Central registry of all buildings',\n",
    "                        'critical': True,\n",
    "                        'columns': {\n",
    "                            'building_id': 'str',\n",
    "                            'ogc_fid': ['str', 'int'],\n",
    "                            'idf_path': 'str',\n",
    "                            'sql_path': 'str',\n",
    "                            'zone_count': 'int',\n",
    "                            'variant_id': 'str'\n",
    "                        }\n",
    "                    },\n",
    "                    'metadata/category_schemas.json': {\n",
    "                        'type': 'json',\n",
    "                        'description': 'Schema definitions for each category',\n",
    "                        'critical': False,\n",
    "                        'fields': {\n",
    "                            'columns': 'list',\n",
    "                            'dtypes': 'dict',\n",
    "                            'row_count': 'int',\n",
    "                            'building_count': 'int'\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            'idf_data': {\n",
    "                'files': {\n",
    "                    'idf_data/by_category/geometry_zones.parquet': {\n",
    "                        'type': 'parquet',\n",
    "                        'description': 'Zone geometry and properties',\n",
    "                        'critical': True,\n",
    "                        'columns': {\n",
    "                            'building_id': 'str',\n",
    "                            'object_name': 'str',\n",
    "                            'zone_name': 'str',\n",
    "                            'volume': ['str', 'float'],\n",
    "                            'floor_area': ['str', 'float'],\n",
    "                            'ceiling_height': ['str', 'float']\n",
    "                        }\n",
    "                    },\n",
    "                    'idf_data/by_category/hvac_equipment.parquet': {\n",
    "                        'type': 'parquet',\n",
    "                        'description': 'HVAC equipment specifications',\n",
    "                        'critical': True,\n",
    "                        'columns': {\n",
    "                            'building_id': 'str',\n",
    "                            'object_name': 'str',\n",
    "                            'zone_name': 'str',\n",
    "                            'maximum_heating_supply_air_temperature': ['str', 'float'],\n",
    "                            'minimum_cooling_supply_air_temperature': ['str', 'float']\n",
    "                        }\n",
    "                    },\n",
    "                    'idf_data/by_category/lighting.parquet': {\n",
    "                        'type': 'parquet',\n",
    "                        'description': 'Lighting loads and schedules',\n",
    "                        'critical': True,\n",
    "                        'columns': {\n",
    "                            'building_id': 'str',\n",
    "                            'zone_name': 'str',\n",
    "                            'watts_per_zone_floor_area': ['str', 'float'],\n",
    "                            'fraction_radiant': ['str', 'float']\n",
    "                        }\n",
    "                    },\n",
    "                    'idf_data/by_category/outputs_all.parquet': {\n",
    "                        'type': 'parquet',\n",
    "                        'description': 'Consolidated output definitions',\n",
    "                        'critical': True,\n",
    "                        'columns': {\n",
    "                            'building_id': 'str',\n",
    "                            'output_type': 'str',\n",
    "                            'name': 'str',\n",
    "                            'reporting_frequency': 'str'\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            'sql_results': {\n",
    "                'files': {\n",
    "                    'sql_results/timeseries/hourly/*_2020.parquet': {\n",
    "                        'type': 'parquet',\n",
    "                        'description': 'Hourly time series data',\n",
    "                        'critical': True,\n",
    "                        'columns': {\n",
    "                            'building_id': 'str',\n",
    "                            'DateTime': 'datetime',\n",
    "                            'Zone': 'str',\n",
    "                            'Variable': 'str',\n",
    "                            'Value': 'float'\n",
    "                        }\n",
    "                    },\n",
    "                    'sql_results/summary_metrics/building_metrics.parquet': {\n",
    "                        'type': 'parquet',\n",
    "                        'description': 'Building-level summary metrics',\n",
    "                        'critical': True,\n",
    "                        'columns': {\n",
    "                            'building_id': 'str',\n",
    "                            'total_floor_area': 'float',\n",
    "                            'total_volume': 'float',\n",
    "                            'zone_count': 'int'\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            'relationships': {\n",
    "                'files': {\n",
    "                    'relationships/zone_mappings.parquet': {\n",
    "                        'type': 'parquet',\n",
    "                        'description': 'IDF to SQL zone name mappings',\n",
    "                        'critical': True,\n",
    "                        'columns': {\n",
    "                            'building_id': 'str',\n",
    "                            'idf_zone_name': 'str',\n",
    "                            'sql_zone_name': 'str'\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            'output_validation': {\n",
    "                'files': {\n",
    "                    'sql_results/output_validation/validation_results.parquet': {\n",
    "                        'type': 'parquet',\n",
    "                        'description': 'Output completeness validation',\n",
    "                        'critical': False,\n",
    "                        'columns': {\n",
    "                            'building_id': 'str',\n",
    "                            'total_requested': 'int',\n",
    "                            'found': 'int',\n",
    "                            'coverage': 'float'\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def check_file_exists(self, base_path: Path, file_path: str) -> Tuple[bool, Optional[str]]:\n",
    "        \"\"\"Check if file exists, handling wildcards.\"\"\"\n",
    "        full_path = base_path / file_path\n",
    "        \n",
    "        if '*' in file_path:\n",
    "            parent = full_path.parent\n",
    "            pattern = full_path.name\n",
    "            if parent.exists():\n",
    "                matching_files = list(parent.glob(pattern))\n",
    "                if matching_files:\n",
    "                    return True, str(matching_files[0])\n",
    "            return False, None\n",
    "        \n",
    "        return full_path.exists(), str(full_path) if full_path.exists() else None\n",
    "    \n",
    "    def check_directory(self, dir_path: Path, dir_name: str) -> Dict:\n",
    "        \"\"\"Check a single directory comprehensively.\"\"\"\n",
    "        results = {\n",
    "            'directory': dir_name,\n",
    "            'path': str(dir_path),\n",
    "            'exists': dir_path.exists(),\n",
    "            'categories': {},\n",
    "            'summary': {\n",
    "                'total_expected': 0,\n",
    "                'found': 0,\n",
    "                'missing': 0,\n",
    "                'critical_missing': 0,\n",
    "                'extra_files': 0\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        if not dir_path.exists():\n",
    "            return results\n",
    "        \n",
    "        # Check each category\n",
    "        for category, cat_info in self.expected_structure.items():\n",
    "            cat_results = {\n",
    "                'files': {},\n",
    "                'found': 0,\n",
    "                'missing': 0,\n",
    "                'critical_missing': 0\n",
    "            }\n",
    "            \n",
    "            for file_path, file_info in cat_info['files'].items():\n",
    "                exists, abs_path = self.check_file_exists(dir_path, file_path)\n",
    "                \n",
    "                file_result = {\n",
    "                    'exists': exists,\n",
    "                    'path': abs_path,\n",
    "                    'type': file_info['type'],\n",
    "                    'description': file_info['description'],\n",
    "                    'critical': file_info.get('critical', False),\n",
    "                    'status': 'OK' if exists else ('CRITICAL' if file_info.get('critical', False) else 'MISSING')\n",
    "                }\n",
    "                \n",
    "                if exists:\n",
    "                    cat_results['found'] += 1\n",
    "                    results['summary']['found'] += 1\n",
    "                    \n",
    "                    # Check file contents\n",
    "                    if file_info['type'] == 'parquet':\n",
    "                        file_result.update(self._check_parquet_contents(abs_path, file_info.get('columns', {})))\n",
    "                    elif file_info['type'] == 'json':\n",
    "                        file_result.update(self._check_json_contents(abs_path, file_info.get('fields', {})))\n",
    "                else:\n",
    "                    cat_results['missing'] += 1\n",
    "                    results['summary']['missing'] += 1\n",
    "                    if file_info.get('critical', False):\n",
    "                        cat_results['critical_missing'] += 1\n",
    "                        results['summary']['critical_missing'] += 1\n",
    "                \n",
    "                cat_results['files'][file_path] = file_result\n",
    "                results['summary']['total_expected'] += 1\n",
    "            \n",
    "            results['categories'][category] = cat_results\n",
    "        \n",
    "        # Check for extra files\n",
    "        results['extra_files'] = self._find_extra_files(dir_path)\n",
    "        results['summary']['extra_files'] = len(results['extra_files'])\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def _check_parquet_contents(self, file_path: str, expected_columns: Dict) -> Dict:\n",
    "        \"\"\"Check parquet file contents.\"\"\"\n",
    "        try:\n",
    "            df = pd.read_parquet(file_path)\n",
    "            \n",
    "            result = {\n",
    "                'row_count': len(df),\n",
    "                'actual_columns': list(df.columns),\n",
    "                'column_check': {},\n",
    "                'extra_columns': [],\n",
    "                'missing_columns': []\n",
    "            }\n",
    "            \n",
    "            # Check expected columns\n",
    "            for col, dtype in expected_columns.items():\n",
    "                if col in df.columns:\n",
    "                    actual_dtype = self._get_dtype_string(df[col].dtype)\n",
    "                    type_match = self._check_type_match(actual_dtype, dtype)\n",
    "                    \n",
    "                    result['column_check'][col] = {\n",
    "                        'exists': True,\n",
    "                        'expected_type': dtype,\n",
    "                        'actual_type': actual_dtype,\n",
    "                        'type_match': type_match,\n",
    "                        'null_count': int(df[col].isnull().sum()),\n",
    "                        'unique_count': int(df[col].nunique())\n",
    "                    }\n",
    "                else:\n",
    "                    result['missing_columns'].append(col)\n",
    "                    result['column_check'][col] = {\n",
    "                        'exists': False,\n",
    "                        'expected_type': dtype\n",
    "                    }\n",
    "            \n",
    "            # Find extra columns\n",
    "            for col in df.columns:\n",
    "                if col not in expected_columns:\n",
    "                    result['extra_columns'].append(col)\n",
    "            \n",
    "            # Check for variant tracking in modified results\n",
    "            if 'variant_id' in df.columns:\n",
    "                result['has_variant_tracking'] = True\n",
    "                result['variant_ids'] = df['variant_id'].unique().tolist()\n",
    "            \n",
    "            return result\n",
    "            \n",
    "        except Exception as e:\n",
    "            return {'error': str(e)}\n",
    "    \n",
    "    def _check_json_contents(self, file_path: str, expected_fields: Dict) -> Dict:\n",
    "        \"\"\"Check JSON file contents.\"\"\"\n",
    "        try:\n",
    "            with open(file_path, 'r') as f:\n",
    "                data = json.load(f)\n",
    "            \n",
    "            result = {\n",
    "                'field_check': {},\n",
    "                'extra_fields': [],\n",
    "                'missing_fields': []\n",
    "            }\n",
    "            \n",
    "            # Check expected fields\n",
    "            for field, dtype in expected_fields.items():\n",
    "                if field in data:\n",
    "                    result['field_check'][field] = {\n",
    "                        'exists': True,\n",
    "                        'expected_type': dtype,\n",
    "                        'actual_type': type(data[field]).__name__\n",
    "                    }\n",
    "                else:\n",
    "                    result['missing_fields'].append(field)\n",
    "                    result['field_check'][field] = {\n",
    "                        'exists': False,\n",
    "                        'expected_type': dtype\n",
    "                    }\n",
    "            \n",
    "            # Find extra fields\n",
    "            for field in data:\n",
    "                if field not in expected_fields:\n",
    "                    result['extra_fields'].append(field)\n",
    "            \n",
    "            return result\n",
    "            \n",
    "        except Exception as e:\n",
    "            return {'error': str(e)}\n",
    "    \n",
    "    def _find_extra_files(self, dir_path: Path) -> List[str]:\n",
    "        \"\"\"Find files not in expected structure.\"\"\"\n",
    "        extra_files = []\n",
    "        \n",
    "        # Get all expected file patterns\n",
    "        expected_patterns = set()\n",
    "        for category in self.expected_structure.values():\n",
    "            for file_path in category['files'].keys():\n",
    "                if '*' in file_path:\n",
    "                    # Handle wildcards\n",
    "                    parts = file_path.split('/')\n",
    "                    pattern = parts[-1]\n",
    "                    parent = '/'.join(parts[:-1])\n",
    "                    expected_patterns.add((parent, pattern))\n",
    "                else:\n",
    "                    expected_patterns.add((file_path, None))\n",
    "        \n",
    "        # Walk directory and find extras\n",
    "        for root, dirs, files in os.walk(dir_path):\n",
    "            rel_root = Path(root).relative_to(dir_path)\n",
    "            \n",
    "            for file in files:\n",
    "                if file.endswith(('.parquet', '.json')):\n",
    "                    rel_path = str(rel_root / file).replace('\\\\', '/')\n",
    "                    \n",
    "                    # Check if this matches any expected pattern\n",
    "                    is_expected = False\n",
    "                    for expected, pattern in expected_patterns:\n",
    "                        if pattern:  # Wildcard pattern\n",
    "                            if str(rel_root).replace('\\\\', '/') == expected.split('/')[0]:\n",
    "                                import fnmatch\n",
    "                                if fnmatch.fnmatch(file, pattern):\n",
    "                                    is_expected = True\n",
    "                                    break\n",
    "                        else:  # Exact match\n",
    "                            if rel_path == expected:\n",
    "                                is_expected = True\n",
    "                                break\n",
    "                    \n",
    "                    if not is_expected:\n",
    "                        extra_files.append(rel_path)\n",
    "        \n",
    "        return extra_files\n",
    "    \n",
    "    def _get_dtype_string(self, dtype) -> str:\n",
    "        \"\"\"Convert numpy/pandas dtype to string.\"\"\"\n",
    "        dtype_str = str(dtype)\n",
    "        \n",
    "        if 'int' in dtype_str:\n",
    "            return 'int'\n",
    "        elif 'float' in dtype_str:\n",
    "            return 'float'\n",
    "        elif 'object' in dtype_str or 'string' in dtype_str:\n",
    "            return 'str'\n",
    "        elif 'datetime' in dtype_str:\n",
    "            return 'datetime'\n",
    "        elif 'bool' in dtype_str:\n",
    "            return 'bool'\n",
    "        else:\n",
    "            return dtype_str\n",
    "    \n",
    "    def _check_type_match(self, actual_type: str, expected_type) -> bool:\n",
    "        \"\"\"Check if actual type matches expected type(s).\"\"\"\n",
    "        if isinstance(expected_type, list):\n",
    "            return actual_type in expected_type\n",
    "        else:\n",
    "            return actual_type == expected_type or \\\n",
    "                   (expected_type == 'str' and actual_type in ['object', 'str'])\n",
    "    \n",
    "    def compare_base_and_modified(self) -> Dict:\n",
    "        \"\"\"Compare base and modified results.\"\"\"\n",
    "        comparison = {\n",
    "            'base': self.check_directory(self.base_parsed_dir, 'base'),\n",
    "            'modified': self.check_directory(self.modified_parsed_dir, 'modified') if self.modified_parsed_dir else None,\n",
    "            'differences': {}\n",
    "        }\n",
    "        \n",
    "        if comparison['modified']:\n",
    "            # Find differences\n",
    "            for category in self.expected_structure:\n",
    "                cat_diff = {\n",
    "                    'base_only': [],\n",
    "                    'modified_only': [],\n",
    "                    'both_missing': [],\n",
    "                    'content_differences': {}\n",
    "                }\n",
    "                \n",
    "                base_files = comparison['base']['categories'].get(category, {}).get('files', {})\n",
    "                mod_files = comparison['modified']['categories'].get(category, {}).get('files', {})\n",
    "                \n",
    "                for file_path in set(list(base_files.keys()) + list(mod_files.keys())):\n",
    "                    base_exists = base_files.get(file_path, {}).get('exists', False)\n",
    "                    mod_exists = mod_files.get(file_path, {}).get('exists', False)\n",
    "                    \n",
    "                    if base_exists and not mod_exists:\n",
    "                        cat_diff['base_only'].append(file_path)\n",
    "                    elif not base_exists and mod_exists:\n",
    "                        cat_diff['modified_only'].append(file_path)\n",
    "                    elif not base_exists and not mod_exists:\n",
    "                        cat_diff['both_missing'].append(file_path)\n",
    "                    elif base_exists and mod_exists:\n",
    "                        # Check content differences\n",
    "                        base_content = base_files[file_path]\n",
    "                        mod_content = mod_files[file_path]\n",
    "                        \n",
    "                        if 'row_count' in base_content and 'row_count' in mod_content:\n",
    "                            if base_content['row_count'] != mod_content['row_count']:\n",
    "                                cat_diff['content_differences'][file_path] = {\n",
    "                                    'base_rows': base_content['row_count'],\n",
    "                                    'modified_rows': mod_content['row_count']\n",
    "                                }\n",
    "                \n",
    "                comparison['differences'][category] = cat_diff\n",
    "        \n",
    "        return comparison\n",
    "    \n",
    "    def create_visual_summary(self, results: Dict):\n",
    "        \"\"\"Create a visual summary of the results.\"\"\"\n",
    "        print(\"\\n\" + \"=\"*100)\n",
    "        print(f\"{self.COLORS['BOLD']}PARSER DATA STRUCTURE CHECK - VISUAL SUMMARY{self.COLORS['END']}\")\n",
    "        print(\"=\"*100)\n",
    "        \n",
    "        # Overall summary\n",
    "        if 'base' in results:\n",
    "            self._print_directory_summary(results['base'], 'BASE PARSED DATA')\n",
    "        \n",
    "        if results.get('modified'):\n",
    "            print(\"\\n\" + \"-\"*100 + \"\\n\")\n",
    "            self._print_directory_summary(results['modified'], 'MODIFIED PARSED DATA')\n",
    "        \n",
    "        # Comparison summary if both exist\n",
    "        if results.get('modified') and results.get('differences'):\n",
    "            print(\"\\n\" + \"-\"*100)\n",
    "            print(f\"\\n{self.COLORS['BOLD']}COMPARISON SUMMARY{self.COLORS['END']}\")\n",
    "            print(\"-\"*50)\n",
    "            \n",
    "            for category, diffs in results['differences'].items():\n",
    "                if any([diffs['base_only'], diffs['modified_only'], diffs['both_missing']]):\n",
    "                    print(f\"\\n{self.COLORS['CYAN']}{category.upper()}{self.COLORS['END']}\")\n",
    "                    \n",
    "                    if diffs['base_only']:\n",
    "                        print(f\"  {self.COLORS['YELLOW']}Base only:{self.COLORS['END']} {len(diffs['base_only'])} files\")\n",
    "                    if diffs['modified_only']:\n",
    "                        print(f\"  {self.COLORS['BLUE']}Modified only:{self.COLORS['END']} {len(diffs['modified_only'])} files\")\n",
    "                    if diffs['both_missing']:\n",
    "                        print(f\"  {self.COLORS['RED']}Both missing:{self.COLORS['END']} {len(diffs['both_missing'])} files\")\n",
    "    \n",
    "    def _print_directory_summary(self, dir_results: Dict, title: str):\n",
    "        \"\"\"Print summary for a single directory.\"\"\"\n",
    "        print(f\"\\n{self.COLORS['BOLD']}{title}{self.COLORS['END']}\")\n",
    "        print(f\"Path: {dir_results['path']}\")\n",
    "        \n",
    "        if not dir_results['exists']:\n",
    "            print(f\"{self.COLORS['RED']}DIRECTORY DOES NOT EXIST{self.COLORS['END']}\")\n",
    "            return\n",
    "        \n",
    "        summary = dir_results['summary']\n",
    "        \n",
    "        # Overall status bar\n",
    "        total = summary['total_expected']\n",
    "        found = summary['found']\n",
    "        missing = summary['missing']\n",
    "        critical = summary['critical_missing']\n",
    "        \n",
    "        print(f\"\\nOverall: {found}/{total} files found\")\n",
    "        \n",
    "        # Visual progress bar\n",
    "        bar_length = 50\n",
    "        found_length = int((found / total) * bar_length) if total > 0 else 0\n",
    "        critical_length = int((critical / total) * bar_length) if total > 0 else 0\n",
    "        \n",
    "        bar = f\"[{self.COLORS['GREEN']}{'█' * found_length}{self.COLORS['END']}\"\n",
    "        bar += f\"{self.COLORS['RED']}{'▓' * critical_length}{self.COLORS['END']}\"\n",
    "        bar += f\"{self.COLORS['YELLOW']}{'░' * (bar_length - found_length - critical_length)}{self.COLORS['END']}]\"\n",
    "        \n",
    "        print(bar)\n",
    "        print(f\"Legend: {self.COLORS['GREEN']}█ Found{self.COLORS['END']} | \"\n",
    "              f\"{self.COLORS['RED']}▓ Critical Missing{self.COLORS['END']} | \"\n",
    "              f\"{self.COLORS['YELLOW']}░ Optional Missing{self.COLORS['END']}\")\n",
    "        \n",
    "        # Category breakdown\n",
    "        print(\"\\nCategory Status:\")\n",
    "        for category, cat_results in dir_results['categories'].items():\n",
    "            cat_found = cat_results['found']\n",
    "            cat_total = len(cat_results['files'])\n",
    "            cat_critical = cat_results['critical_missing']\n",
    "            \n",
    "            # Status icon\n",
    "            if cat_critical > 0:\n",
    "                status = f\"{self.COLORS['RED']}✗{self.COLORS['END']}\"\n",
    "            elif cat_found == cat_total:\n",
    "                status = f\"{self.COLORS['GREEN']}✓{self.COLORS['END']}\"\n",
    "            else:\n",
    "                status = f\"{self.COLORS['YELLOW']}⚠{self.COLORS['END']}\"\n",
    "            \n",
    "            print(f\"  {status} {category:<20} {cat_found}/{cat_total} files\")\n",
    "            \n",
    "            # Show critical missing files\n",
    "            if cat_critical > 0:\n",
    "                for file_path, file_info in cat_results['files'].items():\n",
    "                    if file_info['status'] == 'CRITICAL':\n",
    "                        print(f\"      {self.COLORS['RED']}↳ Missing: {file_path.split('/')[-1]}{self.COLORS['END']}\")\n",
    "        \n",
    "        # Extra files\n",
    "        if dir_results['summary']['extra_files'] > 0:\n",
    "            print(f\"\\n{self.COLORS['PURPLE']}Extra files found: {dir_results['summary']['extra_files']}{self.COLORS['END']}\")\n",
    "            for extra in dir_results['extra_files'][:5]:\n",
    "                print(f\"  + {extra}\")\n",
    "            if len(dir_results['extra_files']) > 5:\n",
    "                print(f\"  ... and {len(dir_results['extra_files']) - 5} more\")\n",
    "    \n",
    "    def create_detailed_report(self, results: Dict) -> pd.DataFrame:\n",
    "        \"\"\"Create detailed report as DataFrame.\"\"\"\n",
    "        rows = []\n",
    "        \n",
    "        for dataset in ['base', 'modified']:\n",
    "            if dataset in results and results[dataset]['exists']:\n",
    "                dir_results = results[dataset]\n",
    "                \n",
    "                for category, cat_results in dir_results['categories'].items():\n",
    "                    for file_path, file_info in cat_results['files'].items():\n",
    "                        row = {\n",
    "                            'Dataset': dataset.upper(),\n",
    "                            'Category': category,\n",
    "                            'File': file_path.split('/')[-1],\n",
    "                            'Path': file_path,\n",
    "                            'Type': file_info['type'],\n",
    "                            'Critical': '✓' if file_info.get('critical', False) else '',\n",
    "                            'Exists': '✓' if file_info['exists'] else '✗',\n",
    "                            'Status': file_info['status'],\n",
    "                            'Description': file_info['description']\n",
    "                        }\n",
    "                        \n",
    "                        if file_info['exists']:\n",
    "                            if 'row_count' in file_info:\n",
    "                                row['Rows'] = file_info['row_count']\n",
    "                            if 'missing_columns' in file_info:\n",
    "                                row['Missing_Columns'] = len(file_info['missing_columns'])\n",
    "                            if 'extra_columns' in file_info:\n",
    "                                row['Extra_Columns'] = len(file_info['extra_columns'])\n",
    "                            if 'has_variant_tracking' in file_info:\n",
    "                                row['Has_Variants'] = '✓'\n",
    "                                row['Variant_Count'] = len(file_info.get('variant_ids', []))\n",
    "                        \n",
    "                        rows.append(row)\n",
    "        \n",
    "        return pd.DataFrame(rows)\n",
    "    \n",
    "    def analyze_variant_tracking(self, results: Dict) -> Dict:\n",
    "        \"\"\"Analyze variant tracking in modified results.\"\"\"\n",
    "        variant_analysis = {\n",
    "            'has_variant_tracking': False,\n",
    "            'files_with_variants': [],\n",
    "            'unique_variants': set(),\n",
    "            'building_variant_mapping': defaultdict(set)\n",
    "        }\n",
    "        \n",
    "        if not results.get('modified') or not results['modified']['exists']:\n",
    "            return variant_analysis\n",
    "        \n",
    "        mod_results = results['modified']\n",
    "        \n",
    "        for category, cat_results in mod_results['categories'].items():\n",
    "            for file_path, file_info in cat_results['files'].items():\n",
    "                if file_info.get('exists') and file_info.get('has_variant_tracking'):\n",
    "                    variant_analysis['has_variant_tracking'] = True\n",
    "                    variant_analysis['files_with_variants'].append(file_path)\n",
    "                    \n",
    "                    if 'variant_ids' in file_info:\n",
    "                        variant_analysis['unique_variants'].update(file_info['variant_ids'])\n",
    "                    \n",
    "                    # Try to load file to get building-variant mapping\n",
    "                    if file_info.get('path') and file_info['type'] == 'parquet':\n",
    "                        try:\n",
    "                            df = pd.read_parquet(file_info['path'])\n",
    "                            if 'building_id' in df.columns and 'variant_id' in df.columns:\n",
    "                                for _, row in df[['building_id', 'variant_id']].drop_duplicates().iterrows():\n",
    "                                    variant_analysis['building_variant_mapping'][row['building_id']].add(row['variant_id'])\n",
    "                        except:\n",
    "                            pass\n",
    "        \n",
    "        variant_analysis['unique_variants'] = list(variant_analysis['unique_variants'])\n",
    "        variant_analysis['building_variant_mapping'] = dict(variant_analysis['building_variant_mapping'])\n",
    "        \n",
    "        return variant_analysis\n",
    "    \n",
    "    def save_results(self, results: Dict, output_dir: Path = None):\n",
    "        \"\"\"Save all results.\"\"\"\n",
    "        if output_dir is None:\n",
    "            output_dir = self.base_parsed_dir / 'data_structure_check'\n",
    "        \n",
    "        output_dir.mkdir(exist_ok=True)\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        \n",
    "        # Save detailed report\n",
    "        df_report = self.create_detailed_report(results)\n",
    "        df_report.to_csv(output_dir / f'structure_check_{timestamp}.csv', index=False)\n",
    "        df_report.to_excel(output_dir / f'structure_check_{timestamp}.xlsx', index=False)\n",
    "        \n",
    "        # Save JSON results\n",
    "        with open(output_dir / f'structure_check_results_{timestamp}.json', 'w') as f:\n",
    "            json.dump(self._make_json_serializable(results), f, indent=2)\n",
    "        \n",
    "        # Save variant analysis if applicable\n",
    "        if results.get('modified'):\n",
    "            variant_analysis = self.analyze_variant_tracking(results)\n",
    "            with open(output_dir / f'variant_analysis_{timestamp}.json', 'w') as f:\n",
    "                json.dump(self._make_json_serializable(variant_analysis), f, indent=2)\n",
    "        \n",
    "        # Create markdown report\n",
    "        self.create_markdown_report(output_dir / f'report_{timestamp}.md', results)\n",
    "        \n",
    "        print(f\"\\n{self.COLORS['GREEN']}Results saved to: {output_dir}{self.COLORS['END']}\")\n",
    "        \n",
    "        return output_dir\n",
    "    \n",
    "    def create_markdown_report(self, output_path: Path, results: Dict):\n",
    "        \"\"\"Create comprehensive markdown report.\"\"\"\n",
    "        with open(output_path, 'w', encoding='utf-8') as f:\n",
    "            f.write(\"# Parser Data Structure Check Report\\n\\n\")\n",
    "            f.write(f\"**Generated:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\\n\")\n",
    "            \n",
    "            # Base results\n",
    "            if 'base' in results:\n",
    "                f.write(\"## Base Parsed Data\\n\\n\")\n",
    "                self._write_directory_markdown(f, results['base'])\n",
    "            \n",
    "            # Modified results\n",
    "            if results.get('modified'):\n",
    "                f.write(\"\\n## Modified Parsed Data\\n\\n\")\n",
    "                self._write_directory_markdown(f, results['modified'])\n",
    "                \n",
    "                # Variant analysis\n",
    "                variant_analysis = self.analyze_variant_tracking(results)\n",
    "                if variant_analysis['has_variant_tracking']:\n",
    "                    f.write(\"\\n### Variant Tracking Analysis\\n\\n\")\n",
    "                    f.write(f\"- **Variant tracking enabled:** ✓\\n\")\n",
    "                    f.write(f\"- **Unique variants:** {len(variant_analysis['unique_variants'])}\\n\")\n",
    "                    f.write(f\"- **Files with variants:** {len(variant_analysis['files_with_variants'])}\\n\")\n",
    "                    \n",
    "                    if variant_analysis['unique_variants']:\n",
    "                        f.write(\"\\n**Variants found:**\\n\")\n",
    "                        for variant in sorted(variant_analysis['unique_variants']):\n",
    "                            f.write(f\"- {variant}\\n\")\n",
    "            \n",
    "            # Comparison\n",
    "            if results.get('differences'):\n",
    "                f.write(\"\\n## Comparison Summary\\n\\n\")\n",
    "                for category, diffs in results['differences'].items():\n",
    "                    if any([diffs['base_only'], diffs['modified_only'], diffs['both_missing']]):\n",
    "                        f.write(f\"\\n### {category}\\n\\n\")\n",
    "                        \n",
    "                        if diffs['base_only']:\n",
    "                            f.write(\"**Files only in base:**\\n\")\n",
    "                            for file in diffs['base_only']:\n",
    "                                f.write(f\"- {file}\\n\")\n",
    "                        \n",
    "                        if diffs['modified_only']:\n",
    "                            f.write(\"\\n**Files only in modified:**\\n\")\n",
    "                            for file in diffs['modified_only']:\n",
    "                                f.write(f\"- {file}\\n\")\n",
    "                        \n",
    "                        if diffs['both_missing']:\n",
    "                            f.write(\"\\n**Files missing in both:**\\n\")\n",
    "                            for file in diffs['both_missing']:\n",
    "                                f.write(f\"- {file}\\n\")\n",
    "    \n",
    "    def _write_directory_markdown(self, f, dir_results: Dict):\n",
    "        \"\"\"Write directory results to markdown.\"\"\"\n",
    "        f.write(f\"**Path:** `{dir_results['path']}`\\n\\n\")\n",
    "        \n",
    "        if not dir_results['exists']:\n",
    "            f.write(\"⚠️ **Directory does not exist**\\n\\n\")\n",
    "            return\n",
    "        \n",
    "        summary = dir_results['summary']\n",
    "        f.write(f\"**Summary:** {summary['found']}/{summary['total_expected']} files found\\n\\n\")\n",
    "        \n",
    "        if summary['critical_missing'] > 0:\n",
    "            f.write(f\"⚠️ **Critical files missing:** {summary['critical_missing']}\\n\\n\")\n",
    "        \n",
    "        # Category table\n",
    "        f.write(\"| Category | Status | Found | Total | Critical Missing |\\n\")\n",
    "        f.write(\"|----------|--------|-------|-------|------------------|\\n\")\n",
    "        \n",
    "        for category, cat_results in dir_results['categories'].items():\n",
    "            found = cat_results['found']\n",
    "            total = len(cat_results['files'])\n",
    "            critical = cat_results['critical_missing']\n",
    "            \n",
    "            if critical > 0:\n",
    "                status = \"❌\"\n",
    "            elif found == total:\n",
    "                status = \"✅\"\n",
    "            else:\n",
    "                status = \"⚠️\"\n",
    "            \n",
    "            f.write(f\"| {category} | {status} | {found} | {total} | {critical} |\\n\")\n",
    "    \n",
    "    def _make_json_serializable(self, obj):\n",
    "        \"\"\"Convert non-serializable objects for JSON.\"\"\"\n",
    "        if isinstance(obj, set):\n",
    "            return list(obj)\n",
    "        elif isinstance(obj, defaultdict):\n",
    "            return dict(obj)\n",
    "        elif isinstance(obj, Path):\n",
    "            return str(obj)\n",
    "        elif isinstance(obj, dict):\n",
    "            return {k: self._make_json_serializable(v) for k, v in obj.items()}\n",
    "        elif isinstance(obj, list):\n",
    "            return [self._make_json_serializable(item) for item in obj]\n",
    "        else:\n",
    "            return obj\n",
    "\n",
    "# Main function\n",
    "def check_parser_data_enhanced(base_parsed_dir: str, modified_parsed_dir: str = None):\n",
    "    \"\"\"Run enhanced parser data structure check.\"\"\"\n",
    "    \n",
    "    checker = EnhancedParserDataChecker(base_parsed_dir, modified_parsed_dir)\n",
    "    \n",
    "    # Run comprehensive check\n",
    "    if modified_parsed_dir:\n",
    "        print(f\"\\nChecking both base and modified parsed data...\")\n",
    "        results = checker.compare_base_and_modified()\n",
    "    else:\n",
    "        print(f\"\\nChecking base parsed data only...\")\n",
    "        results = {'base': checker.check_directory(checker.base_parsed_dir, 'base')}\n",
    "    \n",
    "    # Create visual summary\n",
    "    checker.create_visual_summary(results)\n",
    "    \n",
    "    # Analyze variants if modified data exists\n",
    "    if results.get('modified'):\n",
    "        variant_analysis = checker.analyze_variant_tracking(results)\n",
    "        if variant_analysis['has_variant_tracking']:\n",
    "            print(f\"\\n{checker.COLORS['BOLD']}VARIANT TRACKING ANALYSIS{checker.COLORS['END']}\")\n",
    "            print(\"-\"*50)\n",
    "            print(f\"Unique variants found: {len(variant_analysis['unique_variants'])}\")\n",
    "            print(f\"Files with variant tracking: {len(variant_analysis['files_with_variants'])}\")\n",
    "            \n",
    "            if variant_analysis['building_variant_mapping']:\n",
    "                print(f\"\\nBuildings with variants: {len(variant_analysis['building_variant_mapping'])}\")\n",
    "                for building_id, variants in list(variant_analysis['building_variant_mapping'].items())[:5]:\n",
    "                    print(f\"  {building_id}: {', '.join(variants)}\")\n",
    "                if len(variant_analysis['building_variant_mapping']) > 5:\n",
    "                    print(f\"  ... and {len(variant_analysis['building_variant_mapping']) - 5} more buildings\")\n",
    "    \n",
    "    # Save results\n",
    "    output_dir = checker.save_results(results)\n",
    "    \n",
    "    # Create detailed report\n",
    "    df_report = checker.create_detailed_report(results)\n",
    "    \n",
    "    return results, df_report\n",
    "\n",
    "# Usage example\n",
    "if __name__ == \"__main__\":\n",
    "    # Your paths\n",
    "    base_dir = r\"D:\\Documents\\daily\\E_Plus_2040_py\\output\\82e2b83f-5013-4270-8266-a37a67dbd4ff\\parsed_data\"\n",
    "    modified_dir = r\"D:\\Documents\\daily\\E_Plus_2040_py\\output\\82e2b83f-5013-4270-8266-a37a67dbd4ff\\parsed_modified_results\"\n",
    "    \n",
    "    # Run the check\n",
    "    results, df_report = check_parser_data_enhanced(base_dir, modified_dir)\n",
    "    \n",
    "    # Show critical issues\n",
    "    print(\"\\n\" + \"=\"*100)\n",
    "    print(\"CRITICAL ISSUES SUMMARY\")\n",
    "    print(\"=\"*100)\n",
    "    \n",
    "    critical_df = df_report[df_report['Status'] == 'CRITICAL']\n",
    "    if not critical_df.empty:\n",
    "        print(critical_df[['Dataset', 'Category', 'File', 'Description']].to_string(index=False))\n",
    "    else:\n",
    "        print(\"No critical issues found!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DDsaie",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
