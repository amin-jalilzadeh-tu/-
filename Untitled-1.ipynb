{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca01fbf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exploring SQL structure...\n",
      "Connected to: simulation_bldg0_4136733.sql\n",
      "================================================================================\n",
      "\n",
      "Available Variables by Frequency:\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Daily Frequency:\n",
      "  Total variables: 1\n",
      "  Sample variables:\n",
      "    - Electricity:Facility (None) - 365 points\n",
      "\n",
      "Monthly Frequency:\n",
      "  Total variables: 475\n",
      "  Sample variables:\n",
      "    - Cooling:EnergyTransfer (None) - 12 points\n",
      "    - Heating:EnergyTransfer (None) - 12 points\n",
      "    - Site Diffuse Solar Radiation Rate per Area (Environment) - 12 points\n",
      "    - Site Outdoor Air Drybulb Temperature (Environment) - 12 points\n",
      "    - Site Outdoor Air Relative Humidity (Environment) - 12 points\n",
      "\n",
      "Time Range: 2013-01-01 to 2013-12-31\n",
      "Total Days: 365\n",
      "\n",
      "================================================================================\n",
      "Current Parquet Structure:\n",
      "================================================================================\n",
      "\n",
      "Base Data Structure:\n",
      "\n",
      "  hourly data:\n",
      "    energy_2013.parquet:\n",
      "      Shape: (1275, 11)\n",
      "      Columns: ['TimeIndex', 'DateTime', 'Variable', 'Zone', 'Value', 'Units', 'ReportingFrequency', 'category', 'IDF_Zone', 'building_id', 'variant_id']\n",
      "      Sample row:\n",
      "      {'TimeIndex': 1, 'DateTime': Timestamp('2013-01-02 00:00:00'), 'Variable': 'Electricity:Facility', 'Zone': None, 'Value': 6399384.977889667, 'Units': 'J', 'ReportingFrequency': 'Daily', 'category': 'energy_meters', 'IDF_Zone': None, 'building_id': '4136733', 'variant_id': 'base'}\n",
      "    hvac_2013.parquet:\n",
      "      Shape: (804, 11)\n",
      "      Columns: ['TimeIndex', 'DateTime', 'Variable', 'Zone', 'Value', 'Units', 'ReportingFrequency', 'category', 'IDF_Zone', 'building_id', 'variant_id']\n",
      "      Sample row:\n",
      "      {'TimeIndex': 32, 'DateTime': Timestamp('2013-02-01 00:00:00'), 'Variable': 'Water Heater Heating Energy', 'Zone': 'MYDHW_0_WATERHEATER', 'Value': 1059503368.855152, 'Units': 'J', 'ReportingFrequency': 'Monthly', 'category': 'dhw', 'IDF_Zone': 'MYDHW_0_WATERHEATER', 'building_id': '4136733', 'variant_id': 'base'}\n",
      "    ventilation_2013.parquet:\n",
      "      Shape: (1104, 11)\n",
      "      Columns: ['TimeIndex', 'DateTime', 'Variable', 'Zone', 'Value', 'Units', 'ReportingFrequency', 'category', 'IDF_Zone', 'building_id', 'variant_id']\n",
      "      Sample row:\n",
      "      {'TimeIndex': 32, 'DateTime': Timestamp('2013-02-01 00:00:00'), 'Variable': 'Zone Mechanical Ventilation Mass Flow Rate', 'Zone': 'ZONE1_FRONTPERIMETER', 'Value': 0.0031372884410621592, 'Units': 'kg/s', 'ReportingFrequency': 'Monthly', 'category': 'ventilation', 'IDF_Zone': 'ZONE1_FRONTPERIMETER', 'building_id': '4136733', 'variant_id': 'base'}\n",
      "\n",
      "  daily data:\n",
      "\n",
      "  monthly data:\n",
      "\n",
      "\n",
      "Variant Data Structure:\n",
      "\n",
      "================================================================================\n",
      "Variant Structure Analysis:\n",
      "================================================================================\n",
      "\n",
      "Found 19 modified IDF files\n",
      "\n",
      "Buildings and their variants:\n",
      "  Building 4136733: ['variant_0', 'variant_1', 'variant_10', 'variant_11', 'variant_12', 'variant_13', 'variant_14', 'variant_15', 'variant_16', 'variant_17']\n",
      "\n",
      "Found 19 SQL files in modified results\n",
      "  simulation_bldg0_4136733.sql\n",
      "  simulation_bldg10_4136733.sql\n",
      "  simulation_bldg11_4136733.sql\n",
      "  simulation_bldg12_4136733.sql\n",
      "  simulation_bldg13_4136733.sql\n",
      "\n",
      "Total buildings with variants: 1\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "SQL Data Explorer - To understand the current data structure\n",
    "Run this to examine your SQL files and current parquet structure\n",
    "\"\"\"\n",
    "\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "def explore_sql_structure(sql_path):\n",
    "    \"\"\"Explore the structure of SQL data\"\"\"\n",
    "    conn = sqlite3.connect(sql_path)\n",
    "    \n",
    "    print(f\"Connected to: {Path(sql_path).name}\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Get available variables with their frequencies\n",
    "    query = \"\"\"\n",
    "    SELECT DISTINCT \n",
    "        rdd.Name as Variable,\n",
    "        rdd.KeyValue as Zone,\n",
    "        rdd.ReportingFrequency,\n",
    "        rdd.Units,\n",
    "        COUNT(DISTINCT rd.TimeIndex) as DataPoints\n",
    "    FROM ReportDataDictionary rdd\n",
    "    LEFT JOIN ReportData rd ON rdd.ReportDataDictionaryIndex = rd.ReportDataDictionaryIndex\n",
    "    GROUP BY rdd.Name, rdd.KeyValue, rdd.ReportingFrequency\n",
    "    ORDER BY rdd.ReportingFrequency, rdd.Name\n",
    "    \"\"\"\n",
    "    \n",
    "    variables_df = pd.read_sql_query(query, conn)\n",
    "    \n",
    "    print(\"\\nAvailable Variables by Frequency:\")\n",
    "    print(\"-\"*80)\n",
    "    for freq in variables_df['ReportingFrequency'].unique():\n",
    "        print(f\"\\n{freq} Frequency:\")\n",
    "        freq_vars = variables_df[variables_df['ReportingFrequency'] == freq]\n",
    "        print(f\"  Total variables: {len(freq_vars)}\")\n",
    "        print(f\"  Sample variables:\")\n",
    "        for _, row in freq_vars.head(5).iterrows():\n",
    "            print(f\"    - {row['Variable']} ({row['Zone']}) - {row['DataPoints']} points\")\n",
    "    \n",
    "    # Get time range\n",
    "    time_query = \"\"\"\n",
    "    SELECT \n",
    "        MIN(date(printf('%04d-%02d-%02d', Year, Month, Day))) as start_date,\n",
    "        MAX(date(printf('%04d-%02d-%02d', Year, Month, Day))) as end_date,\n",
    "        COUNT(DISTINCT date(printf('%04d-%02d-%02d', Year, Month, Day))) as total_days\n",
    "    FROM Time\n",
    "    WHERE EnvironmentPeriodIndex = 3\n",
    "    \"\"\"\n",
    "    \n",
    "    time_info = pd.read_sql_query(time_query, conn)\n",
    "    print(f\"\\nTime Range: {time_info.iloc[0]['start_date']} to {time_info.iloc[0]['end_date']}\")\n",
    "    print(f\"Total Days: {time_info.iloc[0]['total_days']}\")\n",
    "    \n",
    "    conn.close()\n",
    "    return variables_df\n",
    "\n",
    "def check_current_parquet_structure(parsed_dir):\n",
    "    \"\"\"Check current parquet file structure\"\"\"\n",
    "    parsed_path = Path(parsed_dir)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"Current Parquet Structure:\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Check base data\n",
    "    base_path = parsed_path / 'sql_results' / 'timeseries'\n",
    "    if base_path.exists():\n",
    "        print(\"\\nBase Data Structure:\")\n",
    "        for freq in ['hourly', 'daily', 'monthly']:\n",
    "            freq_path = base_path / freq\n",
    "            if freq in ['daily', 'monthly']:\n",
    "                freq_path = base_path / 'aggregated' / freq\n",
    "            \n",
    "            if freq_path.exists():\n",
    "                print(f\"\\n  {freq} data:\")\n",
    "                for file in sorted(freq_path.glob('*.parquet'))[:3]:\n",
    "                    df = pd.read_parquet(file)\n",
    "                    print(f\"    {file.name}:\")\n",
    "                    print(f\"      Shape: {df.shape}\")\n",
    "                    print(f\"      Columns: {list(df.columns)}\")\n",
    "                    if not df.empty:\n",
    "                        print(f\"      Sample row:\")\n",
    "                        print(f\"      {df.iloc[0].to_dict()}\")\n",
    "    \n",
    "    # Check variant data\n",
    "    variant_path = parsed_path.parent / 'parsed_modified_results'\n",
    "    if variant_path.exists():\n",
    "        print(\"\\n\\nVariant Data Structure:\")\n",
    "        variant_ts_path = variant_path / 'timeseries' / 'variants' / 'daily'\n",
    "        if variant_ts_path.exists():\n",
    "            for file in sorted(variant_ts_path.glob('*.parquet'))[:3]:\n",
    "                df = pd.read_parquet(file)\n",
    "                print(f\"\\n  {file.name}:\")\n",
    "                print(f\"    Shape: {df.shape}\")\n",
    "                print(f\"    Columns: {list(df.columns)}\")\n",
    "                if not df.empty:\n",
    "                    print(f\"    Unique variants: {df.get('variant_id', ['N/A']).unique() if 'variant_id' in df.columns else 'No variant_id column'}\")\n",
    "                    print(f\"    Sample row:\")\n",
    "                    print(f\"    {df.iloc[0].to_dict()}\")\n",
    "\n",
    "def analyze_variant_structure(job_output_dir):\n",
    "    \"\"\"Analyze the variant structure from modified IDFs\"\"\"\n",
    "    job_path = Path(job_output_dir)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"Variant Structure Analysis:\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Initialize variant_map at the beginning\n",
    "    variant_map = {}\n",
    "    \n",
    "    # Check modified IDFs\n",
    "    modified_idfs_dir = job_path / 'modified_idfs'\n",
    "    if modified_idfs_dir.exists():\n",
    "        idf_files = list(modified_idfs_dir.glob('*.idf'))\n",
    "        print(f\"\\nFound {len(idf_files)} modified IDF files\")\n",
    "        \n",
    "        # Extract building and variant info\n",
    "        for idf in sorted(idf_files)[:10]:\n",
    "            parts = idf.stem.split('_')\n",
    "            if len(parts) >= 4 and parts[0] == 'building' and parts[2] == 'variant':\n",
    "                building_id = parts[1]\n",
    "                variant_num = parts[3]\n",
    "                \n",
    "                if building_id not in variant_map:\n",
    "                    variant_map[building_id] = []\n",
    "                variant_map[building_id].append(f\"variant_{variant_num}\")\n",
    "        \n",
    "        print(\"\\nBuildings and their variants:\")\n",
    "        for building_id, variants in variant_map.items():\n",
    "            print(f\"  Building {building_id}: {sorted(variants)}\")\n",
    "    else:\n",
    "        print(f\"\\nModified IDFs directory not found: {modified_idfs_dir}\")\n",
    "    \n",
    "    # Check SQL files for variants\n",
    "    sim_results_dir = job_path / 'Modified_Sim_Results'\n",
    "    if sim_results_dir.exists():\n",
    "        sql_files = list(sim_results_dir.glob('**/*.sql'))\n",
    "        print(f\"\\nFound {len(sql_files)} SQL files in modified results\")\n",
    "        \n",
    "        for sql_file in sorted(sql_files)[:5]:\n",
    "            print(f\"  {sql_file.name}\")\n",
    "    else:\n",
    "        print(f\"\\nModified simulation results directory not found: {sim_results_dir}\")\n",
    "    \n",
    "    return variant_map\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    # Update these paths to your actual paths\n",
    "    job_output_dir = r\"output\\221d33e9-f628-4f1e-86e5-3466f6d140a3\"\n",
    "    base_sql_path = r\"output\\221d33e9-f628-4f1e-86e5-3466f6d140a3\\Sim_Results\\2020\\simulation_bldg0_4136733.sql\"\n",
    "    \n",
    "    # Explore SQL structure\n",
    "    if Path(base_sql_path).exists():\n",
    "        print(\"Exploring SQL structure...\")\n",
    "        variables_df = explore_sql_structure(base_sql_path)\n",
    "    else:\n",
    "        print(f\"SQL file not found: {base_sql_path}\")\n",
    "    \n",
    "    # Check current parquet structure\n",
    "    parsed_dir = Path(job_output_dir) / 'parsed_data'\n",
    "    if parsed_dir.exists():\n",
    "        check_current_parquet_structure(parsed_dir)\n",
    "    else:\n",
    "        print(f\"\\nParsed data directory not found: {parsed_dir}\")\n",
    "    \n",
    "    # Analyze variant structure\n",
    "    variant_map = analyze_variant_structure(job_output_dir)\n",
    "    print(f\"\\nTotal buildings with variants: {len(variant_map)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee76f2ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming base data to semi-wide format...\n",
      "Saved semi-wide base data to: output\\221d33e9-f628-4f1e-86e5-3466f6d140a3\\parsed_data\\timeseries\\base\\daily\\all_variables.parquet\n",
      "Shape: (3, 371)\n",
      "Date columns: 365\n",
      "\n",
      "Transforming variant data to comparison format...\n",
      "  Loading variant variant_9 from SQL...\n",
      "  Loading variant variant_10 from SQL...\n",
      "  Loading variant variant_11 from SQL...\n",
      "  Loading variant variant_12 from SQL...\n",
      "  Loading variant variant_13 from SQL...\n",
      "  Loading variant variant_14 from SQL...\n",
      "  Loading variant variant_15 from SQL...\n",
      "  Loading variant variant_16 from SQL...\n",
      "  Loading variant variant_17 from SQL...\n",
      "  Loading variant variant_0 from SQL...\n",
      "  Loading variant variant_1 from SQL...\n",
      "  Loading variant variant_2 from SQL...\n",
      "  Loading variant variant_3 from SQL...\n",
      "  Loading variant variant_4 from SQL...\n",
      "  Loading variant variant_5 from SQL...\n",
      "  Loading variant variant_6 from SQL...\n",
      "  Loading variant variant_7 from SQL...\n",
      "  Loading variant variant_8 from SQL...\n",
      "  Processing variable: Electricity:Facility\n",
      "    Saved: electricity_facility.parquet (shape: (2191, 25))\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Enhanced SQL Data Transformer\n",
    "Transforms SQL data into the desired semi-wide and variant comparison formats\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "import sqlite3\n",
    "from datetime import datetime\n",
    "\n",
    "class EnhancedSQLTransformer:\n",
    "    \"\"\"Transform SQL data into desired formats\"\"\"\n",
    "    \n",
    "    def __init__(self, job_output_dir: str):\n",
    "        self.job_output_dir = Path(job_output_dir)\n",
    "        self.base_parsed_dir = self.job_output_dir / 'parsed_data'\n",
    "        self.modified_parsed_dir = self.job_output_dir / 'parsed_modified_results'\n",
    "        \n",
    "    def transform_base_to_semi_wide(self):\n",
    "        \"\"\"Transform base simulation data to semi-wide format with dates as columns\"\"\"\n",
    "        print(\"Transforming base data to semi-wide format...\")\n",
    "        \n",
    "        # Load all base data\n",
    "        base_data = self._load_all_base_data()\n",
    "        \n",
    "        if base_data.empty:\n",
    "            print(\"No base data found!\")\n",
    "            return\n",
    "        \n",
    "        # Convert to semi-wide format\n",
    "        semi_wide_df = self._convert_to_semi_wide(base_data)\n",
    "        \n",
    "        # Save the transformed data\n",
    "        output_path = self.base_parsed_dir / 'timeseries' / 'base' / 'daily'\n",
    "        output_path.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        output_file = output_path / 'all_variables.parquet'\n",
    "        semi_wide_df.to_parquet(output_file, index=False)\n",
    "        \n",
    "        print(f\"Saved semi-wide base data to: {output_file}\")\n",
    "        print(f\"Shape: {semi_wide_df.shape}\")\n",
    "        print(f\"Date columns: {len([c for c in semi_wide_df.columns if c.startswith('2')])}\")\n",
    "        \n",
    "        return semi_wide_df\n",
    "    \n",
    "    def transform_variants_to_comparison(self):\n",
    "        \"\"\"Transform variant data to comparison format with variant values as columns\"\"\"\n",
    "        print(\"\\nTransforming variant data to comparison format...\")\n",
    "        \n",
    "        # Get variant mapping\n",
    "        variant_map = self._get_variant_mapping()\n",
    "        \n",
    "        # Load base and variant data\n",
    "        base_data = self._load_all_base_data()\n",
    "        variant_data_dict = self._load_all_variant_data(variant_map)\n",
    "        \n",
    "        if base_data.empty or not variant_data_dict:\n",
    "            print(\"Missing base or variant data!\")\n",
    "            return\n",
    "        \n",
    "        # Create comparison files for each variable\n",
    "        output_path = self.base_parsed_dir / 'timeseries' / 'variants' / 'daily'\n",
    "        output_path.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Get unique variables\n",
    "        variables = base_data['Variable'].unique()\n",
    "        \n",
    "        for variable in variables:\n",
    "            print(f\"  Processing variable: {variable}\")\n",
    "            \n",
    "            # Create comparison dataframe for this variable\n",
    "            comparison_df = self._create_variant_comparison(\n",
    "                base_data, \n",
    "                variant_data_dict, \n",
    "                variable\n",
    "            )\n",
    "            \n",
    "            if not comparison_df.empty:\n",
    "                # Clean variable name for filename\n",
    "                clean_var_name = variable.lower().replace(':', '_').replace(' ', '_').replace('[', '').replace(']', '')\n",
    "                output_file = output_path / f\"{clean_var_name}.parquet\"\n",
    "                \n",
    "                comparison_df.to_parquet(output_file, index=False)\n",
    "                print(f\"    Saved: {output_file.name} (shape: {comparison_df.shape})\")\n",
    "    \n",
    "    def _load_all_base_data(self) -> pd.DataFrame:\n",
    "        \"\"\"Load all base simulation data\"\"\"\n",
    "        all_data = []\n",
    "        \n",
    "        # Load from different frequencies and categories\n",
    "        base_paths = [\n",
    "            self.base_parsed_dir / 'sql_results' / 'timeseries' / 'hourly',\n",
    "            self.base_parsed_dir / 'sql_results' / 'timeseries' / 'aggregated' / 'daily',\n",
    "            self.base_parsed_dir / 'sql_results' / 'timeseries' / 'raw' / 'daily'\n",
    "        ]\n",
    "        \n",
    "        for base_path in base_paths:\n",
    "            if base_path.exists():\n",
    "                for parquet_file in base_path.glob('*.parquet'):\n",
    "                    try:\n",
    "                        df = pd.read_parquet(parquet_file)\n",
    "                        \n",
    "                        # Ensure we have the required columns\n",
    "                        if 'DateTime' in df.columns and 'Variable' in df.columns:\n",
    "                            # Add variant_id if missing\n",
    "                            if 'variant_id' not in df.columns:\n",
    "                                df['variant_id'] = 'base'\n",
    "                            \n",
    "                            # Only keep daily data\n",
    "                            if 'ReportingFrequency' in df.columns:\n",
    "                                df = df[df['ReportingFrequency'].isin(['Daily', 'Hourly'])]\n",
    "                            \n",
    "                            all_data.append(df)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error loading {parquet_file}: {e}\")\n",
    "        \n",
    "        if all_data:\n",
    "            combined_df = pd.concat(all_data, ignore_index=True)\n",
    "            \n",
    "            # Convert DateTime to pandas datetime\n",
    "            combined_df['DateTime'] = pd.to_datetime(combined_df['DateTime'])\n",
    "            \n",
    "            # If we have hourly data, aggregate to daily\n",
    "            if 'Hourly' in combined_df.get('ReportingFrequency', []).unique():\n",
    "                combined_df = self._aggregate_to_daily(combined_df)\n",
    "            \n",
    "            return combined_df\n",
    "        \n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    def _load_all_variant_data(self, variant_map: Dict[str, List[str]]) -> Dict[str, pd.DataFrame]:\n",
    "        \"\"\"Load all variant simulation data\"\"\"\n",
    "        variant_data_dict = {}\n",
    "        \n",
    "        # First try the modified results directory\n",
    "        modified_results_path = self.job_output_dir / 'Modified_Sim_Results'\n",
    "        if modified_results_path.exists():\n",
    "            # Load from SQL files directly\n",
    "            for sql_file in modified_results_path.glob('**/*.sql'):\n",
    "                # Extract variant info from filename\n",
    "                variant_id = self._extract_variant_from_filename(sql_file.name)\n",
    "                if variant_id and variant_id != 'base':\n",
    "                    print(f\"  Loading variant {variant_id} from SQL...\")\n",
    "                    variant_df = self._extract_sql_data(sql_file)\n",
    "                    if not variant_df.empty:\n",
    "                        variant_df['variant_id'] = variant_id\n",
    "                        variant_data_dict[variant_id] = variant_df\n",
    "        \n",
    "        # Also check parsed modified results\n",
    "        variant_paths = [\n",
    "            self.modified_parsed_dir / 'sql_results' / 'timeseries' / 'hourly',\n",
    "            self.modified_parsed_dir / 'sql_results' / 'timeseries' / 'aggregated' / 'daily',\n",
    "            self.modified_parsed_dir / 'timeseries' / 'variants' / 'daily'\n",
    "        ]\n",
    "        \n",
    "        for variant_path in variant_paths:\n",
    "            if variant_path.exists():\n",
    "                for parquet_file in variant_path.glob('*.parquet'):\n",
    "                    try:\n",
    "                        df = pd.read_parquet(parquet_file)\n",
    "                        \n",
    "                        # Group by variant_id if column exists\n",
    "                        if 'variant_id' in df.columns:\n",
    "                            for vid in df['variant_id'].unique():\n",
    "                                if vid != 'base':\n",
    "                                    variant_df = df[df['variant_id'] == vid].copy()\n",
    "                                    \n",
    "                                    if vid not in variant_data_dict:\n",
    "                                        variant_data_dict[vid] = []\n",
    "                                    variant_data_dict[vid].append(variant_df)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error loading {parquet_file}: {e}\")\n",
    "        \n",
    "        # Consolidate variant data\n",
    "        consolidated_variants = {}\n",
    "        for variant_id, data_list in variant_data_dict.items():\n",
    "            if isinstance(data_list, list):\n",
    "                consolidated_variants[variant_id] = pd.concat(data_list, ignore_index=True)\n",
    "                # Aggregate to daily if needed\n",
    "                if 'DateTime' in consolidated_variants[variant_id].columns:\n",
    "                    consolidated_variants[variant_id]['DateTime'] = pd.to_datetime(\n",
    "                        consolidated_variants[variant_id]['DateTime']\n",
    "                    )\n",
    "                    consolidated_variants[variant_id] = self._aggregate_to_daily(\n",
    "                        consolidated_variants[variant_id]\n",
    "                    )\n",
    "            else:\n",
    "                consolidated_variants[variant_id] = data_list\n",
    "        \n",
    "        return consolidated_variants\n",
    "    \n",
    "    def _convert_to_semi_wide(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Convert long format to semi-wide format with dates as columns\"\"\"\n",
    "        # Ensure DateTime is datetime type\n",
    "        df['DateTime'] = pd.to_datetime(df['DateTime'])\n",
    "        \n",
    "        # Create date string for column names\n",
    "        df['date_str'] = df['DateTime'].dt.strftime('%Y-%m-%d')\n",
    "        \n",
    "        # Define index columns (everything except date and value)\n",
    "        index_cols = ['building_id', 'variant_id', 'Variable', 'category', 'Zone', 'Units']\n",
    "        \n",
    "        # Make sure all index columns exist\n",
    "        for col in index_cols:\n",
    "            if col not in df.columns:\n",
    "                if col == 'variant_id':\n",
    "                    df[col] = 'base'\n",
    "                elif col == 'Zone':\n",
    "                    df[col] = df.get('KeyValue', 'Building')\n",
    "                elif col == 'Units':\n",
    "                    df[col] = df.get('units', '')\n",
    "                else:\n",
    "                    df[col] = ''\n",
    "        \n",
    "        # Remove None/null values from Zone\n",
    "        df['Zone'] = df['Zone'].fillna('Building')\n",
    "        \n",
    "        # Filter to only the columns we need\n",
    "        pivot_df = df[index_cols + ['date_str', 'Value']].copy()\n",
    "        \n",
    "        # Pivot the data\n",
    "        semi_wide = pivot_df.pivot_table(\n",
    "            index=index_cols,\n",
    "            columns='date_str',\n",
    "            values='Value',\n",
    "            aggfunc='mean'  # Handle any duplicates\n",
    "        ).reset_index()\n",
    "        \n",
    "        # Rename columns to match desired format (VariableName instead of Variable)\n",
    "        semi_wide = semi_wide.rename(columns={'Variable': 'VariableName'})\n",
    "        \n",
    "        # Reorder columns to put date columns at the end\n",
    "        non_date_cols = [col for col in semi_wide.columns if not col.startswith('20')]\n",
    "        date_cols = sorted([col for col in semi_wide.columns if col.startswith('20')])\n",
    "        \n",
    "        semi_wide = semi_wide[non_date_cols + date_cols]\n",
    "        \n",
    "        return semi_wide\n",
    "    \n",
    "    def _create_variant_comparison(self, base_df: pd.DataFrame, \n",
    "                                 variant_dict: Dict[str, pd.DataFrame], \n",
    "                                 variable: str) -> pd.DataFrame:\n",
    "        \"\"\"Create comparison dataframe for a specific variable\"\"\"\n",
    "        # Filter base data for this variable\n",
    "        base_var = base_df[base_df['Variable'] == variable].copy()\n",
    "        \n",
    "        if base_var.empty:\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        # Prepare base data\n",
    "        base_var['DateTime'] = pd.to_datetime(base_var['DateTime'])\n",
    "        base_var = base_var.rename(columns={'Value': 'base_value'})\n",
    "        \n",
    "        # Get necessary columns\n",
    "        merge_cols = ['DateTime', 'building_id', 'Zone']\n",
    "        keep_cols = merge_cols + ['category', 'Units', 'base_value']\n",
    "        \n",
    "        # Start with base data\n",
    "        result = base_var[keep_cols].copy()\n",
    "        \n",
    "        # Add each variant\n",
    "        for variant_id, variant_df in variant_dict.items():\n",
    "            # Filter variant data for this variable\n",
    "            variant_var = variant_df[variant_df['Variable'] == variable].copy()\n",
    "            \n",
    "            if not variant_var.empty:\n",
    "                variant_var['DateTime'] = pd.to_datetime(variant_var['DateTime'])\n",
    "                variant_var = variant_var.rename(columns={'Value': f'{variant_id}_value'})\n",
    "                \n",
    "                # Merge variant data\n",
    "                result = result.merge(\n",
    "                    variant_var[merge_cols + [f'{variant_id}_value']],\n",
    "                    on=merge_cols,\n",
    "                    how='outer'\n",
    "                )\n",
    "        \n",
    "        # Add variable name and reorder columns\n",
    "        result['variable_name'] = variable\n",
    "        \n",
    "        # Ensure we have timestamp column (rename DateTime)\n",
    "        result = result.rename(columns={'DateTime': 'timestamp'})\n",
    "        \n",
    "        # Reorder columns\n",
    "        col_order = ['timestamp', 'building_id', 'Zone', 'variable_name', 'category', 'Units', 'base_value']\n",
    "        variant_cols = sorted([col for col in result.columns if col.endswith('_value') and col != 'base_value'])\n",
    "        col_order.extend(variant_cols)\n",
    "        \n",
    "        # Only keep columns that exist\n",
    "        col_order = [col for col in col_order if col in result.columns]\n",
    "        \n",
    "        return result[col_order].sort_values(['timestamp', 'building_id', 'Zone']).reset_index(drop=True)\n",
    "    \n",
    "    def _aggregate_to_daily(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Aggregate data to daily frequency\"\"\"\n",
    "        if 'DateTime' not in df.columns:\n",
    "            return df\n",
    "        \n",
    "        # Determine aggregation method based on variable\n",
    "        df['agg_method'] = df['Variable'].apply(lambda x: 'sum' if 'Energy' in x else 'mean')\n",
    "        \n",
    "        # Group by all columns except Value and DateTime\n",
    "        group_cols = [col for col in df.columns if col not in ['Value', 'DateTime', 'agg_method']]\n",
    "        \n",
    "        # Aggregate separately for sum and mean variables\n",
    "        sum_df = df[df['agg_method'] == 'sum'].copy()\n",
    "        mean_df = df[df['agg_method'] == 'mean'].copy()\n",
    "        \n",
    "        aggregated = []\n",
    "        \n",
    "        if not sum_df.empty:\n",
    "            sum_df['Date'] = sum_df['DateTime'].dt.date\n",
    "            sum_agg = sum_df.groupby(group_cols + ['Date'])['Value'].sum().reset_index()\n",
    "            sum_agg['DateTime'] = pd.to_datetime(sum_agg['Date'])\n",
    "            sum_agg = sum_agg.drop('Date', axis=1)\n",
    "            aggregated.append(sum_agg)\n",
    "        \n",
    "        if not mean_df.empty:\n",
    "            mean_df['Date'] = mean_df['DateTime'].dt.date\n",
    "            mean_agg = mean_df.groupby(group_cols + ['Date'])['Value'].mean().reset_index()\n",
    "            mean_agg['DateTime'] = pd.to_datetime(mean_agg['Date'])\n",
    "            mean_agg = mean_agg.drop('Date', axis=1)\n",
    "            aggregated.append(mean_agg)\n",
    "        \n",
    "        if aggregated:\n",
    "            result = pd.concat(aggregated, ignore_index=True)\n",
    "            return result.drop('agg_method', axis=1) if 'agg_method' in result.columns else result\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def _get_variant_mapping(self) -> Dict[str, List[str]]:\n",
    "        \"\"\"Get mapping of buildings to their variants\"\"\"\n",
    "        variant_map = {}\n",
    "        \n",
    "        # Check modified IDFs\n",
    "        modified_idfs_dir = self.job_output_dir / 'modified_idfs'\n",
    "        if modified_idfs_dir.exists():\n",
    "            for idf_file in modified_idfs_dir.glob('*.idf'):\n",
    "                parts = idf_file.stem.split('_')\n",
    "                if len(parts) >= 4 and parts[0] == 'building' and parts[2] == 'variant':\n",
    "                    building_id = parts[1]\n",
    "                    variant_num = parts[3]\n",
    "                    \n",
    "                    if building_id not in variant_map:\n",
    "                        variant_map[building_id] = []\n",
    "                    variant_map[building_id].append(f\"variant_{variant_num}\")\n",
    "        \n",
    "        return variant_map\n",
    "    \n",
    "    def _extract_variant_from_filename(self, filename: str) -> str:\n",
    "        \"\"\"Extract variant ID from filename\"\"\"\n",
    "        import re\n",
    "        \n",
    "        # Pattern: simulation_bldgX_XXXXXX.sql where X is variant number\n",
    "        match = re.search(r'simulation_bldg(\\d+)_\\d+\\.sql', filename)\n",
    "        if match:\n",
    "            variant_num = match.group(1)\n",
    "            if variant_num == '0':\n",
    "                return 'base'\n",
    "            return f\"variant_{int(variant_num) - 1}\"  # Adjust numbering\n",
    "        \n",
    "        # Pattern: building_XXXXXX_variant_X.sql\n",
    "        match = re.search(r'building_\\d+_variant_(\\d+)', filename)\n",
    "        if match:\n",
    "            return f\"variant_{match.group(1)}\"\n",
    "        \n",
    "        return 'base'\n",
    "    \n",
    "    def _extract_sql_data(self, sql_path: Path) -> pd.DataFrame:\n",
    "        \"\"\"Extract data directly from SQL file\"\"\"\n",
    "        try:\n",
    "            conn = sqlite3.connect(str(sql_path))\n",
    "            \n",
    "            # Query for daily data\n",
    "            query = \"\"\"\n",
    "            SELECT \n",
    "                t.TimeIndex,\n",
    "                datetime(printf('%04d-%02d-%02d', t.Year, t.Month, t.Day)) as DateTime,\n",
    "                rdd.Name as Variable,\n",
    "                rdd.KeyValue as Zone,\n",
    "                rd.Value,\n",
    "                rdd.Units,\n",
    "                rdd.ReportingFrequency\n",
    "            FROM ReportData rd\n",
    "            JOIN Time t ON rd.TimeIndex = t.TimeIndex\n",
    "            JOIN ReportDataDictionary rdd ON rd.ReportDataDictionaryIndex = rdd.ReportDataDictionaryIndex\n",
    "            WHERE t.EnvironmentPeriodIndex = 3\n",
    "            AND rdd.ReportingFrequency IN ('Daily', 'Hourly')\n",
    "            \"\"\"\n",
    "            \n",
    "            df = pd.read_sql_query(query, conn)\n",
    "            conn.close()\n",
    "            \n",
    "            if not df.empty:\n",
    "                df['DateTime'] = pd.to_datetime(df['DateTime'])\n",
    "                \n",
    "                # Extract building ID from filename\n",
    "                parts = sql_path.stem.split('_')\n",
    "                if len(parts) >= 3:\n",
    "                    df['building_id'] = parts[2]  # Assuming pattern like simulation_bldgX_BUILDINGID\n",
    "                \n",
    "                # Add category\n",
    "                df['category'] = df['Variable'].apply(self._categorize_variable)\n",
    "                \n",
    "                return df\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting from {sql_path}: {e}\")\n",
    "        \n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    def _categorize_variable(self, variable_name: str) -> str:\n",
    "        \"\"\"Categorize a variable based on its name\"\"\"\n",
    "        var_lower = variable_name.lower()\n",
    "        \n",
    "        if any(meter in variable_name for meter in ['Electricity:', 'Gas:', 'Cooling:', 'Heating:']):\n",
    "            return 'energy_meters'\n",
    "        elif 'zone' in var_lower and any(word in var_lower for word in ['temperature', 'humidity']):\n",
    "            return 'geometry'\n",
    "        elif 'surface' in var_lower:\n",
    "            return 'materials'\n",
    "        elif 'water heater' in var_lower:\n",
    "            return 'dhw'\n",
    "        elif 'equipment' in var_lower:\n",
    "            return 'equipment'\n",
    "        elif 'lights' in var_lower:\n",
    "            return 'lighting'\n",
    "        elif 'ventilation' in var_lower:\n",
    "            return 'ventilation'\n",
    "        elif 'infiltration' in var_lower:\n",
    "            return 'infiltration'\n",
    "        else:\n",
    "            return 'other'\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    job_output_dir = r\"output\\221d33e9-f628-4f1e-86e5-3466f6d140a3\"\n",
    "    \n",
    "    transformer = EnhancedSQLTransformer(job_output_dir)\n",
    "    \n",
    "    # Transform base data to semi-wide format\n",
    "    base_semi_wide = transformer.transform_base_to_semi_wide()\n",
    "    \n",
    "    # Transform variant data to comparison format\n",
    "    transformer.transform_variants_to_comparison()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "473ea193",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "DATA STRUCTURE DIAGNOSIS\n",
      "================================================================================\n",
      "\n",
      "1. IDF FILE ANALYSIS:\n",
      "----------------------------------------\n",
      "Base buildings found: ['4136733', '4136737', '4136738']\n",
      "\n",
      "Modified buildings and their variants:\n",
      "  Building 4136733: 19 variants - ['variant_0', 'variant_1', 'variant_10', 'variant_11', 'variant_12']...\n",
      "\n",
      "Base-only buildings (no modifications): ['4136737', '4136738']\n",
      "\n",
      "\n",
      "2. SQL FILE ANALYSIS:\n",
      "----------------------------------------\n",
      "\n",
      "Year 2020:\n",
      "  simulation_bldg0_4136733.sql -> Building 4136733, base\n",
      "  simulation_bldg1_4136737.sql -> Building 4136737, variant_0\n",
      "  simulation_bldg2_4136738.sql -> Building 4136738, variant_1\n",
      "\n",
      "Modified SQL files:\n",
      "  simulation_bldg0_4136733.sql -> Building 4136733, base\n",
      "  simulation_bldg10_4136733.sql -> Building 4136733, variant_9\n",
      "  simulation_bldg11_4136733.sql -> Building 4136733, variant_10\n",
      "  simulation_bldg12_4136733.sql -> Building 4136733, variant_11\n",
      "  simulation_bldg13_4136733.sql -> Building 4136733, variant_12\n",
      "  simulation_bldg14_4136733.sql -> Building 4136733, variant_13\n",
      "  simulation_bldg15_4136733.sql -> Building 4136733, variant_14\n",
      "  simulation_bldg16_4136733.sql -> Building 4136733, variant_15\n",
      "  simulation_bldg17_4136733.sql -> Building 4136733, variant_16\n",
      "  simulation_bldg18_4136733.sql -> Building 4136733, variant_17\n",
      "\n",
      "\n",
      "3. CURRENT PARSED DATA ISSUES:\n",
      "----------------------------------------\n",
      "\n",
      "Base data file analysis:\n",
      "  Total rows: 3\n",
      "  Unique buildings: ['4136733' '4136737' '4136738']\n",
      "  Unique variant_ids: ['base' 'variant_1' 'variant_2']\n",
      "\n",
      "\n",
      "4. DATA FREQUENCY ANALYSIS:\n",
      "----------------------------------------\n",
      "\n",
      "Analyzing simulation_bldg0_4136733.sql:\n",
      "\n",
      "Variables by frequency:\n",
      "  Daily: 1 variables\n",
      "  Monthly: 40 variables\n",
      "\n",
      "\n",
      "CORRECT MAPPING:\n",
      "----------------------------------------\n",
      "Base buildings: ['4136733']\n",
      "\n",
      "Buildings with variants:\n",
      "  Building 4136737: ['variant_0']\n",
      "  Building 4136738: ['variant_1']\n",
      "  Building 4136733: ['variant_9', 'variant_10', 'variant_11', 'variant_12', 'variant_13', 'variant_14', 'variant_15', 'variant_16', 'variant_17', 'variant_0', 'variant_1', 'variant_2', 'variant_3', 'variant_4', 'variant_5', 'variant_6', 'variant_7', 'variant_8']\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Diagnostic tool to understand the data structure and identify issues\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import re\n",
    "import sqlite3\n",
    "\n",
    "def diagnose_data_structure(job_output_dir: str):\n",
    "    \"\"\"Diagnose the data structure to understand base vs variant relationships\"\"\"\n",
    "    \n",
    "    job_path = Path(job_output_dir)\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(\"DATA STRUCTURE DIAGNOSIS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # 1. Analyze IDF files to understand base vs modified buildings\n",
    "    print(\"\\n1. IDF FILE ANALYSIS:\")\n",
    "    print(\"-\"*40)\n",
    "    \n",
    "    # Base IDFs\n",
    "    base_idfs_dir = job_path / 'output_IDFs'\n",
    "    base_buildings = set()\n",
    "    if base_idfs_dir.exists():\n",
    "        for idf in base_idfs_dir.glob('*.idf'):\n",
    "            # Extract building ID from filename (building_XXXXX.idf)\n",
    "            match = re.search(r'building_(\\d+)\\.idf', idf.name)\n",
    "            if match:\n",
    "                base_buildings.add(match.group(1))\n",
    "        print(f\"Base buildings found: {sorted(base_buildings)}\")\n",
    "    \n",
    "    # Modified IDFs\n",
    "    modified_idfs_dir = job_path / 'modified_idfs'\n",
    "    modified_buildings = {}\n",
    "    if modified_idfs_dir.exists():\n",
    "        for idf in modified_idfs_dir.glob('*.idf'):\n",
    "            # Extract building ID and variant (building_XXXXX_variant_N.idf)\n",
    "            match = re.search(r'building_(\\d+)_variant_(\\d+)\\.idf', idf.name)\n",
    "            if match:\n",
    "                building_id = match.group(1)\n",
    "                variant_num = match.group(2)\n",
    "                if building_id not in modified_buildings:\n",
    "                    modified_buildings[building_id] = []\n",
    "                modified_buildings[building_id].append(f\"variant_{variant_num}\")\n",
    "    \n",
    "    print(f\"\\nModified buildings and their variants:\")\n",
    "    for bid, variants in modified_buildings.items():\n",
    "        print(f\"  Building {bid}: {len(variants)} variants - {sorted(variants)[:5]}...\")\n",
    "    \n",
    "    print(f\"\\nBase-only buildings (no modifications): {sorted(base_buildings - set(modified_buildings.keys()))}\")\n",
    "    \n",
    "    # 2. Analyze SQL files and their naming pattern\n",
    "    print(\"\\n\\n2. SQL FILE ANALYSIS:\")\n",
    "    print(\"-\"*40)\n",
    "    \n",
    "    # Base SQL files\n",
    "    base_sql_dir = job_path / 'Sim_Results'\n",
    "    sql_mapping = {}\n",
    "    \n",
    "    if base_sql_dir.exists():\n",
    "        # Check year folders\n",
    "        for year_dir in base_sql_dir.iterdir():\n",
    "            if year_dir.is_dir() and year_dir.name.isdigit():\n",
    "                print(f\"\\nYear {year_dir.name}:\")\n",
    "                for sql in sorted(year_dir.glob('*.sql'))[:10]:\n",
    "                    # Pattern: simulation_bldgX_BUILDINGID.sql\n",
    "                    match = re.search(r'simulation_bldg(\\d+)_(\\d+)\\.sql', sql.name)\n",
    "                    if match:\n",
    "                        bldg_num = match.group(1)\n",
    "                        building_id = match.group(2)\n",
    "                        \n",
    "                        if bldg_num == '0':\n",
    "                            variant = 'base'\n",
    "                        else:\n",
    "                            # bldg1 = variant_0, bldg2 = variant_1, etc.\n",
    "                            variant = f'variant_{int(bldg_num) - 1}'\n",
    "                        \n",
    "                        sql_mapping[sql.name] = {\n",
    "                            'building_id': building_id,\n",
    "                            'variant': variant,\n",
    "                            'path': sql\n",
    "                        }\n",
    "                        print(f\"  {sql.name} -> Building {building_id}, {variant}\")\n",
    "    \n",
    "    # Modified SQL files\n",
    "    modified_sql_dir = job_path / 'Modified_Sim_Results'\n",
    "    if modified_sql_dir.exists():\n",
    "        print(f\"\\nModified SQL files:\")\n",
    "        for sql in sorted(modified_sql_dir.glob('**/*.sql'))[:10]:\n",
    "            match = re.search(r'simulation_bldg(\\d+)_(\\d+)\\.sql', sql.name)\n",
    "            if match:\n",
    "                bldg_num = match.group(1)\n",
    "                building_id = match.group(2)\n",
    "                variant = f'variant_{int(bldg_num) - 1}' if bldg_num != '0' else 'base'\n",
    "                print(f\"  {sql.name} -> Building {building_id}, {variant}\")\n",
    "    \n",
    "    # 3. Check current parsed data\n",
    "    print(\"\\n\\n3. CURRENT PARSED DATA ISSUES:\")\n",
    "    print(\"-\"*40)\n",
    "    \n",
    "    # Check base data\n",
    "    base_parquet = job_path / 'parsed_data' / 'timeseries' / 'base' / 'daily' / 'all_variables.parquet'\n",
    "    if base_parquet.exists():\n",
    "        df = pd.read_parquet(base_parquet)\n",
    "        print(f\"\\nBase data file analysis:\")\n",
    "        print(f\"  Total rows: {len(df)}\")\n",
    "        print(f\"  Unique buildings: {df['building_id'].unique()}\")\n",
    "        print(f\"  Unique variant_ids: {df['variant_id'].unique()}\")\n",
    "        \n",
    "        # Check which buildings shouldn't be in base\n",
    "        for bid in df['building_id'].unique():\n",
    "            if bid not in base_buildings:\n",
    "                print(f\"  WARNING: Building {bid} in base data but not in base IDFs!\")\n",
    "    \n",
    "    # 4. Analyze data frequencies\n",
    "    print(\"\\n\\n4. DATA FREQUENCY ANALYSIS:\")\n",
    "    print(\"-\"*40)\n",
    "    \n",
    "    # Sample a SQL file to check frequencies\n",
    "    sample_sql = None\n",
    "    if base_sql_dir.exists():\n",
    "        for year_dir in base_sql_dir.iterdir():\n",
    "            if year_dir.is_dir() and year_dir.name.isdigit():\n",
    "                sql_files = list(year_dir.glob('*bldg0_*.sql'))\n",
    "                if sql_files:\n",
    "                    sample_sql = sql_files[0]\n",
    "                    break\n",
    "    \n",
    "    if sample_sql:\n",
    "        print(f\"\\nAnalyzing {sample_sql.name}:\")\n",
    "        conn = sqlite3.connect(str(sample_sql))\n",
    "        \n",
    "        freq_query = \"\"\"\n",
    "        SELECT ReportingFrequency, COUNT(DISTINCT Name) as NumVariables\n",
    "        FROM ReportDataDictionary\n",
    "        GROUP BY ReportingFrequency\n",
    "        \"\"\"\n",
    "        freq_df = pd.read_sql_query(freq_query, conn)\n",
    "        print(\"\\nVariables by frequency:\")\n",
    "        for _, row in freq_df.iterrows():\n",
    "            print(f\"  {row['ReportingFrequency']}: {row['NumVariables']} variables\")\n",
    "        \n",
    "        conn.close()\n",
    "    \n",
    "    return {\n",
    "        'base_buildings': base_buildings,\n",
    "        'modified_buildings': modified_buildings,\n",
    "        'sql_mapping': sql_mapping\n",
    "    }\n",
    "\n",
    "def create_correct_mapping(job_output_dir: str):\n",
    "    \"\"\"Create the correct building-variant mapping\"\"\"\n",
    "    \n",
    "    job_path = Path(job_output_dir)\n",
    "    mapping = {\n",
    "        'base_buildings': {},  # building_id -> sql_file\n",
    "        'variant_buildings': {}  # building_id -> {variant_id -> sql_file}\n",
    "    }\n",
    "    \n",
    "    # Map SQL files\n",
    "    sim_results = job_path / 'Sim_Results'\n",
    "    if sim_results.exists():\n",
    "        for year_dir in sim_results.iterdir():\n",
    "            if year_dir.is_dir() and year_dir.name.isdigit():\n",
    "                for sql in year_dir.glob('*.sql'):\n",
    "                    match = re.search(r'simulation_bldg(\\d+)_(\\d+)\\.sql', sql.name)\n",
    "                    if match:\n",
    "                        bldg_num = int(match.group(1))\n",
    "                        building_id = match.group(2)\n",
    "                        \n",
    "                        if bldg_num == 0:\n",
    "                            # This is base data\n",
    "                            mapping['base_buildings'][building_id] = sql\n",
    "                        else:\n",
    "                            # This is variant data\n",
    "                            variant_id = f'variant_{bldg_num - 1}'\n",
    "                            if building_id not in mapping['variant_buildings']:\n",
    "                                mapping['variant_buildings'][building_id] = {}\n",
    "                            mapping['variant_buildings'][building_id][variant_id] = sql\n",
    "    \n",
    "    # Also check Modified_Sim_Results\n",
    "    modified_results = job_path / 'Modified_Sim_Results'\n",
    "    if modified_results.exists():\n",
    "        for sql in modified_results.glob('**/*.sql'):\n",
    "            match = re.search(r'simulation_bldg(\\d+)_(\\d+)\\.sql', sql.name)\n",
    "            if match:\n",
    "                bldg_num = int(match.group(1))\n",
    "                building_id = match.group(2)\n",
    "                \n",
    "                if bldg_num > 0:  # Skip base (bldg0)\n",
    "                    variant_id = f'variant_{bldg_num - 1}'\n",
    "                    if building_id not in mapping['variant_buildings']:\n",
    "                        mapping['variant_buildings'][building_id] = {}\n",
    "                    mapping['variant_buildings'][building_id][variant_id] = sql\n",
    "    \n",
    "    print(\"\\n\\nCORRECT MAPPING:\")\n",
    "    print(\"-\"*40)\n",
    "    print(f\"Base buildings: {list(mapping['base_buildings'].keys())}\")\n",
    "    print(f\"\\nBuildings with variants:\")\n",
    "    for bid, variants in mapping['variant_buildings'].items():\n",
    "        print(f\"  Building {bid}: {list(variants.keys())}\")\n",
    "    \n",
    "    return mapping\n",
    "\n",
    "# Run diagnostics\n",
    "if __name__ == \"__main__\":\n",
    "    job_output_dir = \"output/221d33e9-f628-4f1e-86e5-3466f6d140a3\"\n",
    "    \n",
    "    # Diagnose issues\n",
    "    diagnosis = diagnose_data_structure(job_output_dir)\n",
    "    \n",
    "    # Create correct mapping\n",
    "    correct_mapping = create_correct_mapping(job_output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "18bb6478",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Identified base buildings: ['4136733', '4136737', '4136738']\n",
      "\n",
      "Variant mapping created:\n",
      "  Building 4136733: ['variant_0', 'variant_1', 'variant_10', 'variant_11', 'variant_12', 'variant_13', 'variant_14', 'variant_15', 'variant_16', 'variant_17', 'variant_2', 'variant_3', 'variant_4', 'variant_5', 'variant_6', 'variant_7', 'variant_8', 'variant_9']\n",
      "\n",
      "============================================================\n",
      "Transforming BASE data to semi-wide format\n",
      "============================================================\n",
      "\n",
      "Loading base data from SQL files...\n",
      "  Loading base data for building 4136733 from simulation_bldg0_4136733.sql\n",
      "  Loading base data for building 4136737 from simulation_bldg1_4136737.sql\n",
      "  Loading base data for building 4136738 from simulation_bldg2_4136738.sql\n",
      "\n",
      "Base data loaded:\n",
      "  Total rows: 10779\n",
      "  Buildings: ['4136733', '4136737', '4136738']\n",
      "  Variables: 46\n",
      "  Date range: 2013-01-02 00:00:00 to 2014-01-01 00:00:00\n",
      "\n",
      "Saved semi-wide base data to: output\\221d33e9-f628-4f1e-86e5-3466f6d140a3\\parsed_data\\timeseries\\base\\daily\\all_variables.parquet\n",
      "  Shape: (597, 401)\n",
      "  Buildings: ['4136733', '4136737', '4136738']\n",
      "  Variables: 29\n",
      "  Date columns: 395\n",
      "\n",
      "============================================================\n",
      "Transforming VARIANT data to comparison format\n",
      "============================================================\n",
      "\n",
      "Processing variants for building 4136733...\n",
      "    Loading base from simulation_bldg0_4136733.sql\n",
      "  Loading variant_0 from simulation_bldg1_4136733.sql\n",
      "  Loading variant_1 from simulation_bldg2_4136733.sql\n",
      "  Loading variant_10 from simulation_bldg11_4136733.sql\n",
      "  Loading variant_11 from simulation_bldg12_4136733.sql\n",
      "  Loading variant_12 from simulation_bldg13_4136733.sql\n",
      "  Loading variant_13 from simulation_bldg14_4136733.sql\n",
      "  Loading variant_14 from simulation_bldg15_4136733.sql\n",
      "  Loading variant_15 from simulation_bldg16_4136733.sql\n",
      "  Loading variant_16 from simulation_bldg17_4136733.sql\n",
      "  Loading variant_17 from simulation_bldg18_4136733.sql\n",
      "  Loading variant_2 from simulation_bldg3_4136733.sql\n",
      "  Loading variant_3 from simulation_bldg4_4136733.sql\n",
      "  Loading variant_4 from simulation_bldg5_4136733.sql\n",
      "  Loading variant_5 from simulation_bldg6_4136733.sql\n",
      "  Loading variant_6 from simulation_bldg7_4136733.sql\n",
      "  Loading variant_7 from simulation_bldg8_4136733.sql\n",
      "  Loading variant_8 from simulation_bldg9_4136733.sql\n",
      "  Loading variant_9 from simulation_bldg10_4136733.sql\n",
      "\n",
      "  Creating comparison files for 39 variables...\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'Zone'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32mD:\\Drives\\Temp\\ipykernel_38592\\4180569878.py\u001b[0m in \u001b[0;36m?\u001b[1;34m()\u001b[0m\n\u001b[0;32m    589\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    590\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpathlib\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mPath\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    591\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtyping\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mList\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTuple\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSet\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    592\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msqlite3\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 593\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mdatetime\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    594\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    595\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    596\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mFixedSQLTransformer\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Drives\\Temp\\ipykernel_38592\\4180569878.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    220\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mvariant_daily\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    221\u001b[0m                     \u001b[0mvariant_daily_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mvid\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvariant_daily\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    222\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    223\u001b[0m             \u001b[1;31m# Create comparison files for each variable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 224\u001b[1;33m             self._create_building_variant_comparisons(\n\u001b[0m\u001b[0;32m    225\u001b[0m                 \u001b[0mbuilding_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbase_daily\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvariant_daily_dict\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    226\u001b[0m             )\n",
      "\u001b[1;32mD:\\Drives\\Temp\\ipykernel_38592\\4180569878.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, building_id, base_df, variant_dict)\u001b[0m\n\u001b[0;32m    342\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"\\n  Creating comparison files for {len(all_variables)} variables...\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    343\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    344\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mvariable\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_variables\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    345\u001b[0m             \u001b[1;31m# Create comparison for this variable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 346\u001b[1;33m             comparison_df = self._create_single_variable_comparison(\n\u001b[0m\u001b[0;32m    347\u001b[0m                 \u001b[0mvariable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuilding_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbase_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvariant_dict\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    348\u001b[0m             )\n\u001b[0;32m    349\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Drives\\Temp\\ipykernel_38592\\4180569878.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, variable, building_id, base_df, variant_dict)\u001b[0m\n\u001b[0;32m    434\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    435\u001b[0m         \u001b[0mfinal_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfirst_cols\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mvalue_cols\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mother_cols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    436\u001b[0m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfinal_cols\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    437\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 438\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'timestamp'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Zone'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32md:\\New folder (2)\\Anaconda\\DDsaie\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, by, axis, ascending, inplace, kind, na_position, ignore_index, key)\u001b[0m\n\u001b[0;32m   7168\u001b[0m                 \u001b[1;34mf\"Length of ascending ({len(ascending)})\"\u001b[0m  \u001b[1;31m# type: ignore[arg-type]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   7169\u001b[0m                 \u001b[1;34mf\" != length of by ({len(by)})\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   7170\u001b[0m             )\n\u001b[0;32m   7171\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mby\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 7172\u001b[1;33m             \u001b[0mkeys\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_label_or_level_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mby\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   7173\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   7174\u001b[0m             \u001b[1;31m# need to rewrap columns in Series to apply key function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   7175\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\New folder (2)\\Anaconda\\DDsaie\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(.0)\u001b[0m\n\u001b[1;32m-> 7172\u001b[1;33m         \u001b[1;33m...\u001b[0m     \u001b[0mkey\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex_natsorted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"time\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32md:\\New folder (2)\\Anaconda\\DDsaie\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1907\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mxs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mother_axes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1908\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_level_reference\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1909\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_level_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1910\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1911\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1912\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1913\u001b[0m         \u001b[1;31m# Check for duplicates\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1914\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Zone'"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Fixed SQL Data Transformer with correct base/variant identification logic\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Tuple, Set\n",
    "import sqlite3\n",
    "from datetime import datetime\n",
    "import re\n",
    "\n",
    "class FixedSQLTransformer:\n",
    "    \"\"\"Transform SQL data with correct base/variant identification\"\"\"\n",
    "    \n",
    "    def __init__(self, job_output_dir: str):\n",
    "        self.job_output_dir = Path(job_output_dir)\n",
    "        self.base_parsed_dir = self.job_output_dir / 'parsed_data'\n",
    "        self.modified_parsed_dir = self.job_output_dir / 'parsed_modified_results'\n",
    "        \n",
    "        # Identify base buildings and buildings with variants\n",
    "        self.base_buildings = self._identify_base_buildings()\n",
    "        self.variant_mapping = self._create_variant_mapping()\n",
    "        \n",
    "    def _identify_base_buildings(self) -> Set[str]:\n",
    "        \"\"\"Identify which buildings are base (from output_IDFs)\"\"\"\n",
    "        base_buildings = set()\n",
    "        base_idfs_dir = self.job_output_dir / 'output_IDFs'\n",
    "        \n",
    "        if base_idfs_dir.exists():\n",
    "            for idf in base_idfs_dir.glob('building_*.idf'):\n",
    "                match = re.search(r'building_(\\d+)\\.idf', idf.name)\n",
    "                if match:\n",
    "                    base_buildings.add(match.group(1))\n",
    "        \n",
    "        print(f\"Identified base buildings: {sorted(base_buildings)}\")\n",
    "        return base_buildings\n",
    "    \n",
    "    def _create_variant_mapping(self) -> Dict[str, Dict[str, Path]]:\n",
    "        \"\"\"Create mapping of building -> variant -> SQL file path\"\"\"\n",
    "        variant_mapping = {}\n",
    "        \n",
    "        # Check modified IDFs to see which buildings have variants\n",
    "        modified_idfs_dir = self.job_output_dir / 'modified_idfs'\n",
    "        if modified_idfs_dir.exists():\n",
    "            for idf in modified_idfs_dir.glob('building_*_variant_*.idf'):\n",
    "                match = re.search(r'building_(\\d+)_variant_(\\d+)\\.idf', idf.name)\n",
    "                if match:\n",
    "                    building_id = match.group(1)\n",
    "                    variant_num = match.group(2)\n",
    "                    \n",
    "                    if building_id not in variant_mapping:\n",
    "                        variant_mapping[building_id] = {}\n",
    "                    \n",
    "                    # Find corresponding SQL file in Modified_Sim_Results\n",
    "                    # Note: SQL uses bldg1 for variant_0, bldg2 for variant_1, etc.\n",
    "                    sql_bldg_num = int(variant_num) + 1\n",
    "                    sql_pattern = f\"simulation_bldg{sql_bldg_num}_{building_id}.sql\"\n",
    "                    \n",
    "                    # Search for SQL file\n",
    "                    modified_sql_dir = self.job_output_dir / 'Modified_Sim_Results'\n",
    "                    if modified_sql_dir.exists():\n",
    "                        for sql_file in modified_sql_dir.glob(f\"**/{sql_pattern}\"):\n",
    "                            variant_mapping[building_id][f'variant_{variant_num}'] = sql_file\n",
    "                            break\n",
    "        \n",
    "        print(f\"\\nVariant mapping created:\")\n",
    "        for bid, variants in variant_mapping.items():\n",
    "            print(f\"  Building {bid}: {list(variants.keys())}\")\n",
    "        \n",
    "        return variant_mapping\n",
    "    \n",
    "    def transform_base_to_semi_wide(self):\n",
    "        \"\"\"Transform base simulation data to semi-wide format\"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"Transforming BASE data to semi-wide format\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # Load only actual base data\n",
    "        base_data = self._load_true_base_data()\n",
    "        \n",
    "        if base_data.empty:\n",
    "            print(\"No base data found!\")\n",
    "            return None\n",
    "        \n",
    "        print(f\"\\nBase data loaded:\")\n",
    "        print(f\"  Total rows: {len(base_data)}\")\n",
    "        print(f\"  Buildings: {sorted(base_data['building_id'].unique())}\")\n",
    "        print(f\"  Variables: {len(base_data['Variable'].unique())}\")\n",
    "        print(f\"  Date range: {base_data['DateTime'].min()} to {base_data['DateTime'].max()}\")\n",
    "        \n",
    "        # Handle different frequencies\n",
    "        daily_data = self._prepare_daily_data(base_data)\n",
    "        \n",
    "        if daily_data.empty:\n",
    "            print(\"No daily data available after processing!\")\n",
    "            return None\n",
    "        \n",
    "        # Convert to semi-wide format\n",
    "        semi_wide_df = self._convert_to_semi_wide(daily_data)\n",
    "        \n",
    "        # Save the transformed data\n",
    "        output_path = self.base_parsed_dir / 'timeseries' / 'base' / 'daily'\n",
    "        output_path.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        output_file = output_path / 'all_variables.parquet'\n",
    "        semi_wide_df.to_parquet(output_file, index=False)\n",
    "        \n",
    "        print(f\"\\nSaved semi-wide base data to: {output_file}\")\n",
    "        print(f\"  Shape: {semi_wide_df.shape}\")\n",
    "        print(f\"  Buildings: {sorted(semi_wide_df['building_id'].unique())}\")\n",
    "        print(f\"  Variables: {semi_wide_df['VariableName'].nunique()}\")\n",
    "        print(f\"  Date columns: {len([c for c in semi_wide_df.columns if c.startswith('20')])}\")\n",
    "        \n",
    "        return semi_wide_df\n",
    "    \n",
    "    def _load_true_base_data(self) -> pd.DataFrame:\n",
    "        \"\"\"Load only TRUE base simulation data\"\"\"\n",
    "        all_base_data = []\n",
    "        \n",
    "        # Load from Sim_Results (base simulations)\n",
    "        sim_results_dir = self.job_output_dir / 'Sim_Results'\n",
    "        if sim_results_dir.exists():\n",
    "            print(\"\\nLoading base data from SQL files...\")\n",
    "            \n",
    "            for year_dir in sim_results_dir.iterdir():\n",
    "                if year_dir.is_dir() and year_dir.name.isdigit():\n",
    "                    for sql_file in year_dir.glob('*.sql'):\n",
    "                        # Extract building ID from filename\n",
    "                        match = re.search(r'simulation_bldg\\d+_(\\d+)\\.sql', sql_file.name)\n",
    "                        if match:\n",
    "                            building_id = match.group(1)\n",
    "                            \n",
    "                            # Only process if it's a base building\n",
    "                            if building_id in self.base_buildings:\n",
    "                                print(f\"  Loading base data for building {building_id} from {sql_file.name}\")\n",
    "                                df = self._extract_sql_data(sql_file, building_id, 'base')\n",
    "                                if not df.empty:\n",
    "                                    all_base_data.append(df)\n",
    "        \n",
    "        # Also check parsed data directories (if already parsed)\n",
    "        parsed_paths = [\n",
    "            self.base_parsed_dir / 'sql_results' / 'timeseries' / 'hourly',\n",
    "            self.base_parsed_dir / 'sql_results' / 'timeseries' / 'aggregated' / 'daily',\n",
    "            self.base_parsed_dir / 'sql_results' / 'timeseries' / 'raw' / 'daily'\n",
    "        ]\n",
    "        \n",
    "        for parsed_path in parsed_paths:\n",
    "            if parsed_path.exists():\n",
    "                for parquet_file in parsed_path.glob('*.parquet'):\n",
    "                    try:\n",
    "                        df = pd.read_parquet(parquet_file)\n",
    "                        \n",
    "                        # Filter to only base buildings\n",
    "                        if 'building_id' in df.columns:\n",
    "                            df = df[df['building_id'].isin(self.base_buildings)]\n",
    "                            \n",
    "                            # Ensure variant_id is 'base'\n",
    "                            df['variant_id'] = 'base'\n",
    "                            \n",
    "                            if not df.empty:\n",
    "                                all_base_data.append(df)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error loading {parquet_file}: {e}\")\n",
    "        \n",
    "        if all_base_data:\n",
    "            combined_df = pd.concat(all_base_data, ignore_index=True)\n",
    "            \n",
    "            # Ensure DateTime is proper format\n",
    "            if 'DateTime' in combined_df.columns:\n",
    "                combined_df['DateTime'] = pd.to_datetime(combined_df['DateTime'])\n",
    "            \n",
    "            # Remove duplicates\n",
    "            if all(['DateTime', 'building_id', 'Variable', 'Zone'] == col or col in combined_df.columns \n",
    "                   for col in ['DateTime', 'building_id', 'Variable', 'Zone']):\n",
    "                combined_df = combined_df.drop_duplicates(\n",
    "                    subset=['DateTime', 'building_id', 'Variable', 'Zone'],\n",
    "                    keep='last'\n",
    "                )\n",
    "            \n",
    "            return combined_df\n",
    "        \n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    def transform_variants_to_comparison(self):\n",
    "        \"\"\"Transform variant data to comparison format\"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"Transforming VARIANT data to comparison format\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # Process each building that has variants\n",
    "        for building_id, variant_files in self.variant_mapping.items():\n",
    "            print(f\"\\nProcessing variants for building {building_id}...\")\n",
    "            \n",
    "            # Load base data for this building\n",
    "            base_data = self._load_building_base_data(building_id)\n",
    "            \n",
    "            if base_data.empty:\n",
    "                print(f\"  WARNING: No base data found for building {building_id}\")\n",
    "                continue\n",
    "            \n",
    "            # Load variant data\n",
    "            variant_data_dict = {}\n",
    "            for variant_id, sql_path in variant_files.items():\n",
    "                print(f\"  Loading {variant_id} from {sql_path.name}\")\n",
    "                variant_df = self._extract_sql_data(sql_path, building_id, variant_id)\n",
    "                \n",
    "                if not variant_df.empty:\n",
    "                    variant_data_dict[variant_id] = variant_df\n",
    "            \n",
    "            if not variant_data_dict:\n",
    "                print(f\"  No variant data loaded for building {building_id}\")\n",
    "                continue\n",
    "            \n",
    "            # Prepare daily data for all\n",
    "            base_daily = self._prepare_daily_data(base_data)\n",
    "            variant_daily_dict = {}\n",
    "            for vid, vdf in variant_data_dict.items():\n",
    "                variant_daily = self._prepare_daily_data(vdf)\n",
    "                if not variant_daily.empty:\n",
    "                    variant_daily_dict[vid] = variant_daily\n",
    "            \n",
    "            # Create comparison files for each variable\n",
    "            self._create_building_variant_comparisons(\n",
    "                building_id, base_daily, variant_daily_dict\n",
    "            )\n",
    "    \n",
    "    def _load_building_base_data(self, building_id: str) -> pd.DataFrame:\n",
    "        \"\"\"Load base data for a specific building\"\"\"\n",
    "        # First try SQL file\n",
    "        sim_results_dir = self.job_output_dir / 'Sim_Results'\n",
    "        if sim_results_dir.exists():\n",
    "            for year_dir in sim_results_dir.iterdir():\n",
    "                if year_dir.is_dir() and year_dir.name.isdigit():\n",
    "                    # Look for this building's base SQL file\n",
    "                    for sql_file in year_dir.glob(f'*_{building_id}.sql'):\n",
    "                        print(f\"    Loading base from {sql_file.name}\")\n",
    "                        return self._extract_sql_data(sql_file, building_id, 'base')\n",
    "        \n",
    "        # Fallback to parsed data\n",
    "        return self._load_true_base_data()[\n",
    "            lambda df: df['building_id'] == building_id if not df.empty else df\n",
    "        ]\n",
    "    \n",
    "    def _prepare_daily_data(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Prepare data at daily frequency, handling monthly data appropriately\"\"\"\n",
    "        if df.empty:\n",
    "            return df\n",
    "        \n",
    "        df = df.copy()\n",
    "        \n",
    "        # Separate by frequency\n",
    "        daily_mask = df['ReportingFrequency'] == 'Daily'\n",
    "        monthly_mask = df['ReportingFrequency'] == 'Monthly'\n",
    "        hourly_mask = df['ReportingFrequency'] == 'Hourly'\n",
    "        \n",
    "        result_dfs = []\n",
    "        \n",
    "        # Keep daily data as-is\n",
    "        if daily_mask.any():\n",
    "            result_dfs.append(df[daily_mask])\n",
    "        \n",
    "        # Aggregate hourly to daily\n",
    "        if hourly_mask.any():\n",
    "            hourly_df = df[hourly_mask]\n",
    "            daily_agg = self._aggregate_to_daily(hourly_df)\n",
    "            if not daily_agg.empty:\n",
    "                result_dfs.append(daily_agg)\n",
    "        \n",
    "        # For monthly data, we'll expand to daily (for comparison purposes)\n",
    "        # This is a simple approach - you might want more sophisticated interpolation\n",
    "        if monthly_mask.any():\n",
    "            monthly_df = df[monthly_mask]\n",
    "            daily_expanded = self._expand_monthly_to_daily(monthly_df)\n",
    "            if not daily_expanded.empty:\n",
    "                result_dfs.append(daily_expanded)\n",
    "        \n",
    "        if result_dfs:\n",
    "            return pd.concat(result_dfs, ignore_index=True)\n",
    "        \n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    def _expand_monthly_to_daily(self, monthly_df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Expand monthly data to daily by repeating values\"\"\"\n",
    "        expanded_dfs = []\n",
    "        \n",
    "        # Group by everything except DateTime and Value\n",
    "        group_cols = [col for col in monthly_df.columns \n",
    "                     if col not in ['DateTime', 'Value', 'TimeIndex']]\n",
    "        \n",
    "        for group_vals, group_df in monthly_df.groupby(group_cols):\n",
    "            # For each monthly value, create daily values\n",
    "            for _, row in group_df.iterrows():\n",
    "                # Get the month\n",
    "                month_date = pd.to_datetime(row['DateTime'])\n",
    "                \n",
    "                # Create daily dates for this month\n",
    "                if month_date.month == 12:\n",
    "                    next_month = month_date.replace(year=month_date.year + 1, month=1)\n",
    "                else:\n",
    "                    next_month = month_date.replace(month=month_date.month + 1)\n",
    "                \n",
    "                daily_dates = pd.date_range(\n",
    "                    start=month_date,\n",
    "                    end=next_month - pd.Timedelta(days=1),\n",
    "                    freq='D'\n",
    "                )\n",
    "                \n",
    "                # Create daily rows\n",
    "                daily_rows = []\n",
    "                for date in daily_dates:\n",
    "                    daily_row = row.copy()\n",
    "                    daily_row['DateTime'] = date\n",
    "                    daily_row['ReportingFrequency'] = 'Daily'\n",
    "                    \n",
    "                    # For energy variables (sum), divide by days in month\n",
    "                    # For other variables (mean), keep the same value\n",
    "                    if 'Energy' in row.get('Variable', ''):\n",
    "                        daily_row['Value'] = row['Value'] / len(daily_dates)\n",
    "                    \n",
    "                    daily_rows.append(daily_row)\n",
    "                \n",
    "                expanded_dfs.append(pd.DataFrame(daily_rows))\n",
    "        \n",
    "        if expanded_dfs:\n",
    "            return pd.concat(expanded_dfs, ignore_index=True)\n",
    "        \n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    def _create_building_variant_comparisons(self, building_id: str, \n",
    "                                           base_df: pd.DataFrame,\n",
    "                                           variant_dict: Dict[str, pd.DataFrame]):\n",
    "        \"\"\"Create comparison files for a building's variants\"\"\"\n",
    "        output_path = self.base_parsed_dir / 'timeseries' / 'variants' / 'daily'\n",
    "        output_path.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Get unique variables\n",
    "        all_variables = set(base_df['Variable'].unique())\n",
    "        for vdf in variant_dict.values():\n",
    "            all_variables.update(vdf['Variable'].unique())\n",
    "        \n",
    "        print(f\"\\n  Creating comparison files for {len(all_variables)} variables...\")\n",
    "        \n",
    "        for variable in sorted(all_variables):\n",
    "            # Create comparison for this variable\n",
    "            comparison_df = self._create_single_variable_comparison(\n",
    "                variable, building_id, base_df, variant_dict\n",
    "            )\n",
    "            \n",
    "            if not comparison_df.empty:\n",
    "                # Clean variable name for filename\n",
    "                clean_var_name = (variable.lower()\n",
    "                                .replace(':', '_')\n",
    "                                .replace(' ', '_')\n",
    "                                .replace('[', '')\n",
    "                                .replace(']', '')\n",
    "                                .replace('(', '')\n",
    "                                .replace(')', ''))\n",
    "                \n",
    "                output_file = output_path / f\"{clean_var_name}_{building_id}.parquet\"\n",
    "                comparison_df.to_parquet(output_file, index=False)\n",
    "                \n",
    "                # Count how many variants have data\n",
    "                variant_cols = [c for c in comparison_df.columns if c.endswith('_value') and c != 'base_value']\n",
    "                variants_with_data = sum(comparison_df[col].notna().any() for col in variant_cols)\n",
    "                \n",
    "                print(f\"    {variable}: {len(comparison_df)} rows, {variants_with_data} variants with data\")\n",
    "    \n",
    "    def _create_single_variable_comparison(self, variable: str, building_id: str,\n",
    "                                         base_df: pd.DataFrame,\n",
    "                                         variant_dict: Dict[str, pd.DataFrame]) -> pd.DataFrame:\n",
    "        \"\"\"Create comparison dataframe for a single variable\"\"\"\n",
    "        # Filter base data\n",
    "        base_var = base_df[base_df['Variable'] == variable].copy()\n",
    "        \n",
    "        if base_var.empty:\n",
    "            # If no base data, use first variant as template\n",
    "            for vdf in variant_dict.values():\n",
    "                template = vdf[vdf['Variable'] == variable].copy()\n",
    "                if not template.empty:\n",
    "                    base_var = template.copy()\n",
    "                    base_var['Value'] = np.nan  # Set base values to NaN\n",
    "                    break\n",
    "        \n",
    "        if base_var.empty:\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        # Prepare base data\n",
    "        base_var = base_var.rename(columns={'Value': 'base_value'})\n",
    "        \n",
    "        # Determine merge columns\n",
    "        merge_cols = ['DateTime', 'Zone']\n",
    "        if 'Zone' not in base_var.columns or base_var['Zone'].isna().all():\n",
    "            merge_cols = ['DateTime']\n",
    "            base_var['Zone'] = 'Building'\n",
    "        \n",
    "        # Start with base data\n",
    "        keep_cols = merge_cols + ['category', 'Units', 'base_value']\n",
    "        keep_cols = [c for c in keep_cols if c in base_var.columns]\n",
    "        result = base_var[keep_cols].copy()\n",
    "        \n",
    "        # Add each variant\n",
    "        for variant_id in sorted(variant_dict.keys()):\n",
    "            variant_df = variant_dict[variant_id]\n",
    "            variant_var = variant_df[variant_df['Variable'] == variable].copy()\n",
    "            \n",
    "            if not variant_var.empty:\n",
    "                variant_var = variant_var.rename(columns={'Value': f'{variant_id}_value'})\n",
    "                \n",
    "                # Ensure Zone column matches\n",
    "                if 'Zone' not in variant_var.columns or variant_var['Zone'].isna().all():\n",
    "                    variant_var['Zone'] = 'Building'\n",
    "                \n",
    "                # Merge\n",
    "                merge_cols_actual = [c for c in merge_cols if c in variant_var.columns]\n",
    "                result = result.merge(\n",
    "                    variant_var[merge_cols_actual + [f'{variant_id}_value']],\n",
    "                    on=merge_cols_actual,\n",
    "                    how='outer'\n",
    "                )\n",
    "        \n",
    "        # Clean up and add metadata\n",
    "        result['timestamp'] = result['DateTime']\n",
    "        result['building_id'] = building_id\n",
    "        result['variable_name'] = variable\n",
    "        \n",
    "        # Reorder columns\n",
    "        first_cols = ['timestamp', 'building_id', 'Zone', 'variable_name', 'category', 'Units']\n",
    "        first_cols = [c for c in first_cols if c in result.columns]\n",
    "        \n",
    "        value_cols = ['base_value'] + sorted([c for c in result.columns if c.endswith('_value') and c != 'base_value'])\n",
    "        \n",
    "        other_cols = [c for c in result.columns if c not in first_cols + value_cols + ['DateTime']]\n",
    "        \n",
    "        final_cols = first_cols + value_cols + other_cols\n",
    "        result = result[final_cols]\n",
    "        \n",
    "        return result.sort_values(['timestamp', 'Zone']).reset_index(drop=True)\n",
    "    \n",
    "    def _convert_to_semi_wide(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Convert to semi-wide format with dates as columns\"\"\"\n",
    "        df = df.copy()\n",
    "        \n",
    "        # Create date string\n",
    "        df['date_str'] = pd.to_datetime(df['DateTime']).dt.strftime('%Y-%m-%d')\n",
    "        \n",
    "        # Define index columns\n",
    "        index_cols = ['building_id', 'variant_id', 'Variable', 'category', 'Zone', 'Units']\n",
    "        \n",
    "        # Ensure all columns exist\n",
    "        for col in index_cols:\n",
    "            if col not in df.columns:\n",
    "                if col == 'Zone':\n",
    "                    df[col] = 'Building'\n",
    "                elif col == 'category':\n",
    "                    df[col] = df['Variable'].apply(self._categorize_variable)\n",
    "                else:\n",
    "                    df[col] = ''\n",
    "        \n",
    "        # Handle None/null in Zone\n",
    "        df['Zone'] = df['Zone'].fillna('Building')\n",
    "        \n",
    "        # Pivot\n",
    "        pivot_df = df.pivot_table(\n",
    "            index=index_cols,\n",
    "            columns='date_str',\n",
    "            values='Value',\n",
    "            aggfunc='mean'\n",
    "        ).reset_index()\n",
    "        \n",
    "        # Rename Variable to VariableName\n",
    "        pivot_df = pivot_df.rename(columns={'Variable': 'VariableName'})\n",
    "        \n",
    "        return pivot_df\n",
    "    \n",
    "    def _extract_sql_data(self, sql_path: Path, building_id: str, variant_id: str) -> pd.DataFrame:\n",
    "        \"\"\"Extract data from SQL file\"\"\"\n",
    "        try:\n",
    "            conn = sqlite3.connect(str(sql_path))\n",
    "            \n",
    "            # Query all data\n",
    "            query = \"\"\"\n",
    "            SELECT \n",
    "                t.TimeIndex,\n",
    "                CASE \n",
    "                    WHEN t.Hour = 24 THEN datetime(printf('%04d-%02d-%02d 00:00:00', t.Year, t.Month, t.Day), '+1 day')\n",
    "                    ELSE datetime(printf('%04d-%02d-%02d %02d:%02d:00', t.Year, t.Month, t.Day, t.Hour, t.Minute))\n",
    "                END as DateTime,\n",
    "                rdd.Name as Variable,\n",
    "                rdd.KeyValue as Zone,\n",
    "                rd.Value,\n",
    "                rdd.Units,\n",
    "                rdd.ReportingFrequency\n",
    "            FROM ReportData rd\n",
    "            JOIN Time t ON rd.TimeIndex = t.TimeIndex\n",
    "            JOIN ReportDataDictionary rdd ON rd.ReportDataDictionaryIndex = rdd.ReportDataDictionaryIndex\n",
    "            WHERE t.EnvironmentPeriodIndex IN (\n",
    "                SELECT EnvironmentPeriodIndex \n",
    "                FROM EnvironmentPeriods \n",
    "                WHERE EnvironmentType = 3\n",
    "            )\n",
    "            \"\"\"\n",
    "            \n",
    "            df = pd.read_sql_query(query, conn)\n",
    "            conn.close()\n",
    "            \n",
    "            if not df.empty:\n",
    "                df['DateTime'] = pd.to_datetime(df['DateTime'])\n",
    "                df['building_id'] = building_id\n",
    "                df['variant_id'] = variant_id\n",
    "                df['category'] = df['Variable'].apply(self._categorize_variable)\n",
    "                \n",
    "                return df\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting from {sql_path}: {e}\")\n",
    "        \n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    def _aggregate_to_daily(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Aggregate hourly data to daily\"\"\"\n",
    "        if df.empty or 'DateTime' not in df.columns:\n",
    "            return df\n",
    "        \n",
    "        df = df.copy()\n",
    "        df['Date'] = pd.to_datetime(df['DateTime']).dt.date\n",
    "        \n",
    "        # Group columns\n",
    "        group_cols = [col for col in df.columns \n",
    "                     if col not in ['DateTime', 'Value', 'Date', 'TimeIndex']]\n",
    "        \n",
    "        # Determine aggregation method\n",
    "        agg_results = []\n",
    "        \n",
    "        # Energy variables - sum\n",
    "        energy_mask = df['Variable'].str.contains('Energy|Consumption', na=False)\n",
    "        if energy_mask.any():\n",
    "            energy_df = df[energy_mask]\n",
    "            energy_agg = energy_df.groupby(group_cols + ['Date'])['Value'].sum().reset_index()\n",
    "            energy_agg['DateTime'] = pd.to_datetime(energy_agg['Date'])\n",
    "            agg_results.append(energy_agg.drop('Date', axis=1))\n",
    "        \n",
    "        # Other variables - mean\n",
    "        if (~energy_mask).any():\n",
    "            other_df = df[~energy_mask]\n",
    "            other_agg = other_df.groupby(group_cols + ['Date'])['Value'].mean().reset_index()\n",
    "            other_agg['DateTime'] = pd.to_datetime(other_agg['Date'])\n",
    "            agg_results.append(other_agg.drop('Date', axis=1))\n",
    "        \n",
    "        if agg_results:\n",
    "            result = pd.concat(agg_results, ignore_index=True)\n",
    "            result['ReportingFrequency'] = 'Daily'\n",
    "            return result\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def _categorize_variable(self, variable_name: str) -> str:\n",
    "        \"\"\"Categorize variable by name\"\"\"\n",
    "        var_lower = variable_name.lower()\n",
    "        \n",
    "        if any(meter in variable_name for meter in ['Electricity:', 'Gas:', 'Cooling:', 'Heating:']):\n",
    "            return 'energy_meters'\n",
    "        elif 'zone' in var_lower and any(word in var_lower for word in ['temperature', 'humidity']):\n",
    "            return 'geometry'\n",
    "        elif 'surface' in var_lower:\n",
    "            return 'materials'\n",
    "        elif 'water heater' in var_lower:\n",
    "            return 'dhw'\n",
    "        elif 'equipment' in var_lower:\n",
    "            return 'equipment'\n",
    "        elif 'lights' in var_lower:\n",
    "            return 'lighting'\n",
    "        elif 'hvac' in var_lower or 'air system' in var_lower:\n",
    "            return 'hvac'\n",
    "        elif 'ventilation' in var_lower:\n",
    "            return 'ventilation'\n",
    "        elif 'infiltration' in var_lower:\n",
    "            return 'infiltration'\n",
    "        else:\n",
    "            return 'other'\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    job_output_dir = \"output/221d33e9-f628-4f1e-86e5-3466f6d140a3\"\n",
    "    \n",
    "    transformer = FixedSQLTransformer(job_output_dir)\n",
    "    \n",
    "    # Transform base data\n",
    "    base_result = transformer.transform_base_to_semi_wide()\n",
    "    \n",
    "    # Transform variant comparisons\n",
    "    transformer.transform_variants_to_comparison()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f3b34353",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Identified base buildings: ['4136733', '4136737', '4136738']\n",
      "\n",
      "Variant mapping created:\n",
      "  Building 4136733: ['variant_0', 'variant_1', 'variant_10', 'variant_11', 'variant_12', 'variant_13', 'variant_14', 'variant_15', 'variant_16', 'variant_17', 'variant_2', 'variant_3', 'variant_4', 'variant_5', 'variant_6', 'variant_7', 'variant_8', 'variant_9']\n",
      "\n",
      "============================================================\n",
      "Transforming BASE data to semi-wide format\n",
      "============================================================\n",
      "\n",
      "Loading base data from SQL files...\n",
      "  Loading base data for building 4136733 from simulation_bldg0_4136733.sql\n",
      "  Loading base data for building 4136737 from simulation_bldg1_4136737.sql\n",
      "  Loading base data for building 4136738 from simulation_bldg2_4136738.sql\n",
      "\n",
      "Base data loaded:\n",
      "  Total rows: 10779\n",
      "  Buildings: ['4136733', '4136737', '4136738']\n",
      "  Variables: 46\n",
      "  Date range: 2013-01-02 00:00:00 to 2014-01-01 00:00:00\n",
      "\n",
      "Saved semi-wide base data to: output\\221d33e9-f628-4f1e-86e5-3466f6d140a3\\parsed_data\\timeseries\\base\\daily\\all_variables.parquet\n",
      "  Shape: (597, 401)\n",
      "  Buildings: ['4136733', '4136737', '4136738']\n",
      "  Variables: 29\n",
      "  Date columns: 395\n",
      "\n",
      "============================================================\n",
      "Transforming VARIANT data to comparison format\n",
      "============================================================\n",
      "\n",
      "Processing variants for building 4136733...\n",
      "    Loading base from simulation_bldg0_4136733.sql\n",
      "  Loading variant_0 from simulation_bldg1_4136733.sql\n",
      "  Loading variant_1 from simulation_bldg2_4136733.sql\n",
      "  Loading variant_10 from simulation_bldg11_4136733.sql\n",
      "  Loading variant_11 from simulation_bldg12_4136733.sql\n",
      "  Loading variant_12 from simulation_bldg13_4136733.sql\n",
      "  Loading variant_13 from simulation_bldg14_4136733.sql\n",
      "  Loading variant_14 from simulation_bldg15_4136733.sql\n",
      "  Loading variant_15 from simulation_bldg16_4136733.sql\n",
      "  Loading variant_16 from simulation_bldg17_4136733.sql\n",
      "  Loading variant_17 from simulation_bldg18_4136733.sql\n",
      "  Loading variant_2 from simulation_bldg3_4136733.sql\n",
      "  Loading variant_3 from simulation_bldg4_4136733.sql\n",
      "  Loading variant_4 from simulation_bldg5_4136733.sql\n",
      "  Loading variant_5 from simulation_bldg6_4136733.sql\n",
      "  Loading variant_6 from simulation_bldg7_4136733.sql\n",
      "  Loading variant_7 from simulation_bldg8_4136733.sql\n",
      "  Loading variant_8 from simulation_bldg9_4136733.sql\n",
      "  Loading variant_9 from simulation_bldg10_4136733.sql\n",
      "\n",
      "  Creating comparison files for 39 variables...\n",
      "    Electricity:Facility: 365 rows, 18 variants with data\n",
      "    Site Diffuse Solar Radiation Rate per Area: 365 rows, 18 variants with data\n",
      "    Site Outdoor Air Drybulb Temperature: 365 rows, 18 variants with data\n",
      "    Site Outdoor Air Relative Humidity: 365 rows, 18 variants with data\n",
      "    Surface Inside Face Conduction Heat Transfer Rate: 21900 rows, 18 variants with data\n",
      "    Surface Inside Face Temperature: 24090 rows, 18 variants with data\n",
      "    Surface Outside Face Incident Solar Radiation Rate per Area: 6205 rows, 18 variants with data\n",
      "    Surface Outside Face Temperature: 24090 rows, 18 variants with data\n",
      "    Surface Shading Device Is On Time Fraction: 2190 rows, 18 variants with data\n",
      "    Surface Window Blind Slat Angle: 2190 rows, 18 variants with data\n",
      "    Surface Window Heat Gain Energy: 2190 rows, 18 variants with data\n",
      "    Surface Window Heat Gain Rate: 2190 rows, 18 variants with data\n",
      "    Surface Window Heat Loss Energy: 2190 rows, 18 variants with data\n",
      "    Surface Window Heat Loss Rate: 2190 rows, 18 variants with data\n",
      "    Surface Window Shading Device Absorbed Solar Radiation Rate: 2190 rows, 18 variants with data\n",
      "    Surface Window Transmitted Beam Solar Radiation Rate: 2190 rows, 18 variants with data\n",
      "    Surface Window Transmitted Diffuse Solar Radiation Rate: 2190 rows, 18 variants with data\n",
      "    Surface Window Transmitted Solar Radiation Rate: 2190 rows, 18 variants with data\n",
      "    Water Heater Heating Energy: 365 rows, 18 variants with data\n",
      "    Zone Air Heat Balance Air Energy Storage Rate: 3650 rows, 18 variants with data\n",
      "    Zone Air Heat Balance Internal Convective Heat Gain Rate: 3650 rows, 18 variants with data\n",
      "    Zone Air Heat Balance Interzone Air Transfer Rate: 3650 rows, 18 variants with data\n",
      "    Zone Air Heat Balance Outdoor Air Transfer Rate: 3650 rows, 18 variants with data\n",
      "    Zone Air Heat Balance Surface Convection Rate: 3650 rows, 18 variants with data\n",
      "    Zone Air Heat Balance System Air Transfer Rate: 3650 rows, 18 variants with data\n",
      "    Zone Air System Sensible Cooling Energy: 3650 rows, 18 variants with data\n",
      "    Zone Air System Sensible Cooling Rate: 3650 rows, 18 variants with data\n",
      "    Zone Air System Sensible Heating Energy: 3650 rows, 18 variants with data\n",
      "    Zone Air System Sensible Heating Rate: 3650 rows, 18 variants with data\n",
      "    Zone Infiltration Latent Heat Gain Energy: 3650 rows, 18 variants with data\n",
      "    Zone Infiltration Latent Heat Loss Energy: 3650 rows, 18 variants with data\n",
      "    Zone Infiltration Sensible Heat Gain Energy: 3650 rows, 18 variants with data\n",
      "    Zone Infiltration Sensible Heat Loss Energy: 3650 rows, 18 variants with data\n",
      "    Zone Infiltration Standard Density Volume Flow Rate: 3650 rows, 18 variants with data\n",
      "    Zone Mean Air Temperature: 3650 rows, 18 variants with data\n",
      "    Zone Mean Radiant Temperature: 3650 rows, 18 variants with data\n",
      "    Zone Mechanical Ventilation Mass Flow Rate: 3650 rows, 18 variants with data\n",
      "    Zone Windows Total Transmitted Solar Radiation Energy: 3650 rows, 18 variants with data\n",
      "    Zone Windows Total Transmitted Solar Radiation Rate: 3650 rows, 18 variants with data\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Fixed SQL Data Transformer with correct base/variant identification logic\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Tuple, Set\n",
    "import sqlite3\n",
    "from datetime import datetime\n",
    "import re\n",
    "\n",
    "class FixedSQLTransformer:\n",
    "    \"\"\"Transform SQL data with correct base/variant identification\"\"\"\n",
    "    \n",
    "    def __init__(self, job_output_dir: str):\n",
    "        self.job_output_dir = Path(job_output_dir)\n",
    "        self.base_parsed_dir = self.job_output_dir / 'parsed_data'\n",
    "        self.modified_parsed_dir = self.job_output_dir / 'parsed_modified_results'\n",
    "        \n",
    "        # Identify base buildings and buildings with variants\n",
    "        self.base_buildings = self._identify_base_buildings()\n",
    "        self.variant_mapping = self._create_variant_mapping()\n",
    "        \n",
    "    def _identify_base_buildings(self) -> Set[str]:\n",
    "        \"\"\"Identify which buildings are base (from output_IDFs)\"\"\"\n",
    "        base_buildings = set()\n",
    "        base_idfs_dir = self.job_output_dir / 'output_IDFs'\n",
    "        \n",
    "        if base_idfs_dir.exists():\n",
    "            for idf in base_idfs_dir.glob('building_*.idf'):\n",
    "                match = re.search(r'building_(\\d+)\\.idf', idf.name)\n",
    "                if match:\n",
    "                    base_buildings.add(match.group(1))\n",
    "        \n",
    "        print(f\"Identified base buildings: {sorted(base_buildings)}\")\n",
    "        return base_buildings\n",
    "    \n",
    "    def _create_variant_mapping(self) -> Dict[str, Dict[str, Path]]:\n",
    "        \"\"\"Create mapping of building -> variant -> SQL file path\"\"\"\n",
    "        variant_mapping = {}\n",
    "        \n",
    "        # Check modified IDFs to see which buildings have variants\n",
    "        modified_idfs_dir = self.job_output_dir / 'modified_idfs'\n",
    "        if modified_idfs_dir.exists():\n",
    "            for idf in modified_idfs_dir.glob('building_*_variant_*.idf'):\n",
    "                match = re.search(r'building_(\\d+)_variant_(\\d+)\\.idf', idf.name)\n",
    "                if match:\n",
    "                    building_id = match.group(1)\n",
    "                    variant_num = match.group(2)\n",
    "                    \n",
    "                    if building_id not in variant_mapping:\n",
    "                        variant_mapping[building_id] = {}\n",
    "                    \n",
    "                    # Find corresponding SQL file in Modified_Sim_Results\n",
    "                    # Note: SQL uses bldg1 for variant_0, bldg2 for variant_1, etc.\n",
    "                    sql_bldg_num = int(variant_num) + 1\n",
    "                    sql_pattern = f\"simulation_bldg{sql_bldg_num}_{building_id}.sql\"\n",
    "                    \n",
    "                    # Search for SQL file\n",
    "                    modified_sql_dir = self.job_output_dir / 'Modified_Sim_Results'\n",
    "                    if modified_sql_dir.exists():\n",
    "                        for sql_file in modified_sql_dir.glob(f\"**/{sql_pattern}\"):\n",
    "                            variant_mapping[building_id][f'variant_{variant_num}'] = sql_file\n",
    "                            break\n",
    "        \n",
    "        print(f\"\\nVariant mapping created:\")\n",
    "        for bid, variants in variant_mapping.items():\n",
    "            print(f\"  Building {bid}: {list(variants.keys())}\")\n",
    "        \n",
    "        return variant_mapping\n",
    "    \n",
    "    def transform_base_to_semi_wide(self):\n",
    "        \"\"\"Transform base simulation data to semi-wide format\"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"Transforming BASE data to semi-wide format\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # Load only actual base data\n",
    "        base_data = self._load_true_base_data()\n",
    "        \n",
    "        if base_data.empty:\n",
    "            print(\"No base data found!\")\n",
    "            return None\n",
    "        \n",
    "        print(f\"\\nBase data loaded:\")\n",
    "        print(f\"  Total rows: {len(base_data)}\")\n",
    "        print(f\"  Buildings: {sorted(base_data['building_id'].unique())}\")\n",
    "        print(f\"  Variables: {len(base_data['Variable'].unique())}\")\n",
    "        print(f\"  Date range: {base_data['DateTime'].min()} to {base_data['DateTime'].max()}\")\n",
    "        \n",
    "        # Handle different frequencies\n",
    "        daily_data = self._prepare_daily_data(base_data)\n",
    "        \n",
    "        if daily_data.empty:\n",
    "            print(\"No daily data available after processing!\")\n",
    "            return None\n",
    "        \n",
    "        # Convert to semi-wide format\n",
    "        semi_wide_df = self._convert_to_semi_wide(daily_data)\n",
    "        \n",
    "        # Save the transformed data\n",
    "        output_path = self.base_parsed_dir / 'timeseries' / 'base' / 'daily'\n",
    "        output_path.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        output_file = output_path / 'all_variables.parquet'\n",
    "        semi_wide_df.to_parquet(output_file, index=False)\n",
    "        \n",
    "        print(f\"\\nSaved semi-wide base data to: {output_file}\")\n",
    "        print(f\"  Shape: {semi_wide_df.shape}\")\n",
    "        print(f\"  Buildings: {sorted(semi_wide_df['building_id'].unique())}\")\n",
    "        print(f\"  Variables: {semi_wide_df['VariableName'].nunique()}\")\n",
    "        print(f\"  Date columns: {len([c for c in semi_wide_df.columns if c.startswith('20')])}\")\n",
    "        \n",
    "        return semi_wide_df\n",
    "    \n",
    "    def _load_true_base_data(self) -> pd.DataFrame:\n",
    "        \"\"\"Load only TRUE base simulation data\"\"\"\n",
    "        all_base_data = []\n",
    "        \n",
    "        # Load from Sim_Results (base simulations)\n",
    "        sim_results_dir = self.job_output_dir / 'Sim_Results'\n",
    "        if sim_results_dir.exists():\n",
    "            print(\"\\nLoading base data from SQL files...\")\n",
    "            \n",
    "            for year_dir in sim_results_dir.iterdir():\n",
    "                if year_dir.is_dir() and year_dir.name.isdigit():\n",
    "                    for sql_file in year_dir.glob('*.sql'):\n",
    "                        # Extract building ID from filename\n",
    "                        match = re.search(r'simulation_bldg\\d+_(\\d+)\\.sql', sql_file.name)\n",
    "                        if match:\n",
    "                            building_id = match.group(1)\n",
    "                            \n",
    "                            # Only process if it's a base building\n",
    "                            if building_id in self.base_buildings:\n",
    "                                print(f\"  Loading base data for building {building_id} from {sql_file.name}\")\n",
    "                                df = self._extract_sql_data(sql_file, building_id, 'base')\n",
    "                                if not df.empty:\n",
    "                                    all_base_data.append(df)\n",
    "        \n",
    "        # Also check parsed data directories (if already parsed)\n",
    "        parsed_paths = [\n",
    "            self.base_parsed_dir / 'sql_results' / 'timeseries' / 'hourly',\n",
    "            self.base_parsed_dir / 'sql_results' / 'timeseries' / 'aggregated' / 'daily',\n",
    "            self.base_parsed_dir / 'sql_results' / 'timeseries' / 'raw' / 'daily'\n",
    "        ]\n",
    "        \n",
    "        for parsed_path in parsed_paths:\n",
    "            if parsed_path.exists():\n",
    "                for parquet_file in parsed_path.glob('*.parquet'):\n",
    "                    try:\n",
    "                        df = pd.read_parquet(parquet_file)\n",
    "                        \n",
    "                        # Filter to only base buildings\n",
    "                        if 'building_id' in df.columns:\n",
    "                            df = df[df['building_id'].isin(self.base_buildings)]\n",
    "                            \n",
    "                            # Ensure variant_id is 'base'\n",
    "                            df['variant_id'] = 'base'\n",
    "                            \n",
    "                            if not df.empty:\n",
    "                                all_base_data.append(df)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error loading {parquet_file}: {e}\")\n",
    "        \n",
    "        if all_base_data:\n",
    "            combined_df = pd.concat(all_base_data, ignore_index=True)\n",
    "            \n",
    "            # Ensure DateTime is proper format\n",
    "            if 'DateTime' in combined_df.columns:\n",
    "                combined_df['DateTime'] = pd.to_datetime(combined_df['DateTime'])\n",
    "            \n",
    "            # Remove duplicates\n",
    "            if all(['DateTime', 'building_id', 'Variable', 'Zone'] == col or col in combined_df.columns \n",
    "                   for col in ['DateTime', 'building_id', 'Variable', 'Zone']):\n",
    "                combined_df = combined_df.drop_duplicates(\n",
    "                    subset=['DateTime', 'building_id', 'Variable', 'Zone'],\n",
    "                    keep='last'\n",
    "                )\n",
    "            \n",
    "            return combined_df\n",
    "        \n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    def transform_variants_to_comparison(self):\n",
    "        \"\"\"Transform variant data to comparison format\"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"Transforming VARIANT data to comparison format\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # Process each building that has variants\n",
    "        for building_id, variant_files in self.variant_mapping.items():\n",
    "            print(f\"\\nProcessing variants for building {building_id}...\")\n",
    "            \n",
    "            # Load base data for this building\n",
    "            base_data = self._load_building_base_data(building_id)\n",
    "            \n",
    "            if base_data.empty:\n",
    "                print(f\"  WARNING: No base data found for building {building_id}\")\n",
    "                continue\n",
    "            \n",
    "            # Load variant data\n",
    "            variant_data_dict = {}\n",
    "            for variant_id, sql_path in variant_files.items():\n",
    "                print(f\"  Loading {variant_id} from {sql_path.name}\")\n",
    "                variant_df = self._extract_sql_data(sql_path, building_id, variant_id)\n",
    "                \n",
    "                if not variant_df.empty:\n",
    "                    variant_data_dict[variant_id] = variant_df\n",
    "            \n",
    "            if not variant_data_dict:\n",
    "                print(f\"  No variant data loaded for building {building_id}\")\n",
    "                continue\n",
    "            \n",
    "            # Prepare daily data for all\n",
    "            base_daily = self._prepare_daily_data(base_data)\n",
    "            variant_daily_dict = {}\n",
    "            for vid, vdf in variant_data_dict.items():\n",
    "                variant_daily = self._prepare_daily_data(vdf)\n",
    "                if not variant_daily.empty:\n",
    "                    variant_daily_dict[vid] = variant_daily\n",
    "            \n",
    "            # Create comparison files for each variable\n",
    "            self._create_building_variant_comparisons(\n",
    "                building_id, base_daily, variant_daily_dict\n",
    "            )\n",
    "    \n",
    "    def _load_building_base_data(self, building_id: str) -> pd.DataFrame:\n",
    "        \"\"\"Load base data for a specific building\"\"\"\n",
    "        # First try SQL file\n",
    "        sim_results_dir = self.job_output_dir / 'Sim_Results'\n",
    "        if sim_results_dir.exists():\n",
    "            for year_dir in sim_results_dir.iterdir():\n",
    "                if year_dir.is_dir() and year_dir.name.isdigit():\n",
    "                    # Look for this building's base SQL file\n",
    "                    for sql_file in year_dir.glob(f'*_{building_id}.sql'):\n",
    "                        print(f\"    Loading base from {sql_file.name}\")\n",
    "                        return self._extract_sql_data(sql_file, building_id, 'base')\n",
    "        \n",
    "        # Fallback to parsed data\n",
    "        return self._load_true_base_data()[\n",
    "            lambda df: df['building_id'] == building_id if not df.empty else df\n",
    "        ]\n",
    "    \n",
    "    def _prepare_daily_data(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Prepare data at daily frequency, handling monthly data appropriately\"\"\"\n",
    "        if df.empty:\n",
    "            return df\n",
    "        \n",
    "        df = df.copy()\n",
    "        \n",
    "        # Check if ReportingFrequency column exists\n",
    "        if 'ReportingFrequency' not in df.columns:\n",
    "            # Assume it's already daily if no frequency info\n",
    "            return df\n",
    "        \n",
    "        # Separate by frequency\n",
    "        daily_mask = df['ReportingFrequency'] == 'Daily'\n",
    "        monthly_mask = df['ReportingFrequency'] == 'Monthly'\n",
    "        hourly_mask = df['ReportingFrequency'] == 'Hourly'\n",
    "        \n",
    "        result_dfs = []\n",
    "        \n",
    "        # Keep daily data as-is\n",
    "        if daily_mask.any():\n",
    "            result_dfs.append(df[daily_mask])\n",
    "        \n",
    "        # Aggregate hourly to daily\n",
    "        if hourly_mask.any():\n",
    "            hourly_df = df[hourly_mask]\n",
    "            daily_agg = self._aggregate_to_daily(hourly_df)\n",
    "            if not daily_agg.empty:\n",
    "                result_dfs.append(daily_agg)\n",
    "        \n",
    "        # For monthly data, we'll expand to daily (for comparison purposes)\n",
    "        # This is a simple approach - you might want more sophisticated interpolation\n",
    "        if monthly_mask.any():\n",
    "            monthly_df = df[monthly_mask]\n",
    "            daily_expanded = self._expand_monthly_to_daily(monthly_df)\n",
    "            if not daily_expanded.empty:\n",
    "                result_dfs.append(daily_expanded)\n",
    "        \n",
    "        if result_dfs:\n",
    "            return pd.concat(result_dfs, ignore_index=True)\n",
    "        \n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    def _expand_monthly_to_daily(self, monthly_df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Expand monthly data to daily by repeating values\"\"\"\n",
    "        expanded_dfs = []\n",
    "        \n",
    "        # Group by everything except DateTime and Value\n",
    "        group_cols = [col for col in monthly_df.columns \n",
    "                     if col not in ['DateTime', 'Value', 'TimeIndex']]\n",
    "        \n",
    "        for group_vals, group_df in monthly_df.groupby(group_cols):\n",
    "            # For each monthly value, create daily values\n",
    "            for _, row in group_df.iterrows():\n",
    "                # Get the month\n",
    "                month_date = pd.to_datetime(row['DateTime'])\n",
    "                \n",
    "                # Create daily dates for this month\n",
    "                if month_date.month == 12:\n",
    "                    next_month = month_date.replace(year=month_date.year + 1, month=1)\n",
    "                else:\n",
    "                    next_month = month_date.replace(month=month_date.month + 1)\n",
    "                \n",
    "                daily_dates = pd.date_range(\n",
    "                    start=month_date,\n",
    "                    end=next_month - pd.Timedelta(days=1),\n",
    "                    freq='D'\n",
    "                )\n",
    "                \n",
    "                # Create daily rows\n",
    "                daily_rows = []\n",
    "                for date in daily_dates:\n",
    "                    daily_row = row.copy()\n",
    "                    daily_row['DateTime'] = date\n",
    "                    daily_row['ReportingFrequency'] = 'Daily'\n",
    "                    \n",
    "                    # For energy variables (sum), divide by days in month\n",
    "                    # For other variables (mean), keep the same value\n",
    "                    if 'Energy' in row.get('Variable', ''):\n",
    "                        daily_row['Value'] = row['Value'] / len(daily_dates)\n",
    "                    \n",
    "                    daily_rows.append(daily_row)\n",
    "                \n",
    "                expanded_dfs.append(pd.DataFrame(daily_rows))\n",
    "        \n",
    "        if expanded_dfs:\n",
    "            return pd.concat(expanded_dfs, ignore_index=True)\n",
    "        \n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    def _create_building_variant_comparisons(self, building_id: str, \n",
    "                                           base_df: pd.DataFrame,\n",
    "                                           variant_dict: Dict[str, pd.DataFrame]):\n",
    "        \"\"\"Create comparison files for a building's variants\"\"\"\n",
    "        output_path = self.base_parsed_dir / 'timeseries' / 'variants' / 'daily'\n",
    "        output_path.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Get unique variables\n",
    "        all_variables = set(base_df['Variable'].unique())\n",
    "        for vdf in variant_dict.values():\n",
    "            all_variables.update(vdf['Variable'].unique())\n",
    "        \n",
    "        print(f\"\\n  Creating comparison files for {len(all_variables)} variables...\")\n",
    "        \n",
    "        for variable in sorted(all_variables):\n",
    "            # Create comparison for this variable\n",
    "            comparison_df = self._create_single_variable_comparison(\n",
    "                variable, building_id, base_df, variant_dict\n",
    "            )\n",
    "            \n",
    "            if not comparison_df.empty:\n",
    "                # Clean variable name for filename\n",
    "                clean_var_name = (variable.lower()\n",
    "                                .replace(':', '_')\n",
    "                                .replace(' ', '_')\n",
    "                                .replace('[', '')\n",
    "                                .replace(']', '')\n",
    "                                .replace('(', '')\n",
    "                                .replace(')', ''))\n",
    "                \n",
    "                output_file = output_path / f\"{clean_var_name}_{building_id}.parquet\"\n",
    "                comparison_df.to_parquet(output_file, index=False)\n",
    "                \n",
    "                # Count how many variants have data\n",
    "                variant_cols = [c for c in comparison_df.columns if c.endswith('_value') and c != 'base_value']\n",
    "                variants_with_data = sum(comparison_df[col].notna().any() for col in variant_cols)\n",
    "                \n",
    "                print(f\"    {variable}: {len(comparison_df)} rows, {variants_with_data} variants with data\")\n",
    "    \n",
    "    def _create_single_variable_comparison(self, variable: str, building_id: str,\n",
    "                                         base_df: pd.DataFrame,\n",
    "                                         variant_dict: Dict[str, pd.DataFrame]) -> pd.DataFrame:\n",
    "        \"\"\"Create comparison dataframe for a single variable\"\"\"\n",
    "        # Filter base data\n",
    "        base_var = base_df[base_df['Variable'] == variable].copy()\n",
    "        \n",
    "        if base_var.empty:\n",
    "            # If no base data, use first variant as template\n",
    "            for vdf in variant_dict.values():\n",
    "                template = vdf[vdf['Variable'] == variable].copy()\n",
    "                if not template.empty:\n",
    "                    base_var = template.copy()\n",
    "                    base_var['Value'] = np.nan  # Set base values to NaN\n",
    "                    break\n",
    "        \n",
    "        if base_var.empty:\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        # Prepare base data\n",
    "        base_var = base_var.rename(columns={'Value': 'base_value'})\n",
    "        \n",
    "        # Check if Zone column exists and has valid data\n",
    "        has_zone = 'Zone' in base_var.columns and not base_var['Zone'].isna().all()\n",
    "        \n",
    "        # Determine merge columns\n",
    "        if has_zone:\n",
    "            merge_cols = ['DateTime', 'Zone']\n",
    "        else:\n",
    "            merge_cols = ['DateTime']\n",
    "            base_var['Zone'] = 'Building'\n",
    "        \n",
    "        # Start with base data\n",
    "        keep_cols = merge_cols + ['category', 'Units', 'base_value']\n",
    "        keep_cols = [c for c in keep_cols if c in base_var.columns]\n",
    "        result = base_var[keep_cols].copy()\n",
    "        \n",
    "        # Add each variant\n",
    "        for variant_id in sorted(variant_dict.keys()):\n",
    "            variant_df = variant_dict[variant_id]\n",
    "            variant_var = variant_df[variant_df['Variable'] == variable].copy()\n",
    "            \n",
    "            if not variant_var.empty:\n",
    "                variant_var = variant_var.rename(columns={'Value': f'{variant_id}_value'})\n",
    "                \n",
    "                # Ensure Zone column matches\n",
    "                if not has_zone or 'Zone' not in variant_var.columns or variant_var['Zone'].isna().all():\n",
    "                    variant_var['Zone'] = 'Building'\n",
    "                \n",
    "                # Merge\n",
    "                merge_cols_actual = [c for c in merge_cols if c in variant_var.columns and c in result.columns]\n",
    "                if merge_cols_actual:  # Only merge if we have columns to merge on\n",
    "                    result = result.merge(\n",
    "                        variant_var[merge_cols_actual + [f'{variant_id}_value']],\n",
    "                        on=merge_cols_actual,\n",
    "                        how='outer'\n",
    "                    )\n",
    "        \n",
    "        # Clean up and add metadata\n",
    "        result['timestamp'] = result['DateTime']\n",
    "        result['building_id'] = building_id\n",
    "        result['variable_name'] = variable\n",
    "        \n",
    "        # Get category if not present\n",
    "        if 'category' not in result.columns:\n",
    "            result['category'] = self._categorize_variable(variable)\n",
    "        \n",
    "        # Get Units if not present\n",
    "        if 'Units' not in result.columns:\n",
    "            result['Units'] = ''\n",
    "        \n",
    "        # Reorder columns\n",
    "        first_cols = ['timestamp', 'building_id', 'Zone', 'variable_name', 'category', 'Units']\n",
    "        first_cols = [c for c in first_cols if c in result.columns]\n",
    "        \n",
    "        value_cols = ['base_value'] + sorted([c for c in result.columns if c.endswith('_value') and c != 'base_value'])\n",
    "        \n",
    "        other_cols = [c for c in result.columns if c not in first_cols + value_cols + ['DateTime']]\n",
    "        \n",
    "        final_cols = first_cols + value_cols + other_cols\n",
    "        # Only keep columns that exist\n",
    "        final_cols = [c for c in final_cols if c in result.columns]\n",
    "        result = result[final_cols]\n",
    "        \n",
    "        # Sort by available columns\n",
    "        sort_cols = ['timestamp']\n",
    "        if 'Zone' in result.columns:\n",
    "            sort_cols.append('Zone')\n",
    "        \n",
    "        return result.sort_values(sort_cols).reset_index(drop=True)\n",
    "    \n",
    "    def _convert_to_semi_wide(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Convert to semi-wide format with dates as columns\"\"\"\n",
    "        df = df.copy()\n",
    "        \n",
    "        # Create date string\n",
    "        df['date_str'] = pd.to_datetime(df['DateTime']).dt.strftime('%Y-%m-%d')\n",
    "        \n",
    "        # Define index columns\n",
    "        index_cols = ['building_id', 'variant_id', 'Variable', 'category', 'Zone', 'Units']\n",
    "        \n",
    "        # Ensure all columns exist\n",
    "        for col in index_cols:\n",
    "            if col not in df.columns:\n",
    "                if col == 'Zone':\n",
    "                    df[col] = 'Building'\n",
    "                elif col == 'category':\n",
    "                    df[col] = df['Variable'].apply(self._categorize_variable)\n",
    "                else:\n",
    "                    df[col] = ''\n",
    "        \n",
    "        # Handle None/null in Zone\n",
    "        df['Zone'] = df['Zone'].fillna('Building')\n",
    "        \n",
    "        # Pivot\n",
    "        pivot_df = df.pivot_table(\n",
    "            index=index_cols,\n",
    "            columns='date_str',\n",
    "            values='Value',\n",
    "            aggfunc='mean'\n",
    "        ).reset_index()\n",
    "        \n",
    "        # Rename Variable to VariableName\n",
    "        pivot_df = pivot_df.rename(columns={'Variable': 'VariableName'})\n",
    "        \n",
    "        return pivot_df\n",
    "    \n",
    "    def _extract_sql_data(self, sql_path: Path, building_id: str, variant_id: str) -> pd.DataFrame:\n",
    "        \"\"\"Extract data from SQL file\"\"\"\n",
    "        try:\n",
    "            conn = sqlite3.connect(str(sql_path))\n",
    "            \n",
    "            # Query all data\n",
    "            query = \"\"\"\n",
    "            SELECT \n",
    "                t.TimeIndex,\n",
    "                CASE \n",
    "                    WHEN t.Hour = 24 THEN datetime(printf('%04d-%02d-%02d 00:00:00', t.Year, t.Month, t.Day), '+1 day')\n",
    "                    ELSE datetime(printf('%04d-%02d-%02d %02d:%02d:00', t.Year, t.Month, t.Day, t.Hour, t.Minute))\n",
    "                END as DateTime,\n",
    "                rdd.Name as Variable,\n",
    "                rdd.KeyValue as Zone,\n",
    "                rd.Value,\n",
    "                rdd.Units,\n",
    "                rdd.ReportingFrequency\n",
    "            FROM ReportData rd\n",
    "            JOIN Time t ON rd.TimeIndex = t.TimeIndex\n",
    "            JOIN ReportDataDictionary rdd ON rd.ReportDataDictionaryIndex = rdd.ReportDataDictionaryIndex\n",
    "            WHERE t.EnvironmentPeriodIndex IN (\n",
    "                SELECT EnvironmentPeriodIndex \n",
    "                FROM EnvironmentPeriods \n",
    "                WHERE EnvironmentType = 3\n",
    "            )\n",
    "            \"\"\"\n",
    "            \n",
    "            df = pd.read_sql_query(query, conn)\n",
    "            conn.close()\n",
    "            \n",
    "            if not df.empty:\n",
    "                df['DateTime'] = pd.to_datetime(df['DateTime'])\n",
    "                df['building_id'] = building_id\n",
    "                df['variant_id'] = variant_id\n",
    "                df['category'] = df['Variable'].apply(self._categorize_variable)\n",
    "                \n",
    "                return df\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting from {sql_path}: {e}\")\n",
    "        \n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    def _aggregate_to_daily(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Aggregate hourly data to daily\"\"\"\n",
    "        if df.empty or 'DateTime' not in df.columns:\n",
    "            return df\n",
    "        \n",
    "        df = df.copy()\n",
    "        df['Date'] = pd.to_datetime(df['DateTime']).dt.date\n",
    "        \n",
    "        # Group columns\n",
    "        group_cols = [col for col in df.columns \n",
    "                     if col not in ['DateTime', 'Value', 'Date', 'TimeIndex']]\n",
    "        \n",
    "        # Determine aggregation method\n",
    "        agg_results = []\n",
    "        \n",
    "        # Energy variables - sum\n",
    "        energy_mask = df['Variable'].str.contains('Energy|Consumption', na=False)\n",
    "        if energy_mask.any():\n",
    "            energy_df = df[energy_mask]\n",
    "            energy_agg = energy_df.groupby(group_cols + ['Date'])['Value'].sum().reset_index()\n",
    "            energy_agg['DateTime'] = pd.to_datetime(energy_agg['Date'])\n",
    "            agg_results.append(energy_agg.drop('Date', axis=1))\n",
    "        \n",
    "        # Other variables - mean\n",
    "        if (~energy_mask).any():\n",
    "            other_df = df[~energy_mask]\n",
    "            other_agg = other_df.groupby(group_cols + ['Date'])['Value'].mean().reset_index()\n",
    "            other_agg['DateTime'] = pd.to_datetime(other_agg['Date'])\n",
    "            agg_results.append(other_agg.drop('Date', axis=1))\n",
    "        \n",
    "        if agg_results:\n",
    "            result = pd.concat(agg_results, ignore_index=True)\n",
    "            result['ReportingFrequency'] = 'Daily'\n",
    "            return result\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def _categorize_variable(self, variable_name: str) -> str:\n",
    "        \"\"\"Categorize variable by name\"\"\"\n",
    "        var_lower = variable_name.lower()\n",
    "        \n",
    "        if any(meter in variable_name for meter in ['Electricity:', 'Gas:', 'Cooling:', 'Heating:']):\n",
    "            return 'energy_meters'\n",
    "        elif 'zone' in var_lower and any(word in var_lower for word in ['temperature', 'humidity']):\n",
    "            return 'geometry'\n",
    "        elif 'surface' in var_lower:\n",
    "            return 'materials'\n",
    "        elif 'water heater' in var_lower:\n",
    "            return 'dhw'\n",
    "        elif 'equipment' in var_lower:\n",
    "            return 'equipment'\n",
    "        elif 'lights' in var_lower:\n",
    "            return 'lighting'\n",
    "        elif 'hvac' in var_lower or 'air system' in var_lower:\n",
    "            return 'hvac'\n",
    "        elif 'ventilation' in var_lower:\n",
    "            return 'ventilation'\n",
    "        elif 'infiltration' in var_lower:\n",
    "            return 'infiltration'\n",
    "        else:\n",
    "            return 'other'\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    job_output_dir = \"output/221d33e9-f628-4f1e-86e5-3466f6d140a3\"\n",
    "    \n",
    "    transformer = FixedSQLTransformer(job_output_dir)\n",
    "    \n",
    "    # Transform base data\n",
    "    base_result = transformer.transform_base_to_semi_wide()\n",
    "    \n",
    "    # Transform variant comparisons\n",
    "    transformer.transform_variants_to_comparison()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0a29bedb",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "partially initialized module 'pandas' has no attribute '_pandas_datetime_CAPI' (most likely due to a circular import)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 12\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Try to import pandas, but continue if it fails\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 12\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m     13\u001b[0m     HAS_PANDAS \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n",
      "File \u001b[1;32md:\\New folder (2)\\Anaconda\\DDsaie\\lib\\site-packages\\pandas\\__init__.py:77\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;66;03m# let init-time option registration happen\u001b[39;00m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig_init\u001b[39;00m  \u001b[38;5;66;03m# pyright: ignore[reportUnusedImport] # noqa: F401\u001b[39;00m\n\u001b[1;32m---> 77\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     78\u001b[0m     \u001b[38;5;66;03m# dtype\u001b[39;00m\n\u001b[0;32m     79\u001b[0m     ArrowDtype,\n\u001b[0;32m     80\u001b[0m     Int8Dtype,\n\u001b[0;32m     81\u001b[0m     Int16Dtype,\n\u001b[0;32m     82\u001b[0m     Int32Dtype,\n\u001b[0;32m     83\u001b[0m     Int64Dtype,\n\u001b[0;32m     84\u001b[0m     UInt8Dtype,\n\u001b[0;32m     85\u001b[0m     UInt16Dtype,\n\u001b[0;32m     86\u001b[0m     UInt32Dtype,\n\u001b[0;32m     87\u001b[0m     UInt64Dtype,\n\u001b[0;32m     88\u001b[0m     Float32Dtype,\n\u001b[0;32m     89\u001b[0m     Float64Dtype,\n\u001b[0;32m     90\u001b[0m     CategoricalDtype,\n\u001b[0;32m     91\u001b[0m     PeriodDtype,\n\u001b[0;32m     92\u001b[0m     IntervalDtype,\n\u001b[0;32m     93\u001b[0m     DatetimeTZDtype,\n\u001b[0;32m     94\u001b[0m     StringDtype,\n\u001b[0;32m     95\u001b[0m     BooleanDtype,\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;66;03m# missing\u001b[39;00m\n\u001b[0;32m     97\u001b[0m     NA,\n\u001b[0;32m     98\u001b[0m     isna,\n\u001b[0;32m     99\u001b[0m     isnull,\n\u001b[0;32m    100\u001b[0m     notna,\n\u001b[0;32m    101\u001b[0m     notnull,\n\u001b[0;32m    102\u001b[0m     \u001b[38;5;66;03m# indexes\u001b[39;00m\n\u001b[0;32m    103\u001b[0m     Index,\n\u001b[0;32m    104\u001b[0m     CategoricalIndex,\n\u001b[0;32m    105\u001b[0m     RangeIndex,\n\u001b[0;32m    106\u001b[0m     MultiIndex,\n\u001b[0;32m    107\u001b[0m     IntervalIndex,\n\u001b[0;32m    108\u001b[0m     TimedeltaIndex,\n\u001b[0;32m    109\u001b[0m     DatetimeIndex,\n\u001b[0;32m    110\u001b[0m     PeriodIndex,\n\u001b[0;32m    111\u001b[0m     IndexSlice,\n\u001b[0;32m    112\u001b[0m     \u001b[38;5;66;03m# tseries\u001b[39;00m\n\u001b[0;32m    113\u001b[0m     NaT,\n\u001b[0;32m    114\u001b[0m     Period,\n\u001b[0;32m    115\u001b[0m     period_range,\n\u001b[0;32m    116\u001b[0m     Timedelta,\n\u001b[0;32m    117\u001b[0m     timedelta_range,\n\u001b[0;32m    118\u001b[0m     Timestamp,\n\u001b[0;32m    119\u001b[0m     date_range,\n\u001b[0;32m    120\u001b[0m     bdate_range,\n\u001b[0;32m    121\u001b[0m     Interval,\n\u001b[0;32m    122\u001b[0m     interval_range,\n\u001b[0;32m    123\u001b[0m     DateOffset,\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;66;03m# conversion\u001b[39;00m\n\u001b[0;32m    125\u001b[0m     to_numeric,\n\u001b[0;32m    126\u001b[0m     to_datetime,\n\u001b[0;32m    127\u001b[0m     to_timedelta,\n\u001b[0;32m    128\u001b[0m     \u001b[38;5;66;03m# misc\u001b[39;00m\n\u001b[0;32m    129\u001b[0m     Flags,\n\u001b[0;32m    130\u001b[0m     Grouper,\n\u001b[0;32m    131\u001b[0m     factorize,\n\u001b[0;32m    132\u001b[0m     unique,\n\u001b[0;32m    133\u001b[0m     value_counts,\n\u001b[0;32m    134\u001b[0m     NamedAgg,\n\u001b[0;32m    135\u001b[0m     array,\n\u001b[0;32m    136\u001b[0m     Categorical,\n\u001b[0;32m    137\u001b[0m     set_eng_float_format,\n\u001b[0;32m    138\u001b[0m     Series,\n\u001b[0;32m    139\u001b[0m     DataFrame,\n\u001b[0;32m    140\u001b[0m )\n\u001b[0;32m    142\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdtypes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdtypes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SparseDtype\n\u001b[0;32m    144\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtseries\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m infer_freq\n",
      "File \u001b[1;32md:\\New folder (2)\\Anaconda\\DDsaie\\lib\\site-packages\\pandas\\core\\api.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      2\u001b[0m     NaT,\n\u001b[0;32m      3\u001b[0m     Period,\n\u001b[0;32m      4\u001b[0m     Timedelta,\n\u001b[0;32m      5\u001b[0m     Timestamp,\n\u001b[0;32m      6\u001b[0m )\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmissing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m NA\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdtypes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdtypes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     10\u001b[0m     ArrowDtype,\n\u001b[0;32m     11\u001b[0m     CategoricalDtype,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     14\u001b[0m     PeriodDtype,\n\u001b[0;32m     15\u001b[0m )\n",
      "File \u001b[1;32md:\\New folder (2)\\Anaconda\\DDsaie\\lib\\site-packages\\pandas\\_libs\\__init__.py:18\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpandas_parser\u001b[39;00m  \u001b[38;5;66;03m# isort: skip # type: ignore[reportUnusedImport]\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpandas_datetime\u001b[39;00m  \u001b[38;5;66;03m# noqa: F401 # isort: skip # type: ignore[reportUnusedImport]\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minterval\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Interval\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtslibs\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     20\u001b[0m     NaT,\n\u001b[0;32m     21\u001b[0m     NaTType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     26\u001b[0m     iNaT,\n\u001b[0;32m     27\u001b[0m )\n",
      "File \u001b[1;32minterval.pyx:1\u001b[0m, in \u001b[0;36minit pandas._libs.interval\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mhashtable.pyx:1\u001b[0m, in \u001b[0;36minit pandas._libs.hashtable\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mmissing.pyx:40\u001b[0m, in \u001b[0;36minit pandas._libs.missing\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: partially initialized module 'pandas' has no attribute '_pandas_datetime_CAPI' (most likely due to a circular import)"
     ]
    }
   ],
   "source": [
    "# Directory Tree Generator for Jupyter - Works without pandas\n",
    "# For E_Plus_2040_py output directory analysis\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import List, Optional\n",
    "import fnmatch\n",
    "from datetime import datetime\n",
    "\n",
    "# Try to import pandas, but continue if it fails\n",
    "try:\n",
    "    import pandas as pd\n",
    "    HAS_PANDAS = True\n",
    "except ImportError:\n",
    "    HAS_PANDAS = False\n",
    "    print(\"Note: pandas not available. Parquet file preview disabled.\")\n",
    "\n",
    "class DirectoryTree:\n",
    "    def __init__(\n",
    "        self,\n",
    "        path: str,\n",
    "        include_patterns: Optional[List[str]] = None,\n",
    "        exclude_patterns: Optional[List[str]] = None,\n",
    "        max_depth: Optional[int] = None,\n",
    "        show_hidden: bool = False,\n",
    "        show_size: bool = False,\n",
    "        dirs_only: bool = False,\n",
    "        show_parquet_sample: bool = True\n",
    "    ):\n",
    "        self.path = Path(path)\n",
    "        self.include_patterns = include_patterns or []\n",
    "        self.exclude_patterns = exclude_patterns or []\n",
    "        self.max_depth = max_depth\n",
    "        self.show_hidden = show_hidden\n",
    "        self.show_size = show_size\n",
    "        self.dirs_only = dirs_only\n",
    "        self.show_parquet_sample = show_parquet_sample and HAS_PANDAS\n",
    "        self.parquet_files = []\n",
    "        self.tree_chars = {\n",
    "            'pipe': '│   ',\n",
    "            'tee': '├── ',\n",
    "            'last': '└── ',\n",
    "            'blank': '    '\n",
    "        }\n",
    "    \n",
    "    def _should_include(self, path: Path) -> bool:\n",
    "        \"\"\"Check if a file/folder should be included based on filters.\"\"\"\n",
    "        name = path.name\n",
    "        \n",
    "        if not self.show_hidden and name.startswith('.'):\n",
    "            return False\n",
    "        \n",
    "        if path.is_dir():\n",
    "            for pattern in self.exclude_patterns:\n",
    "                if fnmatch.fnmatch(name, pattern):\n",
    "                    return False\n",
    "            return True\n",
    "        \n",
    "        if self.dirs_only:\n",
    "            return False\n",
    "        \n",
    "        for pattern in self.exclude_patterns:\n",
    "            if fnmatch.fnmatch(name, pattern):\n",
    "                return False\n",
    "        \n",
    "        if self.include_patterns:\n",
    "            for pattern in self.include_patterns:\n",
    "                if fnmatch.fnmatch(name, pattern):\n",
    "                    return True\n",
    "            return False\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    def _get_size_str(self, path: Path) -> str:\n",
    "        \"\"\"Get human-readable file size.\"\"\"\n",
    "        if not path.is_file() or not self.show_size:\n",
    "            return \"\"\n",
    "        \n",
    "        try:\n",
    "            size = path.stat().st_size\n",
    "            for unit in ['B', 'KB', 'MB', 'GB']:\n",
    "                if size < 1024:\n",
    "                    return f\" ({size:.1f} {unit})\"\n",
    "                size /= 1024\n",
    "            return f\" ({size:.1f} TB)\"\n",
    "        except:\n",
    "            return \"\"\n",
    "    \n",
    "    def _generate_tree(self, path: Path, prefix: str = \"\", depth: int = 0) -> List[str]:\n",
    "        \"\"\"Recursively generate tree structure.\"\"\"\n",
    "        if self.max_depth is not None and depth > self.max_depth:\n",
    "            return []\n",
    "        \n",
    "        lines = []\n",
    "        \n",
    "        try:\n",
    "            entries = []\n",
    "            for entry in sorted(path.iterdir()):\n",
    "                if self._should_include(entry):\n",
    "                    entries.append(entry)\n",
    "            \n",
    "            entries.sort(key=lambda x: (not x.is_dir(), x.name.lower()))\n",
    "            \n",
    "            for i, entry in enumerate(entries):\n",
    "                is_last = i == len(entries) - 1\n",
    "                \n",
    "                if is_last:\n",
    "                    current = self.tree_chars['last']\n",
    "                    extension = self.tree_chars['blank']\n",
    "                else:\n",
    "                    current = self.tree_chars['tee']\n",
    "                    extension = self.tree_chars['pipe']\n",
    "                \n",
    "                size_str = self._get_size_str(entry)\n",
    "                lines.append(f\"{prefix}{current}{entry.name}{size_str}\")\n",
    "                \n",
    "                if entry.is_file() and entry.suffix.lower() == '.parquet' and self.show_parquet_sample:\n",
    "                    self.parquet_files.append(entry)\n",
    "                \n",
    "                if entry.is_dir():\n",
    "                    lines.extend(self._generate_tree(entry, prefix + extension, depth + 1))\n",
    "        \n",
    "        except PermissionError:\n",
    "            lines.append(f\"{prefix}[Permission Denied]\")\n",
    "        except Exception as e:\n",
    "            lines.append(f\"{prefix}[Error: {str(e)}]\")\n",
    "        \n",
    "        return lines\n",
    "    \n",
    "    def _get_parquet_sample(self, file_path: Path) -> str:\n",
    "        \"\"\"Get a sample row from a parquet file.\"\"\"\n",
    "        if not HAS_PANDAS:\n",
    "            return f\"\\n{file_path.name}: [pandas not available for preview]\"\n",
    "            \n",
    "        try:\n",
    "            df = pd.read_parquet(file_path)\n",
    "            \n",
    "            if df.empty:\n",
    "                return f\"\\n{file_path.name}: [Empty DataFrame]\"\n",
    "            \n",
    "            info_lines = [\n",
    "                f\"\\n{file_path.name}:\",\n",
    "                f\"  Shape: {df.shape[0]:,} rows × {df.shape[1]} columns\",\n",
    "                f\"  Columns: {', '.join(df.columns[:10])}\" + (\" ...\" if len(df.columns) > 10 else \"\"),\n",
    "                \"  Sample row (first):\"\n",
    "            ]\n",
    "            \n",
    "            first_row = df.iloc[0].to_dict()\n",
    "            \n",
    "            row_lines = []\n",
    "            for col, val in first_row.items():\n",
    "                if pd.isna(val):\n",
    "                    val_str = \"NaN\"\n",
    "                elif isinstance(val, float):\n",
    "                    val_str = f\"{val:.4f}\" if abs(val) < 1000 else f\"{val:.2e}\"\n",
    "                elif isinstance(val, pd.Timestamp):\n",
    "                    val_str = val.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "                else:\n",
    "                    val_str = str(val)\n",
    "                \n",
    "                if len(val_str) > 50:\n",
    "                    val_str = val_str[:47] + \"...\"\n",
    "                \n",
    "                row_lines.append(f\"    {col}: {val_str}\")\n",
    "            \n",
    "            if len(row_lines) > 15:\n",
    "                row_lines = row_lines[:15] + [\"    ... (more columns)\"]\n",
    "            \n",
    "            return \"\\n\".join(info_lines + row_lines)\n",
    "            \n",
    "        except Exception as e:\n",
    "            return f\"\\n{file_path.name}: [Error reading file: {str(e)}]\"\n",
    "    \n",
    "    def generate(self) -> str:\n",
    "        \"\"\"Generate the complete directory tree with optional parquet samples.\"\"\"\n",
    "        if not self.path.exists():\n",
    "            return f\"Error: Path '{self.path}' does not exist.\"\n",
    "        \n",
    "        if not self.path.is_dir():\n",
    "            return f\"Error: Path '{self.path}' is not a directory.\"\n",
    "        \n",
    "        self.parquet_files = []\n",
    "        \n",
    "        lines = [f\"{self.path.name}/\"]\n",
    "        lines.extend(self._generate_tree(self.path))\n",
    "        \n",
    "        result = '\\n'.join(lines)\n",
    "        \n",
    "        if self.parquet_files and self.show_parquet_sample and HAS_PANDAS:\n",
    "            result += \"\\n\\n\" + \"=\"*60\n",
    "            result += \"\\nPARQUET FILE SAMPLES:\"\n",
    "            result += \"\\n\" + \"=\"*60\n",
    "            \n",
    "            for parquet_file in self.parquet_files:\n",
    "                result += self._get_parquet_sample(parquet_file)\n",
    "                result += \"\\n\"\n",
    "        elif self.parquet_files and not HAS_PANDAS:\n",
    "            result += \"\\n\\n\" + \"=\"*60\n",
    "            result += f\"\\nFound {len(self.parquet_files)} parquet files (install pandas for preview)\"\n",
    "            result += \"\\n\" + \"=\"*60\n",
    "        \n",
    "        return result\n",
    "\n",
    "\n",
    "def show_tree(\n",
    "    path: str = r\"D:\\Documents\\daily\\E_Plus_2040_py\\output\",\n",
    "    include: Optional[List[str]] = None,\n",
    "    exclude: Optional[List[str]] = None,\n",
    "    max_depth: Optional[int] = None,\n",
    "    show_size: bool = False,\n",
    "    show_parquet: bool = True,\n",
    "    dirs_only: bool = False\n",
    "):\n",
    "    \"\"\"Show directory tree with optional parquet preview.\"\"\"\n",
    "    tree = DirectoryTree(\n",
    "        path=path,\n",
    "        include_patterns=include,\n",
    "        exclude_patterns=exclude,\n",
    "        max_depth=max_depth,\n",
    "        show_size=show_size,\n",
    "        dirs_only=dirs_only,\n",
    "        show_parquet_sample=show_parquet\n",
    "    )\n",
    "    \n",
    "    print(tree.generate())\n",
    "\n",
    "\n",
    "def list_all_files(\n",
    "    path: str = r\"D:\\Documents\\daily\\E_Plus_2040_py\\output\",\n",
    "    pattern: str = \"*\",\n",
    "    recursive: bool = True\n",
    ") -> List[Path]:\n",
    "    \"\"\"List all files matching pattern.\"\"\"\n",
    "    base_path = Path(path)\n",
    "    if recursive:\n",
    "        files = list(base_path.rglob(pattern))\n",
    "    else:\n",
    "        files = list(base_path.glob(pattern))\n",
    "    return sorted(files)\n",
    "\n",
    "\n",
    "def get_directory_stats(path: str = r\"D:\\Documents\\daily\\E_Plus_2040_py\\output\"):\n",
    "    \"\"\"Get statistics about the directory.\"\"\"\n",
    "    base_path = Path(path)\n",
    "    \n",
    "    if not base_path.exists():\n",
    "        print(f\"Path does not exist: {path}\")\n",
    "        return\n",
    "    \n",
    "    total_size = 0\n",
    "    file_count = 0\n",
    "    dir_count = 0\n",
    "    file_types = {}\n",
    "    \n",
    "    for item in base_path.rglob(\"*\"):\n",
    "        if item.is_file():\n",
    "            file_count += 1\n",
    "            size = item.stat().st_size\n",
    "            total_size += size\n",
    "            \n",
    "            ext = item.suffix.lower()\n",
    "            if ext not in file_types:\n",
    "                file_types[ext] = {\"count\": 0, \"size\": 0}\n",
    "            file_types[ext][\"count\"] += 1\n",
    "            file_types[ext][\"size\"] += size\n",
    "        elif item.is_dir():\n",
    "            dir_count += 1\n",
    "    \n",
    "    print(f\"Directory Statistics for: {base_path.name}\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Total files: {file_count:,}\")\n",
    "    print(f\"Total directories: {dir_count:,}\")\n",
    "    print(f\"Total size: {total_size / (1024**3):.2f} GB\")\n",
    "    print(f\"\\nFile types:\")\n",
    "    \n",
    "    for ext, info in sorted(file_types.items(), key=lambda x: x[1][\"size\"], reverse=True):\n",
    "        if ext == \"\":\n",
    "            ext = \"[no extension]\"\n",
    "        size_mb = info[\"size\"] / (1024**2)\n",
    "        print(f\"  {ext}: {info['count']:,} files, {size_mb:.1f} MB\")\n",
    "\n",
    "\n",
    "# Quick fix for pandas issue\n",
    "def fix_pandas_issue():\n",
    "    \"\"\"Instructions to fix the pandas import issue.\"\"\"\n",
    "    print(\"To fix the pandas import issue, try these solutions:\")\n",
    "    print(\"\\n1. Check for circular imports:\")\n",
    "    print(\"   - Make sure you don't have a file named 'pandas.py' in your directory\")\n",
    "    print(\"   - Check: import os; print(os.listdir('.'))\")\n",
    "    print(\"\\n2. Reinstall pandas:\")\n",
    "    print(\"   In a new cell, run:\")\n",
    "    print(\"   !pip uninstall pandas -y\")\n",
    "    print(\"   !pip install pandas\")\n",
    "    print(\"\\n3. If using conda:\")\n",
    "    print(\"   !conda update pandas\")\n",
    "    print(\"\\n4. Restart kernel:\")\n",
    "    print(\"   Kernel -> Restart Kernel\")\n",
    "    print(\"\\n5. Check for conflicting installations:\")\n",
    "    print(\"   import sys\")\n",
    "    print(\"   print(sys.path)\")\n",
    "\n",
    "\n",
    "# Simple usage without pandas\n",
    "def simple_tree(path: str = r\"D:\\Documents\\daily\\E_Plus_2040_py\\output\", max_depth: int = 3):\n",
    "    \"\"\"Simple directory tree without any dependencies.\"\"\"\n",
    "    def print_tree(directory, prefix=\"\", depth=0):\n",
    "        if max_depth and depth >= max_depth:\n",
    "            return\n",
    "        \n",
    "        try:\n",
    "            entries = sorted(Path(directory).iterdir(), key=lambda x: (not x.is_dir(), x.name.lower()))\n",
    "            for i, entry in enumerate(entries):\n",
    "                is_last = i == len(entries) - 1\n",
    "                current = \"└── \" if is_last else \"├── \"\n",
    "                print(prefix + current + entry.name)\n",
    "                \n",
    "                if entry.is_dir():\n",
    "                    extension = \"    \" if is_last else \"│   \"\n",
    "                    print_tree(entry, prefix + extension, depth + 1)\n",
    "        except PermissionError:\n",
    "            print(prefix + \"[Permission Denied]\")\n",
    "    \n",
    "    print(f\"{Path(path).name}/\")\n",
    "    print_tree(path)\n",
    "\n",
    "\n",
    "# Main info when running\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Directory Tree Generator Loaded Successfully!\")\n",
    "    print(\"\\nQuick usage:\")\n",
    "    print(\"  show_tree()                    # Full tree with parquet preview\")\n",
    "    print(\"  simple_tree()                  # Simple tree (no pandas needed)\")\n",
    "    print(\"  get_directory_stats()          # Directory statistics\")\n",
    "    print(\"  fix_pandas_issue()             # Help fix pandas import\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DDsaie",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
