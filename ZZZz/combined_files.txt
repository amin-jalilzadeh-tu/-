File: D:\Documents\E_Plus_2030_py\cal\unified_surrogate.py
============================================================
"""
unified_surrogate.py

Purpose:
  1) Loads scenario-based parameter CSVs (dhw, elec, fenez, hvac, vent) from 
     the scenario_folder, converting any known categorical text (e.g. "Electricity") 
     to numeric. Skips unknown text values.
  2) Pivots them so each scenario is one row, each parameter is one column.
  3) Loads simulation results (like merged_daily_mean_mocked.csv) which 
     has columns [BuildingID, VariableName, Day1, Day2, ...].
  4) Sums daily columns => "TotalEnergy_J" per (BuildingID, VariableName).
  5) Merges scenario data with E+ results, focusing on a single or multiple 
     target_variable(s).
  6) Trains a RandomForest surrogate (single-output if target_variable is a string, 
     multi-output if it's a list).
  7) Saves the fitted model and the list of feature columns for future predictions.

Typical usage in main.py:
    if sur_cfg.get("perform_surrogate", False):
        df_scen = sur_load_scenario_params(scenario_folder)
        pivot_df = pivot_scenario_params(df_scen)
        # optional: pivot_df = filter_top_parameters(pivot_df, "morris_sensitivity.csv", top_n=5)
        df_sim = load_sim_results(results_csv)
        df_agg = aggregate_results(df_sim)
        merged_df = merge_params_with_results(pivot_df, df_agg, target_var)
        build_and_save_surrogate(
            df_data=merged_df,
            target_col=target_var,
            model_out_path=model_out,
            columns_out_path=cols_out,
            test_size=test_size
        )

Author: Your Team
"""

import os
import pandas as pd
import numpy as np
import joblib
from typing import Optional, List, Union
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split, RandomizedSearchCV
from sklearn.multioutput import MultiOutputRegressor
from sklearn.metrics import r2_score, mean_absolute_error


###############################################################################
# 1) HELPER: Encode known text -> numeric
###############################################################################

def encode_categorical_if_known(param_name: str, param_value) -> Optional[float]:
    """
    1) Attempt float conversion
    2) If fails, check known label encodings
    3) If still unknown => return None => skip row

    Modify or expand to handle your typical discrete strings.
    """
    if param_value is None or pd.isna(param_value):
        return None

    # (A) Direct float conversion
    try:
        return float(param_value)
    except (ValueError, TypeError):
        pass

    # (B) Known label encodings

    # Example: "Electricity" => 0.0, "Gas" => 1.0
    if param_value == "Electricity":
        return 0.0
    elif param_value == "Gas":
        return 1.0

    # Example: Roughness
    rough_map = {
        "Smooth": 0.0,
        "MediumSmooth": 1.0,
        "MediumRough": 2.0,
        "Rough": 3.0
    }
    if param_value in rough_map:
        return rough_map[param_value]

    # Example: "Yes"/"No" => 1.0 / 0.0
    if param_value in ["Yes", "No"]:
        return 1.0 if param_value == "Yes" else 0.0

    # Not recognized => skip
    return None


###############################################################################
# 2) SCENARIO LOADING & PIVOT
###############################################################################

def load_scenario_file(filepath: str) -> pd.DataFrame:
    """
    Reads one scenario CSV, ensures a column 'assigned_value' which is numeric.
    If row can't be converted => skip. Returns a numeric-only DataFrame for that file.
    """
    df_in = pd.read_csv(filepath)

    # unify to 'assigned_value'
    if "assigned_value" not in df_in.columns and "param_value" in df_in.columns:
        df_in.rename(columns={"param_value": "assigned_value"}, inplace=True)

    # We'll keep only rows that produce a numeric assigned_value
    rows_out = []
    for _, row in df_in.iterrows():
        val = row.get("assigned_value", None)
        if val is None or pd.isna(val):
            continue

        # Attempt numeric or known label
        param_name = str(row.get("param_name", ""))
        num_val = encode_categorical_if_known(param_name, val)
        if num_val is None:
            # skip
            continue

        new_row = row.copy()
        new_row["assigned_value"] = num_val
        rows_out.append(new_row)

    if not rows_out:
        return pd.DataFrame()

    return pd.DataFrame(rows_out)


def load_scenario_params(scenario_folder: str) -> pd.DataFrame:
    """
    Merges scenario_params_{dhw, elec, fenez, hvac, vent}.csv from scenario_folder.
    Each file is label-encoded. Unknown text -> skipped.
    Returns a unified DataFrame with columns like:
      [scenario_index, param_name, assigned_value, ogc_fid, ...]
    """
    scenario_files = [
        "scenario_params_dhw.csv",
        "scenario_params_elec.csv",
        "scenario_params_fenez.csv",
        "scenario_params_hvac.csv",
        "scenario_params_vent.csv"
    ]

    all_dfs = []
    for fname in scenario_files:
        fpath = os.path.join(scenario_folder, fname)
        if not os.path.isfile(fpath):
            print(f"[INFO] Not found => {fpath}")
            continue

        df_scenario = load_scenario_file(fpath)
        if df_scenario.empty:
            print(f"[WARN] No numeric row data in => {fpath} (skipped all).")
        else:
            # Optionally add a 'source_file' column
            df_scenario["source_file"] = fname
            all_dfs.append(df_scenario)

    if not all_dfs:
        raise FileNotFoundError(f"[ERROR] No scenario CSV with numeric data found in '{scenario_folder}'.")
    return pd.concat(all_dfs, ignore_index=True)


def pivot_scenario_params(df: pd.DataFrame) -> pd.DataFrame:
    """
    Pivots so each scenario_index is a row, each param_name is a column, assigned_value are cells.
    Also preserves 'ogc_fid' if present.

    Example final columns:
      scenario_index, ogc_fid, paramA, paramB, ...
    """
    if "scenario_index" not in df.columns or "param_name" not in df.columns or "assigned_value" not in df.columns:
        raise ValueError("DataFrame must have columns: scenario_index, param_name, assigned_value")

    if "ogc_fid" not in df.columns:
        df["ogc_fid"] = 0

    pivot_df = df.pivot_table(
        index=["scenario_index", "ogc_fid"],
        columns="param_name",
        values="assigned_value",
        aggfunc="first"
    ).reset_index()

    pivot_df.columns.name = None
    return pivot_df


def filter_top_parameters(
    df_pivot: pd.DataFrame,
    sensitivity_csv: str,
    top_n: int,
    param_col: str = "param",
    metric_col: str = "mu_star"
) -> pd.DataFrame:
    """
    Reads a Morris sensitivity CSV, picks top_n 'param' by mu_star, 
    filters df_pivot to only those columns plus scenario_index, ogc_fid.
    """
    if not os.path.isfile(sensitivity_csv):
        print(f"[INFO] Sensitivity file '{sensitivity_csv}' not found => skipping filter.")
        return df_pivot

    sens_df = pd.read_csv(sensitivity_csv)
    if param_col not in sens_df.columns or metric_col not in sens_df.columns:
        print(f"[ERROR] param_col='{param_col}' or metric_col='{metric_col}' not in {sensitivity_csv}.")
        return df_pivot

    top_params = sens_df.sort_values(metric_col, ascending=False)[param_col].head(top_n).tolist()
    keep_cols = ["scenario_index", "ogc_fid"] + [p for p in top_params if p in df_pivot.columns]
    filtered = df_pivot[keep_cols].copy()
    print(f"[INFO] Filtered pivot from {df_pivot.shape} -> {filtered.shape} using top {top_n} params.")
    return filtered


###############################################################################
# 3) LOADING & AGGREGATING SIM RESULTS
###############################################################################

def load_sim_results(results_csv: str) -> pd.DataFrame:
    """
    Typically: [BuildingID, VariableName, Day1, Day2, ...]
    """
    return pd.read_csv(results_csv)


def aggregate_results(df_sim: pd.DataFrame) -> pd.DataFrame:
    """
    Sums across days => [BuildingID, VariableName, TotalEnergy_J].
    Ensures we have "BuildingID", "VariableName".
    """
    needed = {"BuildingID", "VariableName"}
    if not needed.issubset(df_sim.columns):
        raise ValueError("df_sim must have columns BuildingID, VariableName plus day columns.")
    melted = df_sim.melt(
        id_vars=["BuildingID", "VariableName"],
        var_name="Day",
        value_name="Value"
    )
    daily_sum = melted.groupby(["BuildingID", "VariableName"])["Value"].sum().reset_index()
    daily_sum.rename(columns={"Value": "TotalEnergy_J"}, inplace=True)
    return daily_sum


def merge_params_with_results(
    pivot_df: pd.DataFrame,
    df_agg: pd.DataFrame,
    target_var: Union[str, List[str], None] = None
) -> pd.DataFrame:
    """
    Merges pivoted scenario data (with columns [scenario_index->BuildingID, paramA..])
    + aggregated results => single DataFrame for model training.

    If target_var is None => merges all "VariableName" => multiple rows per building.
    If str => picks that var => single column => rename to var name => merges one row per building
    If list => pivot each var => multi columns => merges one row per building => multi-output
    """
    merged = pivot_df.copy()
    # rename scenario_index => BuildingID
    merged.rename(columns={"scenario_index": "BuildingID"}, inplace=True)

    if target_var is None:
        # Just join, no filtering => multiple rows
        return pd.merge(merged, df_agg, on="BuildingID", how="inner")

    if isinstance(target_var, str):
        # single var => one column
        df_sub = df_agg[df_agg["VariableName"] == target_var].copy()
        df_sub.rename(columns={"TotalEnergy_J": target_var}, inplace=True)
        df_sub.drop(columns=["VariableName"], inplace=True, errors="ignore")
        return pd.merge(merged, df_sub, on="BuildingID", how="inner")

    if isinstance(target_var, list):
        # multi-output => pivot each var => multiple columns
        df_sub = df_agg[df_agg["VariableName"].isin(target_var)]
        pivot_vars = df_sub.pivot(
            index="BuildingID",
            columns="VariableName",
            values="TotalEnergy_J"
        ).reset_index()
        return pd.merge(merged, pivot_vars, on="BuildingID", how="inner")

    raise ValueError("target_var must be None, str, or list[str].")


###############################################################################
# 4) SURROGATE TRAINING
###############################################################################

def build_and_save_surrogate(
    df_data: pd.DataFrame,
    target_col: Union[str, List[str]] = "TotalEnergy_J",
    model_out_path: str = "surrogate_model.joblib",
    columns_out_path: str = "surrogate_columns.joblib",
    test_size: float = 0.3,
    random_state: int = 42
):
    """
    1) Splits data into X,y. If target_col is a single string => single-output.
       If list => multi-output.
    2) Builds a RandomForest via RandomizedSearchCV => best params.
    3) If multi-output => wraps in MultiOutputRegressor.
    4) Saves model + list of feature columns.

    Returns (model, feature_cols).
    """
    # 1) Determine target columns
    if isinstance(target_col, str):
        if target_col not in df_data.columns:
            print(f"[ERROR] target_col '{target_col}' not in df_data.")
            return None, None
        y_data = df_data[[target_col]].copy()
        multi_output = False
    elif isinstance(target_col, list):
        missing = [t for t in target_col if t not in df_data.columns]
        if missing:
            print(f"[ERROR] Some target columns missing: {missing}")
            return None, None
        y_data = df_data[target_col].copy()
        multi_output = (len(target_col) > 1)
    else:
        print("[ERROR] target_col must be str or list[str].")
        return None, None

    # 2) Build features
    exclude_cols = ["BuildingID", "ogc_fid", "VariableName", "source_file"]
    if multi_output:
        exclude_cols.extend(target_col)
    else:
        exclude_cols.append(target_col)

    candidate_cols = [c for c in df_data.columns if c not in exclude_cols]
    # Keep only numeric columns
    numeric_cols = [c for c in candidate_cols if pd.api.types.is_numeric_dtype(df_data[c])]

    if not numeric_cols:
        print("[ERROR] No numeric feature columns found => can't train surrogate.")
        return None, None

    # Drop rows with any NaN in X or y
    full_df = df_data[numeric_cols + list(y_data.columns)].dropna()
    if full_df.empty:
        print("[ERROR] All data is NaN => can't train surrogate.")
        return None, None

    X_data = full_df[numeric_cols]
    Y_data = full_df[y_data.columns]

    # Must have enough rows
    if len(X_data) < 5:
        print(f"[ERROR] Not enough data => only {len(X_data)} row(s).")
        return None, None

    # 3) Train/test split
    X_train, X_test, Y_train, Y_test = train_test_split(
        X_data, Y_data, test_size=test_size, random_state=random_state
    )

    # 4) RandomizedSearchCV for best RF params
    param_dist = {
        "n_estimators": [50, 100, 200],
        "max_depth": [None, 5, 10, 20],
        "max_features": ["auto", "sqrt", 0.5]
    }
    base_rf = RandomForestRegressor(random_state=random_state)
    search = RandomizedSearchCV(
        base_rf,
        param_distributions=param_dist,
        n_iter=10,
        cv=3,
        random_state=random_state,
        n_jobs=-1
    )

    if multi_output:
        # Use only first target col for the hyperparam search 
        # (or do a multi-object approach if you prefer).
        first_col = Y_train.columns[0]
        search.fit(X_train, Y_train[first_col].values.ravel())
        best_params = search.best_params_

        # Then train multi-output
        best_rf = RandomForestRegressor(random_state=random_state, **best_params)
        model = MultiOutputRegressor(best_rf)
        model.fit(X_train, Y_train)
    else:
        # Single-output
        search.fit(X_train, Y_train.values.ravel())
        best_params = search.best_params_
        best_rf = RandomForestRegressor(random_state=random_state, **best_params)
        best_rf.fit(X_train, Y_train.values.ravel())
        model = best_rf

    # 5) Evaluate
    Y_pred_train = model.predict(X_train)
    Y_pred_test  = model.predict(X_test)

    # Ensure shape for multi-output
    if not multi_output:
        Y_pred_train = Y_pred_train.reshape(-1, 1)
        Y_pred_test  = Y_pred_test.reshape(-1, 1)

    print("\n[Surrogate Training Summary]")
    print(f"Best Params: {best_params}")

    for i, col_name in enumerate(Y_data.columns):
        r2_train = r2_score(Y_train.iloc[:, i], Y_pred_train[:, i])
        r2_test  = r2_score(Y_test.iloc[:, i],  Y_pred_test[:, i])
        mae_test = mean_absolute_error(Y_test.iloc[:, i], Y_pred_test[:, i])
        print(f"Target='{col_name}': Train R2={r2_train:.3f},  Test R2={r2_test:.3f},  MAE={mae_test:.3f}")

    # 6) Save model & columns
    joblib.dump(model, model_out_path)
    joblib.dump(numeric_cols, columns_out_path)
    print(f"[INFO] Saved surrogate model => {model_out_path}")
    print(f"[INFO] Saved columns => {columns_out_path}")

    return model, numeric_cols


###############################################################################
# 5) LOADING A TRAINED SURROGATE, PREDICTING
###############################################################################

def load_surrogate_and_predict(
    model_path: str,
    columns_path: str,
    sample_features: dict
):
    """
    1) Load trained model + feature columns
    2) Convert sample_features => row DataFrame with the same columns
    3) predict => returns array
    """
    # Load
    model = joblib.load(model_path)
    feature_cols = joblib.load(columns_path)

    # Construct DF
    df_sample = pd.DataFrame([sample_features])
    # Insert missing columns
    for col in feature_cols:
        if col not in df_sample.columns:
            df_sample[col] = 0.0  # or np.nan

    df_sample = df_sample[feature_cols].fillna(0.0)

    # Predict
    y_pred = model.predict(df_sample)
    return y_pred

------------------------------------------------------------

File: D:\Documents\E_Plus_2030_py\combined.json
============================================================
{
  "dhw": [
    {
      "building_id": 413673000,
      "param_name": "occupant_density_m2_per_person",
      "min_val": 127.0,
      "max_val": 233.0
    },
    {
      "building_id": 413673000,
      "param_name": "liters_per_person_per_day",
      "fixed_value": 145.0
    },
    {
      "building_function": "residential00",
      "age_range": "1992-2005",
      "param_name": "setpoint_c",
      "min_val": 58.0,
      "max_val": 60.0
    },
    {
      "building_function": "non_residential000",
      "param_name": "occupant_density_m2_per_person",
      "min_val": 12.0,
      "max_val": 18.0
    }
  ],
  "epw": [
    {
      "building_id": 413673000,
      "fixed_epw_path": "data/weather/2050.epw"
    },
    {
      "desired_year": 2050,
      "override_year_to": 2020
    }
  ],
  "zone_sizing": [
    {
      "param_name": "cooling_supply_air_temp",
      "min_val": 12.0,
      "max_val": 15.0
    },
    {
      "param_name": "heating_supply_air_temp",
      "min_val": 40.0,
      "max_val": 50.0
    },
    {
      "param_name": "cooling_supply_air_hr",
      "min_val": 0.008,
      "max_val": 0.010
    },
    {
      "param_name": "heating_supply_air_hr",
      "min_val": 0.003,
      "max_val": 0.005
    },
    {
      "param_name": "cooling_design_air_flow_method",
      "choices": [
        "Flow/Zone",
        "DesignDay",
        "DesignDayWithLimit"
      ]
    }
  ],
  "equipment": [
    {
      "param_name": "equip_wm2",
      "min_val": 3.0,
      "max_val": 7.0
    },
    {
      "param_name": "tD",
      "description": "Schedule value for day occupancy (W)",
      "min_val": 400.0,
      "max_val": 800.0
    },
    {
      "param_name": "tN",
      "description": "Schedule value for night occupancy (W)",
      "min_val": 150.0,
      "max_val": 350.0
    }
  ],
  "shading": [
    {
      "param_name": "slat_angle_deg",
      "min_val": 30.0,
      "max_val": 60.0
    },
    {
      "param_name": "slat_beam_solar_reflectance",
      "min_val": 0.6,
      "max_val": 0.8
    },
    {
      "param_name": "blind_to_glass_distance",
      "fixed_value": 0.075
    },
    {
      "param_name": "slat_width",
      "min_val": 0.020,
      "max_val": 0.030
    }
  ],
  "mail_user": [
    {
      "email": "amin.jalilzade1@gmail.com"
    }
  ],
  "fenestration": [
    {
      "building_id": 413673000,
      "building_function": "residential",
      "building_type": "Two-and-a-half-story House",
      "age_range": "1992-2005",
      "scenario": "scenario1",
      "param_name": "wwr",
      "min_val": 0.025,
      "max_val": 0.03
    },
    {
      "building_function": "non_residential0",
      "age_range": "2015 and later",
      "scenario": "scenario1",
      "param_name": "roof_R_value",
      "fixed_value": 3.0
    }
  ],
  "geometry": [
    {
      "building_id": 413673,
      "param_name": "perimeter_depth",
      "min_val": 3.5,
      "max_val": 4.0,
      "fixed_value": true
    },
    {
      "building_id": 4136733,
      "param_name": "has_core",
      "fixed_value": true
    },
    {
      "building_id": 4136737,
      "param_name": "has_core",
      "fixed_value": false
    },
    {
      "building_type": "Meeting Function",
      "param_name": "has_core",
      "fixed_value": true
    },
    {
      "building_id": 4136738,
      "param_name": "has_core",
      "fixed_value": false
    }
  ],
  "hvac": [
    {
      "building_id": 413673000,
      "param_name": "heating_day_setpoint",
      "min_val": 10.0,
      "max_val": 11.0
    },
    {
      "building_function": "residential0",
      "param_name": "cooling_day_setpoint",
      "fixed_value": 25.6
    },
    {
      "scenario": "scenario10",
      "age_range": "2015 and later",
      "param_name": "max_heating_supply_air_temp",
      "min_val": 48.6,
      "max_val": 50.0
    }
  ],
  "lighting": [
    {
      "building_id": 413673000,
      "param_name": "lights_wm2",
      "min_val": 18.0,
      "max_val": 20.0
    },
    {
      "building_type": "Residential0",
      "param_name": "tN",
      "min_val": 10999,
      "max_val": 11999
    },
    {
      "building_id": 4136731,
      "building_type": "non_residential",
      "param_name": "parasitic_wm2",
      "min_val": 0.28,
      "max_val": 0.3
    }
  ],
  "main_config": {
    "paths": {
      "building_data": "data/df_buildings.csv",
      "fenez_excel": "excel_data/envelop.xlsx",
      "dhw_excel": "excel_data/dhw_overrides.xlsx",
      "epw_excel": "excel_data/epw_overrides.xlsx",
      "lighting_excel": "excel_data/lighting_overrides.xlsx",
      "hvac_excel": "excel_data/hvac_overrides.xlsx",
      "vent_excel": "excel_data/vent_overrides.xlsx"
    },
    "use_database": false,
    "filter_by": "meestvoorkomendepostcode",
    "db_filter": {
      "meestvoorkomendepostcode": [
        "3066HG"
      ],
      "pand_id": [
        "0383100000001369",
        "0383100000001370",
        "5473VX"
      ],
      "pand_ids": [
        "XYZ123",
        "XYZ999"
      ],
      "bbox_xy": [
        120000.0,
        487000.0,
        121000.0,
        488000.0
      ],
      "bbox_latlon": [
        51.873,
        5.589,
        51.878,
        5.6
      ]
    },
    "excel_overrides": {
      "override_fenez_excel": false,
      "override_dhw_excel": false,
      "override_epw_excel": false,
      "override_lighting_excel": false,
      "override_hvac_excel": false,
      "override_vent_excel": false
    },
    "user_config_overrides": {
      "override_fenez_json": true,
      "override_dhw_json": true,
      "override_epw_json": true,
      "override_lighting_json": true,
      "override_hvac_json": true,
      "override_vent_json": true,
      "override_geometry_json": true,
      "override_shading_json": false
    },
    "idf_creation": {
      "perform_idf_creation": true,
      "scenario": "scenario1",
      "calibration_stage": "pre_calibration",
      "strategy": "B",
      "random_seed": 42,
      "iddfile": "EnergyPlus/Energy+.idd",
      "idf_file_path": "EnergyPlus/Minimal.idf",
      "output_idf_dir": "output_IDFs",
      "run_simulations": true,
      "simulate_config": {
        "num_workers": 4,
        "ep_force_overwrite": true
      },
      "post_process": true,
      "post_process_config": {
        "base_output_dir": "Sim_Results",
        "outputs": [
          {
            "convert_to_daily": true,
            "convert_to_monthly": false,
            "aggregator": "none",
            "output_csv": "output/results/merged_as_is.csv"
          },
          {
            "convert_to_daily": true,
            "convert_to_monthly": false,
            "aggregator": "mean",
            "output_csv": "output/results/merged_daily_mean.csv"
          },
          {
            "convert_to_daily": true,
            "convert_to_monthly": true,
            "aggregator": "mean",
            "output_csv": "output/results/merged_monthly_mean.csv"
          }
        ]
      },
      "output_definitions": {
        "desired_variables": [
          "Facility Total Electric Demand Power",
          "Zone Air Temperature",
          "Boiler Heating Energy",
          "Water Heater Heating Energy"
        ],
        "desired_meters": [
          "Electricity:Facility",
          "Heating:EnergyTransfer",
          "Cooling:EnergyTransfer"
        ],
        "override_variable_frequency": "Daily",
        "override_meter_frequency": "Daily",
        "include_tables": true,
        "include_summary": true
      }
    },
    "structuring": {
      "perform_structuring": true,
      "dhw": {
        "csv_in": "assigned/assigned_dhw_params.csv",
        "csv_out": "assigned/structured_dhw_params.csv"
      },
      "fenestration": {
        "csv_in": "assigned/assigned_fenez_params.csv",
        "csv_out": "assigned/structured_fenez_params.csv"
      },
      "hvac": {
        "csv_in": "assigned/assigned_hvac_params.csv",
        "build_out": "assigned/assigned_hvac_building.csv",
        "zone_out": "assigned/assigned_hvac_zones.csv"
      },
      "vent": {
        "csv_in": "assigned/assigned_ventilation.csv",
        "build_out": "assigned/assigned_vent_building.csv",
        "zone_out": "assigned/assigned_vent_zones.csv"
      },
      "shading": {
        "csv_in": "assigned/assigned_shading_params.csv",
        "csv_out": "assigned/structured_shading_params.csv"
      },
      "equipment": {
        "csv_in": "assigned/assigned_equipment.csv",
        "csv_out": "assigned/structured_equipment.csv"
      },
      "zone_sizing": {
        "csv_in": "assigned/assigned_zone_sizing_outdoor_air.csv",
        "csv_out": "assigned/structured_zone_sizing.csv"
      }
    },
    "modification": {
      "perform_modification": true,
      "modify_config": {
        "idd_path": "EnergyPlus/Energy+.idd",
        "assigned_csv": {
          "hvac_building": "assigned/assigned_hvac_building.csv",
          "hvac_zones": "assigned/assigned_hvac_zones.csv",
          "dhw": "assigned/assigned_dhw_params.csv",
          "vent_build": "assigned/assigned_vent_building.csv",
          "vent_zones": "assigned/assigned_vent_zones.csv",
          "elec": "assigned/assigned_lighting.csv",
          "fenez": "assigned/structured_fenez_params.csv",
          "shading": "assigned/structured_shading_params.csv",
          "equip": "assigned/structured_equipment.csv",
          "zone_sizing": "assigned/structured_zone_sizing.csv"
        },
        "scenario_csv": {
          "hvac": "scenarios/scenario_params_hvac.csv",
          "dhw": "scenarios/scenario_params_dhw.csv",
          "vent": "scenarios/scenario_params_vent.csv",
          "elec": "scenarios/scenario_params_elec.csv",
          "fenez": "scenarios/scenario_params_fenez.csv",
          "shading": "scenarios/scenario_params_shading.csv",
          "equip": "scenarios/scenario_params_equipment.csv",
          "zone_sizing": "scenarios/scenario_params_zone_sizing.csv"
        },
        "output_idf_dir": "scenario_idfs",
        "building_id": 4136733,
        "num_scenarios": 5,
        "picking_method": "random_uniform",
        "picking_scale_factor": 0.5,
        "run_simulations": true,
        "simulation_config": {
          "num_workers": 4,
          "output_dir": "Sim_Results/Scenarios"
        },
        "perform_post_process": true,
        "post_process_config": {
          "output_csv_as_is": "results_scenarioes/merged_as_is_scenarios.csv",
          "output_csv_daily_mean": "results_scenarioes/merged_daily_mean_scenarios.csv"
        },
        "perform_validation": false,
        "validation_config": {
          "real_data_csv": "data/mock_merged_daily_mean.csv",
          "sim_data_csv": "results/merged_daily_mean.csv",
          "bldg_ranges": {
            "0": [
              0,
              1,
              2
            ]
          },
          "variables_to_compare": [
            "Electricity:Facility [J](Hourly)",
            "Heating:EnergyTransfer [J](Hourly)",
            "Cooling:EnergyTransfer [J](Hourly)"
          ],
          "threshold_cv_rmse": 30.0,
          "skip_plots": true,
          "output_csv": "scenario_validation_report.csv"
        }
      }
    },
    "validation_base": {
      "perform_validation": true,
      "config": {
        "real_data_csv": "data/mock_merged_daily_mean.csv",
        "sim_data_csv": "results/merged_daily_mean.csv",
        "bldg_ranges": {"0": [0, 1, 2, 3]       },
        "variables_to_compare": [
          "Electricity:Facility [J](Hourly)", "Heating:EnergyTransfer [J](Hourly)", "Cooling:EnergyTransfer [J](Hourly)"],
        "threshold_cv_rmse": 30.0,
        "skip_plots": true,
        "output_csv": "validation_report_base.csv",

        "analysis_options": {
          "date_format":  [
            "%d/%m/%Y %H:%M",
            "%d/%m/%y",
            "%Y-%m-%d %H:%M:%S"
          ],  
          "granularity": ["annual", "monthly", "seasonal"],
          "weekday_weekend": true,
          "diurnal_profiles": true,
          "peak_analysis": {
            "perform": true,
            "n_peaks": 10
          },
          "ramp_rate_analysis": true,
          "extreme_day_analysis": {
            "perform": true,
            "n_days": 5,
            "weather_variable": "Environment:Site Outdoor Air Drybulb Temperature [C](Daily)"
      }}}
    },
    "validation_scenarios": {
      "perform_validation": false,
      "config": {
        "real_data_csv": "data/mock_merged_daily_mean.csv",
        "sim_data_csv": "results_scenarioes/merged_daily_mean_scenarios.csv",
        "bldg_ranges": {
          "0": [
            0,
            1,
            2
          ]
        },
        "variables_to_compare": [
          "Electricity:Facility [J](Hourly)",
          "Heating:EnergyTransfer [J](Hourly)",
          "Cooling:EnergyTransfer [J](Hourly)"
        ],
        "threshold_cv_rmse": 30.0,
        "skip_plots": true,
        "output_csv": "validation_report_scenarios.csv"
      }
    },
    "sensitivity": {
      "perform_sensitivity": true,
      "scenario_folder": "scenarios",
      "method": "morris",
      "results_csv": "results_scenarioes/merged_daily_mean_scenarios.csv",
      "target_variable": [
        "Heating:EnergyTransfer [J](Hourly)",
        "Cooling:EnergyTransfer [J](Hourly)",
        "Electricity:Facility [J](Hourly)"
      ],
      "output_csv": "multi_corr_sensitivity.csv",
      "n_morris_trajectories": 10,
      "num_levels": 4
    },
    "surrogate": {
      "perform_surrogate": true,
      "scenario_folder": "scenarios",
      "results_csv": "results_scenarioes/merged_daily_mean_scenarios.csv",
      "target_variable": "Heating:EnergyTransfer [J](Hourly)",
      "model_out": "heating_surrogate_model.joblib",
      "cols_out": "heating_surrogate_columns.joblib",
      "test_size": 0.3
    },
    "calibration": {
      "perform_calibration": true,
      "scenario_folder": "scenarios",
      "scenario_files": [
        "scenario_params_dhw.csv",
        "scenario_params_elec.csv"
      ],
      "subset_sensitivity_csv": "multi_corr_sensitivity.csv",
      "top_n_params": 10,
      "method": "ga",
      "use_surrogate": true,
      "real_data_csv": "data/mock_merged_daily_mean.csv",
      "surrogate_model_path": "heating_surrogate_model.joblib",
      "surrogate_columns_path": "heating_surrogate_columns.joblib",
      "calibrate_min_max": true,
      "ga_pop_size": 10,
      "ga_generations": 5,
      "ga_crossover_prob": 0.7,
      "ga_mutation_prob": 0.2,
      "bayes_n_calls": 15,
      "random_n_iter": 20,
      "output_history_csv": "calibration_history.csv",
      "best_params_folder": "calibrated",
      "history_folder": "calibrated"
    }
  },
  "shading_tree": [
    {
      "building_id": 413673000,
      "param_name": "top_n_buildings",
      "fixed_value": 5
    },
    {
      "building_function": "residential",
      "param_name": "summer_value",
      "min_val": 0.4,
      "max_val": 0.6
    },
    {
      "param_name": "top_n_trees",
      "fixed_value": 3
    }
  ],
  "vent": [
    {
      "building_id": 413673000,
      "param_name": "infiltration_base",
      "min_val": 100.3,
      "max_val": 100.4
    },
    {
      "building_function": "residential",
      "age_range": "1992-2005",
      "param_name": "year_factor",
      "min_val": 21.4,
      "max_val": 21.5
    },
    {
      "building_id": 4136731,
      "param_name": "system_type",
      "fixed_value": "D"
    },
    {
      "building_id": 4136731,
      "param_name": "fan_pressure",
      "min_val": 100,
      "max_val": 120
    }
  ]
}
------------------------------------------------------------

File: D:\Documents\E_Plus_2030_py\orchestrator.py
============================================================
"""
orchestrator.py

Orchestrates the entire EnergyPlus workflow using a job-specific subfolder
for config files and a job-specific folder in /output for results.

Steps:
  1. Retrieve 'job_id' from job_config (set by job_manager or app).
  2. Form an output directory: <OUTPUT_DIR>/<job_id>.
  3. Load main_config.json from user_configs/<job_id>.
  4. Merge with posted_data["main_config"] if present.
  5. Apply Excel overrides, JSON overrides, create IDFs, run sims, etc.
  6. If scenario modification is enabled, override paths so scenario IDFs/results
     stay in the same job folder, then run scenario-based modifications.
  7. Perform structuring (e.g., flatten assigned CSVs) if requested.
  8. Perform global validation, sensitivity, surrogate, calibration if requested;
     patch any relative CSV paths to be inside the job folder (unless "data/").
  9. Zip & email final results if mail_user.json is present.
  10. Respect any cancel_event from job_manager.
"""

import os
import json
import logging
import threading
import time
from contextlib import contextmanager
import pandas as pd

# Splitting / deep-merge
from splitter import deep_merge_dicts

# DB loading if needed
from database_handler import load_buildings_from_db

# Excel overrides
from excel_overrides import (
    override_dhw_lookup_from_excel_file,
    override_epw_lookup_from_excel_file,
    override_lighting_lookup_from_excel_file,
    override_hvac_lookup_from_excel_file,
    override_vent_lookup_from_excel_file
)

# Fenestration config
from idf_objects.fenez.fenez_config_manager import build_fenez_config


# IDF creation
import idf_creation
from idf_creation import create_idfs_for_all_buildings

# Scenario modification
from main_modifi import run_modification_workflow

# Validation
from validation.main_validation import run_validation_process

# Sensitivity / Surrogate / Calibration
from cal.unified_sensitivity import run_sensitivity_analysis
from cal.unified_surrogate import (
    load_scenario_params as sur_load_scenario_params,
    pivot_scenario_params,
    load_sim_results,
    aggregate_results,
    merge_params_with_results,
    build_and_save_surrogate
)
from cal.unified_calibration import run_unified_calibration

# Zip & email
from zip_and_mail import zip_user_output, send_results_email

from cleanup_old_jobs import cleanup_old_results


class WorkflowCanceled(Exception):
    """Custom exception used to stop the workflow if a cancel_event is set."""
    pass


@contextmanager
def step_timer(logger, name: str):
    """Context manager to log step durations."""
    logger.info(f"[STEP] Starting {name} ...")
    start = time.perf_counter()
    try:
        yield
    finally:
        elapsed = time.perf_counter() - start
        logger.info(f"[STEP] Finished {name} in {elapsed:.2f} seconds.")


def orchestrate_workflow(job_config: dict, cancel_event: threading.Event = None):
    """
    Orchestrates the entire E+ workflow using a job-specific subfolder for config JSON,
    plus a job-specific output folder for results.

    Args:
        job_config (dict): includes:
            {
              "job_id": "<unique_id_for_this_job>",
              "job_subfolder": "user_configs/<job_id>",
              "posted_data": {...} (optional),
              ...
            }
        cancel_event (threading.Event): If set, we gracefully exit early.

    Raises:
        WorkflowCanceled if cancel_event is set mid-way.

    Returns:
        None (logs extensively, optionally zips/emails final results).
    """
    logger = logging.getLogger(__name__)
    logger.info("=== Starting orchestrate_workflow ===")
    overall_start = time.perf_counter()

    # -------------------------------------------------------------------------
    # 0) Identify job_id, define check_canceled
    # -------------------------------------------------------------------------
    job_id = job_config.get("job_id", "unknown_job_id")
    logger.info(f"[INFO] Orchestrator for job_id={job_id}")

    def check_canceled():
        """Raise WorkflowCanceled if cancel_event is set."""
        if cancel_event and cancel_event.is_set():
            logger.warning("=== CANCEL event detected. Stopping workflow. ===")
            raise WorkflowCanceled("Workflow was canceled by user request.")


    # -------------------------------------------------------------------------
    # 12) Helper to handle patching CSVs that are "relative" but not "data/".
    # -------------------------------------------------------------------------
    def patch_if_relative(csv_path: str):
        """
        1) If absolute, return as-is.
        2) If starts with 'data/', interpret as /usr/src/app/data/... (no job folder).
        3) Else, join with job_output_dir.
        """
        if not csv_path:
            return csv_path
        if os.path.isabs(csv_path):
            return csv_path
        if csv_path.startswith("data/"):
            return os.path.join("/usr/src/app", csv_path)
        return os.path.join(job_output_dir, csv_path)





    # -------------------------------------------------------------------------
    # 1) Identify the user_configs folder (where main_config.json resides)
    # -------------------------------------------------------------------------
    user_configs_folder = job_config.get("job_subfolder")
    if not user_configs_folder or not os.path.isdir(user_configs_folder):
        logger.error(f"[ERROR] job_subfolder not found or invalid => {user_configs_folder}")
        return

    # -------------------------------------------------------------------------
    # 2) Build an output directory for this job under OUTPUT_DIR
    #    e.g. /usr/src/app/output/<job_id>
    # -------------------------------------------------------------------------
    env_out_dir = os.environ.get("OUTPUT_DIR", "/usr/src/app/output")
    job_output_dir = os.path.join(env_out_dir, job_id)
    os.makedirs(job_output_dir, exist_ok=True)
    logger.info(f"[INFO] Using job-specific output folder: {job_output_dir}")

    # -------------------------------------------------------------------------
    # 3) Load main_config.json from user_configs/<job_id>
    # -------------------------------------------------------------------------
    main_config_path = os.path.join(user_configs_folder, "main_config.json")
    if not os.path.isfile(main_config_path):
        logger.error(f"[ERROR] Cannot find main_config.json at {main_config_path}")
        return

    with open(main_config_path, "r") as f:
        existing_config_raw = json.load(f)
    main_config = existing_config_raw.get("main_config", {})
    logger.info(f"[INFO] Loaded existing main_config from {main_config_path}.")

    # Merge posted_data["main_config"] if present
    posted_data = job_config.get("posted_data", {})
    if "main_config" in posted_data:
        logger.info("[INFO] Deep merging posted_data['main_config'] into main_config.")
        deep_merge_dicts(main_config, posted_data["main_config"])
        # optionally re-save
        with open(main_config_path, "w") as f:
            json.dump({"main_config": main_config}, f, indent=2)

    # -------------------------------------------------------------------------
    # 4) Extract sub-sections from main_config
    # -------------------------------------------------------------------------
    check_canceled()
    paths_dict       = main_config.get("paths", {})
    excel_flags      = main_config.get("excel_overrides", {})
    user_flags       = main_config.get("user_config_overrides", {})
    def_dicts        = main_config.get("default_dicts", {})
    structuring_cfg  = main_config.get("structuring", {})
    modification_cfg = main_config.get("modification", {})
    validation_cfg   = main_config.get("validation", {})
    sens_cfg         = main_config.get("sensitivity", {})
    sur_cfg          = main_config.get("surrogate", {})
    cal_cfg          = main_config.get("calibration", {})

    # IDF creation block
    idf_cfg = main_config.get("idf_creation", {})
    perform_idf_creation = idf_cfg.get("perform_idf_creation", False)
    scenario             = idf_cfg.get("scenario", "scenario1")
    calibration_stage    = idf_cfg.get("calibration_stage", "pre_calibration")
    strategy             = idf_cfg.get("strategy", "B")
    random_seed          = idf_cfg.get("random_seed", 42)
    run_simulations      = idf_cfg.get("run_simulations", True)
    simulate_config      = idf_cfg.get("simulate_config", {})
    post_process         = idf_cfg.get("post_process", True)
    post_process_config  = idf_cfg.get("post_process_config", {})
    output_definitions   = idf_cfg.get("output_definitions", {})
    use_database         = main_config.get("use_database", False)
    db_filter            = main_config.get("db_filter", {})
    filter_by            = main_config.get("filter_by")  # if using DB

    # Summarize which major steps will run
    steps_to_run = []
    if perform_idf_creation:
        steps_to_run.append("IDF creation")
        if run_simulations:
            steps_to_run.append("simulations")
    if structuring_cfg.get("perform_structuring", False):
        steps_to_run.append("structuring")
    if modification_cfg.get("perform_modification", False):
        steps_to_run.append("modification")
    if main_config.get("validation_base", {}).get("perform_validation", False):
        steps_to_run.append("base validation")
    if main_config.get("validation_scenarios", {}).get("perform_validation", False):
        steps_to_run.append("scenario validation")
    if sens_cfg.get("perform_sensitivity", False):
        steps_to_run.append("sensitivity analysis")
    if sur_cfg.get("perform_surrogate", False):
        steps_to_run.append("surrogate modeling")
    if cal_cfg.get("perform_calibration", False):
        steps_to_run.append("calibration")

    if steps_to_run:
        logger.info("[INFO] Steps to execute: " + ", ".join(steps_to_run))
    else:
        logger.info("[INFO] No major steps are enabled in configuration.")

    # -------------------------------------------------------------------------
    # 5) Possibly override idf_creation.idf_config from env, then force IDFs
    #    to go in <job_output_dir>/output_IDFs
    # -------------------------------------------------------------------------
    check_canceled()

    env_idd_path = os.environ.get("IDD_PATH")
    if env_idd_path:
        idf_creation.idf_config["iddfile"] = env_idd_path
    env_base_idf = os.environ.get("BASE_IDF_PATH")
    if env_base_idf:
        idf_creation.idf_config["idf_file_path"] = env_base_idf

    job_idf_dir = os.path.join(job_output_dir, "output_IDFs")
    os.makedirs(job_idf_dir, exist_ok=True)
    idf_creation.idf_config["output_dir"] = job_idf_dir

    # If user explicitly set these in main_config, override again
    if "iddfile" in idf_cfg:
        idf_creation.idf_config["iddfile"] = idf_cfg["iddfile"]
    if "idf_file_path" in idf_cfg:
        idf_creation.idf_config["idf_file_path"] = idf_cfg["idf_file_path"]

    if "output_idf_dir" in idf_cfg:
        subfolder = idf_cfg["output_idf_dir"]  # e.g. "output_IDFs"
        full_dir = os.path.join(job_output_dir, subfolder)
        idf_creation.idf_config["output_dir"] = full_dir
    else:
        idf_creation.idf_config["output_dir"] = os.path.join(job_output_dir, "output_IDFs")

    # -------------------------------------------------------------------------
    # 6) Setup default dictionaries
    # -------------------------------------------------------------------------
    base_res_data    = def_dicts.get("res_data", {})
    base_nonres_data = def_dicts.get("nonres_data", {})
    dhw_lookup       = def_dicts.get("dhw", {})
    epw_lookup       = def_dicts.get("epw", [])
    lighting_lookup  = def_dicts.get("lighting", {})
    hvac_lookup      = def_dicts.get("hvac", {})
    vent_lookup      = def_dicts.get("vent", {})

    # -------------------------------------------------------------------------
    # 7) Apply Excel overrides if flags are set
    # -------------------------------------------------------------------------
    check_canceled()

    updated_res_data, updated_nonres_data = build_fenez_config(
        base_res_data=base_res_data,
        base_nonres_data=base_nonres_data,
        excel_path=paths_dict.get("fenez_excel", ""),
        do_excel_override=excel_flags.get("override_fenez_excel", False),
        user_fenez_overrides=[]
    )

    if excel_flags.get("override_dhw_excel", False):
        dhw_lookup = override_dhw_lookup_from_excel_file(
            dhw_excel_path=paths_dict.get("dhw_excel", ""),
            default_dhw_lookup=dhw_lookup,
            override_dhw_flag=True
        )

    if excel_flags.get("override_epw_excel", False):
        epw_lookup = override_epw_lookup_from_excel_file(
            epw_excel_path=paths_dict.get("epw_excel", ""),
            epw_lookup=epw_lookup,
            override_epw_flag=True
        )

    if excel_flags.get("override_lighting_excel", False):
        lighting_lookup = override_lighting_lookup_from_excel_file(
            lighting_excel_path=paths_dict.get("lighting_excel", ""),
            lighting_lookup=lighting_lookup,
            override_lighting_flag=True
        )

    if excel_flags.get("override_hvac_excel", False):
        hvac_lookup = override_hvac_lookup_from_excel_file(
            hvac_excel_path=paths_dict.get("hvac_excel", ""),
            hvac_lookup=hvac_lookup,
            override_hvac_flag=True
        )

    if excel_flags.get("override_vent_excel", False):
        vent_lookup = override_vent_lookup_from_excel_file(
            vent_excel_path=paths_dict.get("vent_excel", ""),
            vent_lookup=vent_lookup,
            override_vent_flag=True
        )

    # -------------------------------------------------------------------------
    # 8) JSON overrides from user_configs/<job_id> if user_flags are set
    # -------------------------------------------------------------------------
    check_canceled()

    def safe_load_subjson(fname, key):
        """
        Loads user_configs/<job_id>/fname if it exists, returns data.get(key).
        """
        full_path = os.path.join(user_configs_folder, fname)
        if os.path.isfile(full_path):
            try:
                with open(full_path, "r") as ff:
                    data = json.load(ff)
                return data.get(key)
            except Exception as e:
                logger.error(f"[ERROR] loading {fname} => {e}")
        return None

    # Fenestration
    user_fenez_data = []
    if user_flags.get("override_fenez_json", False):
        loaded = safe_load_subjson("fenestration.json", "fenestration")
        if loaded:
            user_fenez_data = loaded

    updated_res_data, updated_nonres_data = build_fenez_config(
        base_res_data=updated_res_data,
        base_nonres_data=updated_nonres_data,
        excel_path="",
        do_excel_override=False,
        user_fenez_overrides=user_fenez_data
    )

    # DHW
    user_config_dhw = None
    if user_flags.get("override_dhw_json", False):
        user_config_dhw = safe_load_subjson("dhw.json", "dhw")

    # EPW
    user_config_epw = []
    if user_flags.get("override_epw_json", False):
        e = safe_load_subjson("epw.json", "epw")
        if e:
            user_config_epw = e

    # Lighting
    user_config_lighting = None
    if user_flags.get("override_lighting_json", False):
        user_config_lighting = safe_load_subjson("lighting.json", "lighting")

    # HVAC
    user_config_hvac = None
    if user_flags.get("override_hvac_json", False):
        user_config_hvac = safe_load_subjson("hvac.json", "hvac")

    # Vent
    user_config_vent = []
    if user_flags.get("override_vent_json", False):
        v = safe_load_subjson("vent.json", "vent")
        if v:
            user_config_vent = v

    # Geometry
    geom_data = {}
    if user_flags.get("override_geometry_json", False):
        g = safe_load_subjson("geometry.json", "geometry")
        if g:
            geom_data["geometry"] = g

    # Shading
    shading_data = {}
    if user_flags.get("override_shading_json", False):
        s = safe_load_subjson("shading.json", "shading")
        if s:
            shading_data["shading"] = s

    # -------------------------------------------------------------------------
    # 9) IDF creation
    # -------------------------------------------------------------------------
    check_canceled()
    df_buildings = pd.DataFrame()

    if perform_idf_creation:
        logger.info("[INFO] IDF creation is ENABLED.")
        with step_timer(logger, "IDF creation and simulations"):
            # a) Load building data
            if use_database:
                logger.info("[INFO] Loading building data from DB.")
                if not filter_by:
                    raise ValueError("[ERROR] 'filter_by' must be specified when 'use_database' is True.")
                df_buildings = load_buildings_from_db(db_filter, filter_by)

                # Optionally save the raw DB buildings
                extracted_csv_path = os.path.join(job_output_dir, "extracted_buildings.csv")
                df_buildings.to_csv(extracted_csv_path, index=False)
                logger.info(f"[INFO] Saved extracted buildings to {extracted_csv_path}")

            else:
                bldg_data_path = paths_dict.get("building_data", "")
                if os.path.isfile(bldg_data_path):
                    df_buildings = pd.read_csv(bldg_data_path)
                else:
                    logger.warning(f"[WARN] building_data CSV not found => {bldg_data_path}")

            logger.info(f"[INFO] Number of buildings to simulate: {len(df_buildings)}")

            # b) Create IDFs & (optionally) run sims in job folder
            df_buildings = create_idfs_for_all_buildings(
                df_buildings=df_buildings,
                scenario=scenario,
                calibration_stage=calibration_stage,
                strategy=strategy,
                random_seed=random_seed,
                user_config_geom=geom_data.get("geometry", []),
                user_config_lighting=user_config_lighting,
                user_config_dhw=user_config_dhw,
                res_data=updated_res_data,
                nonres_data=updated_nonres_data,
                user_config_hvac=user_config_hvac,
                user_config_vent=user_config_vent,
                user_config_epw=user_config_epw,
                output_definitions=output_definitions,
                run_simulations=run_simulations,
                simulate_config=simulate_config,
                post_process=post_process,
                post_process_config=post_process_config,
                logs_base_dir=job_output_dir
            )

            # === Store the mapping (ogc_fid -> idf_name) so we can look it up later ===
            idf_map_csv = os.path.join(job_output_dir, "extracted_idf_buildings.csv")
            df_buildings.to_csv(idf_map_csv, index=False)
            logger.info(f"[INFO] Wrote building -> IDF map to {idf_map_csv}")

    else:
        logger.info("[INFO] Skipping IDF creation.")

    # -------------------------------------------------------------------------
    # 10) Perform structuring if requested
    # -------------------------------------------------------------------------
    check_canceled()
    if structuring_cfg.get("perform_structuring", False):
        with step_timer(logger, "structuring"):
            logger.info("[INFO] Performing structuring ...")

            # --- Fenestration -------------------------------------------------
            from idf_objects.structuring.fenestration_structuring import transform_fenez_log_to_structured_with_ranges
            fenez_conf = structuring_cfg.get("fenestration", {})
            fenez_in = fenez_conf.get("csv_in", "assigned/assigned_fenez_params.csv")
            fenez_out = fenez_conf.get("csv_out", "assigned/structured_fenez_params.csv")
            if not os.path.isabs(fenez_in):
                fenez_in = os.path.join(job_output_dir, fenez_in)
            if not os.path.isabs(fenez_out):
                fenez_out = os.path.join(job_output_dir, fenez_out)
            if os.path.isfile(fenez_in):
                transform_fenez_log_to_structured_with_ranges(csv_input=fenez_in, csv_output=fenez_out)
            else:
                logger.warning(f"[STRUCTURING] Fenestration input CSV not found => {fenez_in}")

            # --- DHW ---------------------------------------------------------
            from idf_objects.structuring.dhw_structuring import transform_dhw_log_to_structured
            dhw_conf = structuring_cfg.get("dhw", {})
            dhw_in = dhw_conf.get("csv_in", "assigned/assigned_dhw_params.csv")
            dhw_out = dhw_conf.get("csv_out", "assigned/structured_dhw_params.csv")
            if not os.path.isabs(dhw_in):
                dhw_in = os.path.join(job_output_dir, dhw_in)
            if not os.path.isabs(dhw_out):
                dhw_out = os.path.join(job_output_dir, dhw_out)
            if os.path.isfile(dhw_in):
                transform_dhw_log_to_structured(dhw_in, dhw_out)
            else:
                logger.warning(f"[STRUCTURING] DHW input CSV not found => {dhw_in}")


            # NEW:--- Shading ----------------------------------------------------
            from idf_objects.structuring.shading_structuring import transform_shading_log_to_structured
            shading_conf = structuring_cfg.get("shading", {})
            user_shading_rules = safe_load_subjson("shading.json", "shading") or []
            
            if shading_conf:
                shading_in = patch_if_relative(shading_conf.get("csv_in"))
                shading_out = patch_if_relative(shading_conf.get("csv_out"))

                transform_shading_log_to_structured(
                    csv_input=shading_in,
                    csv_output=shading_out,
                    user_shading_rules=user_shading_rules
                )
            else:
                logger.warning("[STRUCTURING] 'shading' configuration not found in structuring settings.")


            # --- Equipment --------------------------------------------------
            from idf_objects.structuring.equipment_structuring import transform_equipment_log_to_structured
            equip_conf = structuring_cfg.get("equipment", {})
            user_equip_rules = safe_load_subjson("equipment.json", "equipment") or []
            
            if equip_conf:
                equip_in = patch_if_relative(equip_conf.get("csv_in"))
                equip_out = patch_if_relative(equip_conf.get("csv_out"))

                transform_equipment_log_to_structured(
                    csv_input=equip_in,
                    csv_output=equip_out,
                    user_equipment_rules=user_equip_rules
                )
            else:
                logger.warning("[STRUCTURING] 'equipment' configuration not found in structuring settings.")



            # --- Zone Sizing ------------------------------------------------
            from idf_objects.structuring.zone_sizing_structuring import transform_zone_sizing_log_to_structured
            sizing_conf = structuring_cfg.get("zone_sizing", {})
            user_sizing_rules = safe_load_subjson("zone_sizing.json", "zone_sizing") or []
            
            if sizing_conf:
                sizing_in = patch_if_relative(sizing_conf.get("csv_in"))
                sizing_out = patch_if_relative(sizing_conf.get("csv_out"))

                transform_zone_sizing_log_to_structured(
                    csv_input=sizing_in,
                    csv_output=sizing_out,
                    user_sizing_rules=user_sizing_rules
                )
            else:
                logger.warning("[STRUCTURING] 'zone_sizing' configuration not found.")



            # --- HVAC flatten -----------------------------------------------
            from idf_objects.structuring.flatten_hvac import flatten_hvac_data, parse_assigned_value as parse_hvac
            hvac_conf = structuring_cfg.get("hvac", {})
            hvac_in = hvac_conf.get("csv_in", "assigned/assigned_hvac_params.csv")
            hvac_bld = hvac_conf.get("build_out", "assigned/assigned_hvac_building.csv")
            hvac_zone = hvac_conf.get("zone_out", "assigned/assigned_hvac_zones.csv")
            if not os.path.isabs(hvac_in):
                hvac_in = os.path.join(job_output_dir, hvac_in)
            if not os.path.isabs(hvac_bld):
                hvac_bld = os.path.join(job_output_dir, hvac_bld)
            if not os.path.isabs(hvac_zone):
                hvac_zone = os.path.join(job_output_dir, hvac_zone)
            if os.path.isfile(hvac_in):
                df_hvac = pd.read_csv(hvac_in)
                if "assigned_value" in df_hvac.columns:
                    df_hvac["assigned_value"] = df_hvac["assigned_value"].apply(parse_hvac)
                    flatten_hvac_data(
                        df_input=df_hvac,
                        out_build_csv=hvac_bld,
                        out_zone_csv=hvac_zone,
                    )
                else:
                    logger.warning(
                        f"[STRUCTURING] 'assigned_value' column missing in {hvac_in}. Skipping HVAC flatten."
                    )
            else:
                logger.warning(f"[STRUCTURING] HVAC input CSV not found => {hvac_in}")

            # --- Vent flatten -----------------------------------------------
            from idf_objects.structuring.flatten_assigned_vent import flatten_ventilation_data, parse_assigned_value as parse_vent
            vent_conf = structuring_cfg.get("vent", {})
            vent_in = vent_conf.get("csv_in", "assigned/assigned_ventilation.csv")
            vent_bld = vent_conf.get("build_out", "assigned/assigned_vent_building.csv")
            vent_zone = vent_conf.get("zone_out", "assigned/assigned_vent_zones.csv")
            if not os.path.isabs(vent_in):
                vent_in = os.path.join(job_output_dir, vent_in)
            if not os.path.isabs(vent_bld):
                vent_bld = os.path.join(job_output_dir, vent_bld)
            if not os.path.isabs(vent_zone):
                vent_zone = os.path.join(job_output_dir, vent_zone)
            if os.path.isfile(vent_in):
                df_vent = pd.read_csv(vent_in)
                if "assigned_value" in df_vent.columns:
                    df_vent["assigned_value"] = df_vent["assigned_value"].apply(parse_vent)
                    flatten_ventilation_data(
                        df_input=df_vent,
                        out_build_csv=vent_bld,
                        out_zone_csv=vent_zone,
                    )
                else:
                    logger.warning(
                        f"[STRUCTURING] 'assigned_value' column missing in {vent_in}. Skipping ventilation flatten."
                    )
            else:
                logger.warning(f"[STRUCTURING] Vent input CSV not found => {vent_in}")
    else:
        logger.info("[INFO] Skipping structuring.")

    # -------------------------------------------------------------------------
    # 11) Scenario Modification
    # -------------------------------------------------------------------------
    check_canceled()
    if modification_cfg.get("perform_modification", False):
        with step_timer(logger, "modification"):
            logger.info("[INFO] Scenario modification is ENABLED.")

            mod_cfg = modification_cfg["modify_config"]

            # 1) Ensure scenario IDFs go to <job_output_dir>/scenario_idfs
            scenario_idf_dir = os.path.join(job_output_dir, "scenario_idfs")
            os.makedirs(scenario_idf_dir, exist_ok=True)
            mod_cfg["output_idf_dir"] = scenario_idf_dir

            # 2) Ensure scenario sims => <job_output_dir>/Sim_Results/Scenarios
            if "simulation_config" in mod_cfg:
                sim_out = os.path.join(job_output_dir, "Sim_Results", "Scenarios")
                os.makedirs(sim_out, exist_ok=True)
                mod_cfg["simulation_config"]["output_dir"] = sim_out

            # 3) Post-process => <job_output_dir>/results_scenarioes
            if "post_process_config" in mod_cfg:
                ppcfg = mod_cfg["post_process_config"]
                as_is_csv = os.path.join(job_output_dir, "results_scenarioes", "merged_as_is_scenarios.csv")
                daily_csv = os.path.join(job_output_dir, "results_scenarioes", "merged_daily_mean_scenarios.csv")
                os.makedirs(os.path.dirname(as_is_csv), exist_ok=True)
                os.makedirs(os.path.dirname(daily_csv), exist_ok=True)
                ppcfg["output_csv_as_is"] = as_is_csv
                ppcfg["output_csv_daily_mean"] = daily_csv

            # 4) Fix assigned_csv paths
            assigned_csv_dict = mod_cfg.get("assigned_csv", {})
            for key, rel_path in assigned_csv_dict.items():
                assigned_csv_dict[key] = os.path.join(job_output_dir, rel_path)

            # 5) Fix scenario_csv paths
            scenario_csv_dict = mod_cfg.get("scenario_csv", {})
            for key, rel_path in scenario_csv_dict.items():
                scenario_csv_dict[key] = os.path.join(job_output_dir, rel_path)

            # ----------------------------------------------------------------------
            # NEW LOGIC: pick the base_idf_path from building_id automatically
            # ----------------------------------------------------------------------
            # The user sets "building_id" in the config, e.g. 20233330
            building_id = mod_cfg["building_id"]

            # We need the CSV that was saved right after create_idfs_for_all_buildings(...)
            idf_map_csv = os.path.join(job_output_dir, "extracted_idf_buildings.csv")
            if not os.path.isfile(idf_map_csv):
                raise FileNotFoundError(
                    f"Cannot find building->IDF map CSV at {idf_map_csv}. "
                    f"Did you skip 'perform_idf_creation'?"
                )

        # Read the mapping: each row has "ogc_fid" and "idf_name"
            df_idf_map = pd.read_csv(idf_map_csv)
            row_match = df_idf_map.loc[df_idf_map["ogc_fid"] == building_id]

            if row_match.empty:
                raise ValueError(
                    f"No building found for building_id={building_id} in {idf_map_csv}"
                )

        # e.g. "building_0.idf", "building_16.idf", "building_16_ba62d0.idf", etc.
            idf_filename = row_match.iloc[0]["idf_name"]

        # Build the full path to that IDF in output_IDFs
            base_idf_path = os.path.join(job_output_dir, "output_IDFs", idf_filename)
            mod_cfg["base_idf_path"] = base_idf_path
            logger.info(f"[INFO] Auto-selected base IDF => {base_idf_path}")
        # ----------------------------------------------------------------------

            # Finally, run the scenario workflow
            run_modification_workflow(mod_cfg)
    else:
        logger.info("[INFO] Skipping scenario modification.")


    # -------------------------------------------------------------------------
    # 12) Helper to handle patching CSVs that are "relative" but not "data/".
    # -------------------------------------------------------------------------
    def patch_if_relative(csv_path: str):
        """
        1) If absolute, return as-is.
        2) If starts with 'data/', interpret as /usr/src/app/data/... (no job folder).
        3) Else, join with job_output_dir.
        """
        if not csv_path:
            return csv_path
        if os.path.isabs(csv_path):
            return csv_path
        if csv_path.startswith("data/"):
            return os.path.join("/usr/src/app", csv_path)
        return os.path.join(job_output_dir, csv_path)

    # -------------------------------------------------------------------------
    # 13) Global Validation
    # -------------------------------------------------------------------------
        # (A) Validation - BASE
    # -------------------------------------------------------------------------
    check_canceled()
    base_validation_cfg = main_config.get("validation_base", {})
    if base_validation_cfg.get("perform_validation", False):
        with step_timer(logger, "base validation"):
            logger.info("[INFO] BASE Validation is ENABLED.")
            val_conf = base_validation_cfg["config"]

            # Patch relative paths
            sim_csv = val_conf.get("sim_data_csv")
            if sim_csv:
                val_conf["sim_data_csv"] = patch_if_relative(sim_csv)

            real_csv = val_conf.get("real_data_csv")
            if real_csv:
                val_conf["real_data_csv"] = patch_if_relative(real_csv)

            out_csv = val_conf.get("output_csv")
            if out_csv:
                val_conf["output_csv"] = patch_if_relative(out_csv)

            # Now run the validation
            run_validation_process(val_conf)
    else:
        logger.info("[INFO] Skipping BASE validation or not requested.")

    # (B) Validation - SCENARIOS
    # -------------------------------------------------------------------------
    check_canceled()
    scenario_validation_cfg = main_config.get("validation_scenarios", {})
    if scenario_validation_cfg.get("perform_validation", False):
        with step_timer(logger, "scenario validation"):
            logger.info("[INFO] SCENARIO Validation is ENABLED.")
            val_conf = scenario_validation_cfg["config"]

            # Patch relative paths
            sim_csv = val_conf.get("sim_data_csv")
            if sim_csv:
                val_conf["sim_data_csv"] = patch_if_relative(sim_csv)

            real_csv = val_conf.get("real_data_csv")
            if real_csv:
                val_conf["real_data_csv"] = patch_if_relative(real_csv)

            out_csv = val_conf.get("output_csv")
            if out_csv:
                val_conf["output_csv"] = patch_if_relative(out_csv)

            # Now run the validation
            run_validation_process(val_conf)
    else:
        logger.info("[INFO] Skipping SCENARIO validation or not requested.")


    # -------------------------------------------------------------------------
    # 14) Sensitivity Analysis
    # -------------------------------------------------------------------------
    check_canceled()
    if sens_cfg.get("perform_sensitivity", False):
        with step_timer(logger, "sensitivity analysis"):
            logger.info("[INFO] Sensitivity Analysis is ENABLED.")

            scenario_folder = sens_cfg.get("scenario_folder", "")
            sens_cfg["scenario_folder"] = patch_if_relative(scenario_folder)

            results_csv = sens_cfg.get("results_csv", "")
            sens_cfg["results_csv"] = patch_if_relative(results_csv)

            out_csv = sens_cfg.get("output_csv", "sensitivity_output.csv")
            sens_cfg["output_csv"] = patch_if_relative(out_csv)

            run_sensitivity_analysis(
                scenario_folder=sens_cfg["scenario_folder"],
                method=sens_cfg["method"],
                results_csv=sens_cfg.get("results_csv", ""),
                target_variable=sens_cfg.get("target_variable", []),
                output_csv=sens_cfg.get("output_csv", "sensitivity_output.csv"),
                n_morris_trajectories=sens_cfg.get("n_morris_trajectories", 10),
                num_levels=sens_cfg.get("num_levels", 4),
                n_sobol_samples=sens_cfg.get("n_sobol_samples", 128)
            )
    else:
        logger.info("[INFO] Skipping sensitivity analysis.")

    # -------------------------------------------------------------------------
    # 15) Surrogate Modeling
    # -------------------------------------------------------------------------
    check_canceled()
    if sur_cfg.get("perform_surrogate", False):
        with step_timer(logger, "surrogate modeling"):
            logger.info("[INFO] Surrogate Modeling is ENABLED.")

            scenario_folder = sur_cfg.get("scenario_folder", "")
            sur_cfg["scenario_folder"] = patch_if_relative(scenario_folder)

            results_csv = sur_cfg.get("results_csv", "")
            sur_cfg["results_csv"] = patch_if_relative(results_csv)

            model_out = sur_cfg.get("model_out", "")
            sur_cfg["model_out"] = patch_if_relative(model_out)

            cols_out = sur_cfg.get("cols_out", "")
            sur_cfg["cols_out"] = patch_if_relative(cols_out)

            target_var = sur_cfg["target_variable"]
            test_size  = sur_cfg["test_size"]

            df_scen = sur_load_scenario_params(sur_cfg["scenario_folder"])
            pivot_df = pivot_scenario_params(df_scen)

            df_sim = load_sim_results(sur_cfg["results_csv"])
            df_agg = aggregate_results(df_sim)
            merged_df = merge_params_with_results(pivot_df, df_agg, target_var)

            rf_model, trained_cols = build_and_save_surrogate(
                df_data=merged_df,
                target_col=target_var,
                model_out_path=sur_cfg["model_out"],
                columns_out_path=sur_cfg["cols_out"],
                test_size=test_size,
                random_state=42
            )
            if rf_model:
                logger.info("[INFO] Surrogate model built & saved.")
            else:
                logger.warning("[WARN] Surrogate modeling failed or insufficient data.")
    else:
        logger.info("[INFO] Skipping surrogate modeling.")

    # -------------------------------------------------------------------------
    # 16) Calibration
    # -------------------------------------------------------------------------
    check_canceled()
    if cal_cfg.get("perform_calibration", False):
        with step_timer(logger, "calibration"):
            logger.info("[INFO] Calibration is ENABLED.")

            scen_folder = cal_cfg.get("scenario_folder", "")
            cal_cfg["scenario_folder"] = patch_if_relative(scen_folder)

            real_csv = cal_cfg.get("real_data_csv", "")
            cal_cfg["real_data_csv"] = patch_if_relative(real_csv)

            sur_model_path = cal_cfg.get("surrogate_model_path", "")
            cal_cfg["surrogate_model_path"] = patch_if_relative(sur_model_path)

            sur_cols_path = cal_cfg.get("surrogate_columns_path", "")
            cal_cfg["surrogate_columns_path"] = patch_if_relative(sur_cols_path)

            hist_csv = cal_cfg.get("output_history_csv", "")
            cal_cfg["output_history_csv"] = patch_if_relative(hist_csv)

            best_params_folder = cal_cfg.get("best_params_folder", "")
            cal_cfg["best_params_folder"] = patch_if_relative(best_params_folder)

            run_unified_calibration(cal_cfg)
    else:
        logger.info("[INFO] Skipping calibration.")

    # -------------------------------------------------------------------------
    # 17) Zip & Email final results, if mail_user.json present
    # -------------------------------------------------------------------------
    try:
        with step_timer(logger, "zipping and email"):
            mail_user_path = os.path.join(user_configs_folder, "mail_user.json")
            mail_info = {}
            if os.path.isfile(mail_user_path):
                with open(mail_user_path, "r") as f:
                    mail_info = json.load(f)

                mail_user_list = mail_info.get("mail_user", [])
                if len(mail_user_list) > 0:
                    first_user = mail_user_list[0]
                    recipient_email = first_user.get("email", "")
                    if recipient_email:
                        zip_path = zip_user_output(job_output_dir)
                        send_results_email(zip_path, recipient_email)
                        logger.info(f"[INFO] Emailed zip {zip_path} to {recipient_email}")
                    else:
                        logger.warning("[WARN] mail_user.json => missing 'email'")
                else:
                    logger.warning("[WARN] mail_user.json => 'mail_user' list is empty.")
            else:
                logger.info("[INFO] No mail_user.json found, skipping email.")
    except Exception as e:
        logger.error(f"[ERROR] Zipping/Emailing results failed => {e}")

    # -------------------------------------------------------------------------
    # LAST STEP: (Optional) Call the cleanup function
    # -------------------------------------------------------------------------
    try:
        cleanup_old_results()  # This will remove any job folder older than MAX_AGE_HOURS
    except Exception as e:
        logger.error(f"[CLEANUP ERROR] => {e}")

    total_time = time.perf_counter() - overall_start
    logger.info(f"=== End of orchestrate_workflow (took {total_time:.2f} seconds) ===")

------------------------------------------------------------

