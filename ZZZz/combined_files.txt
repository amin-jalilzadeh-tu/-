File: D:\Documents\E_Plus_2030_py\idf_creation.py
============================================================
###############################################################
# File: idf_creation.py
###############################################################
"""
idf_creation.py

Handles the creation of EnergyPlus IDF files for a list of buildings,
plus optional simulation runs and post-processing.

Key functionalities:
  1) create_idf_for_building(...) builds a single IDF using geomeppy,
     applying geometry, fenestration, HVAC, etc.
  2) create_idfs_for_all_buildings(...) loops over multiple buildings,
     then optionally runs simulations and merges results in one or more ways.

Updated to allow writing logs/results inside a specific job folder via logs_base_dir.
"""

import os
import logging
import pandas as pd

# geomeppy for IDF manipulation
from geomeppy import IDF

# --- Import your custom submodules ---
from idf_objects.geomz.building import create_building_with_roof_type
from idf_objects.fenez.fenestration import add_fenestration
from idf_objects.fenez.materials import (
    update_construction_materials,
    assign_constructions_to_surfaces
)
from idf_objects.Elec.lighting import add_lights_and_parasitics
from idf_objects.eequip.equipment import add_electric_equipment
from idf_objects.DHW.water_heater import add_dhw_to_idf
from idf_objects.HVAC.custom_hvac import add_HVAC_Ideal_to_all_zones
from idf_objects.ventilation.add_ventilation import add_ventilation_to_idf
from idf_objects.wshading.create_shading_objects import add_shading_objects # Corrected import if it was wshading
from idf_objects.setzone.add_outdoor_air_and_zone_sizing_to_all_zones import add_outdoor_air_and_zone_sizing_to_all_zones
from idf_objects.tempground.add_ground_temperatures import add_ground_temperatures
from idf_objects.other.zonelist import create_zonelist

# Output & simulation modules
from idf_objects.outputdef.assign_output_settings import assign_output_settings
from idf_objects.outputdef.add_output_definitions import add_output_definitions
from postproc.merge_results import merge_all_results
from epw.run_epw_sims import simulate_all

# Configure logger for this module (or ensure it's configured at the application entry point)
logger = logging.getLogger(__name__)
if not logger.hasHandlers():
    logger.addHandler(logging.NullHandler()) # Be a good library/module

###############################################################################
# Global Default IDF Config
# (Override these via environment variables or main_config if needed.)
###############################################################################
idf_config = {
    "iddfile": "EnergyPlus/Energy+.idd",         # Default path to the IDD file
    "idf_file_path": "EnergyPlus/Minimal.idf",   # Default path to a minimal base IDF
    "output_dir": "output/output_IDFs"           # Default folder to save generated IDFs
}


def create_idf_for_building(
    building_row,
    building_index,
    scenario="scenario1",
    calibration_stage="pre_calibration",
    strategy="B", # Note: Shading modules often default to "A", ensure consistency or pass explicitly
    random_seed=42,
    # Geometry
    user_config_geom=None,
    assigned_geom_log=None,
    # Lighting
    user_config_lighting=None,
    assigned_lighting_log=None,
    # Electric equipment
    user_config_equipment=None,
    assigned_equip_log=None,
    # DHW
    user_config_dhw=None,
    assigned_dhw_log=None,
    # Fenestration
    res_data=None,
    nonres_data=None,
    assigned_fenez_log=None,
    # Window Shading
    shading_type_key_for_blinds="my_external_louvers", # Explicit parameter for clarity
    user_config_shading=None, # This should be the specific config for the shading_type_key_for_blinds
    assigned_shading_log=None,
    apply_blind_shading=True, # To control if blind shading is added
    apply_geometric_shading=False, # To control if geometric shading is added
    shading_strategy = "A", # Shading often uses 'A' (midpoint) as default, can be different from global 'strategy'
    # HVAC
    user_config_hvac=None,
    assigned_hvac_log=None,
    # Vent
    user_config_vent=None,
    assigned_vent_log=None,
    # Zone sizing
    assigned_setzone_log=None,
    # Ground temps
    assigned_groundtemp_log=None,
    # Output definitions
    output_definitions=None
):
    """
    Build an IDF for a single building.
    """
    logger.info(f"Starting IDF creation for building_index: {building_index}, ogc_fid: {building_row.get('ogc_fid', 'N/A')}")
    # 1) Setup IDF from the minimal template
    IDF.setiddname(idf_config["iddfile"])
    idf = IDF(idf_config["idf_file_path"])

    # 2) Basic building object settings
    building_obj = idf.newidfobject("BUILDING")
    building_obj.Name = f"Sample_Building_{building_index}"

    orientation = building_row.get("building_orientation", 0.0)
    if pd.isna(orientation):
        orientation = 0.0
    building_obj.North_Axis = 0.0 # Geometry is rotated, building north axis remains 0

    logger.debug(f"[{building_index}] Set up base IDF and BUILDING object.")

    # 3) Create geometry
    if assigned_geom_log is not None and building_row.get("ogc_fid") not in assigned_geom_log:
        assigned_geom_log[building_row.get("ogc_fid")] = {}

    edge_types = []
    for side_col in ["north_side", "east_side", "south_side", "west_side"]:
        edge_types.append(building_row.get(side_col, "Facade"))

    create_building_with_roof_type(
        idf=idf,
        area=building_row.get("area", 100.0),
        perimeter=building_row.get("perimeter", 40.0),
        orientation=orientation,
        building_row=building_row,
        edge_types=edge_types,
        calibration_stage=calibration_stage,
        strategy=strategy, # Global strategy
        random_seed=random_seed,
        user_config=user_config_geom,
        assigned_geom_log=assigned_geom_log
    )
    logger.debug(f"[{building_index}] Geometry created.")

    # 4) Update materials & constructions
    construction_map = update_construction_materials(
        idf=idf,
        building_row=building_row,
        building_index=building_index,
        scenario=scenario,
        calibration_stage=calibration_stage,
        strategy=strategy, # Global strategy
        random_seed=random_seed,
        user_config_fenez=None,
        assigned_fenez_log=assigned_fenez_log
    )
    assign_constructions_to_surfaces(idf, construction_map)
    logger.debug(f"[{building_index}] Materials and constructions updated and assigned.")

    # Create zone list for convenience
    create_zonelist(idf, zonelist_name="ALL_ZONES")
    logger.debug(f"[{building_index}] Zonelist 'ALL_ZONES' created.")

    # 5) Fenestration
    add_fenestration(
        idf=idf,
        building_row=building_row,
        scenario=scenario,
        calibration_stage=calibration_stage,
        strategy=strategy, # Global strategy
        random_seed=random_seed,
        res_data=res_data,
        nonres_data=nonres_data,
        assigned_fenez_log=assigned_fenez_log
    )
    logger.debug(f"[{building_index}] Fenestration added.")

    # 6) Window shading (e.g., blinds)
    if apply_blind_shading or apply_geometric_shading:
        logger.info(f"[{building_index}] Applying window shading. Blinds: {apply_blind_shading}, Geometric: {apply_geometric_shading}")
        add_shading_objects(
            idf=idf,
            building_row=building_row,
            shading_type_key=shading_type_key_for_blinds,
            strategy=shading_strategy,
            random_seed=random_seed,
            user_config_shading=user_config_shading,
            assigned_shading_log=assigned_shading_log,
            create_blinds=apply_blind_shading,
            create_geometry_shading=apply_geometric_shading
        )
        logger.debug(f"[{building_index}] Window shading objects processed.")
    else:
        logger.info(f"[{building_index}] Skipping window shading.")

    # 7) Lighting
    add_lights_and_parasitics(
        idf=idf,
        building_row=building_row,
        calibration_stage=calibration_stage,
        strategy=strategy, # Global strategy
        random_seed=random_seed,
        user_config=user_config_lighting,
        assigned_values_log=assigned_lighting_log
    )
    logger.debug(f"[{building_index}] Lighting and parasitics added.")

    # 8) Electric equipment
    add_electric_equipment(
        idf=idf,
        building_row=building_row,
        calibration_stage=calibration_stage,
        strategy=strategy, # Global strategy
        random_seed=random_seed,
        user_config=user_config_equipment,
        assigned_values_log=assigned_equip_log,
        zonelist_name="ALL_ZONES",
    )
    logger.debug(f"[{building_index}] Electric equipment added.")

    # 9) DHW
    add_dhw_to_idf(
        idf=idf,
        building_row=building_row,
        calibration_stage=calibration_stage,
        strategy=strategy, # Global strategy
        random_seed=random_seed,
        name_suffix=f"MyDHW_{building_index}",
        user_config_dhw=user_config_dhw,
        assigned_dhw_log=assigned_dhw_log,
        use_nta=True
    )
    logger.debug(f"[{building_index}] DHW system added.")

    # 10) HVAC
    add_HVAC_Ideal_to_all_zones(
        idf=idf,
        building_row=building_row,
        calibration_stage=calibration_stage,
        strategy=strategy, # Global strategy
        random_seed=random_seed,
        user_config_hvac=user_config_hvac,
        assigned_hvac_log=assigned_hvac_log
    )
    logger.debug(f"[{building_index}] Ideal Loads HVAC system added.")

    # 11) Ventilation
    add_ventilation_to_idf(
        idf=idf,
        building_row=building_row,
        calibration_stage=calibration_stage,
        strategy=strategy,
        random_seed=random_seed,
        user_config_vent=user_config_vent,
        assigned_vent_log=assigned_vent_log,
        infiltration_model="weather",
    )
    logger.debug(f"[{building_index}] Ventilation and infiltration added.")

    # 12) Zone sizing
    add_outdoor_air_and_zone_sizing_to_all_zones(
        idf=idf,
        building_row=building_row,
        calibration_stage=calibration_stage,
        strategy=strategy,
        random_seed=random_seed,
        assigned_setzone_log=assigned_setzone_log
    )
    logger.debug(f"[{building_index}] Outdoor air and zone sizing objects added.")

    # 13) Ground temperatures
    add_ground_temperatures(
        idf=idf,
        calibration_stage=calibration_stage,
        strategy=strategy,
        random_seed=random_seed,
        assigned_groundtemp_log=assigned_groundtemp_log
    )
    logger.debug(f"[{building_index}] Ground temperatures added.")

    # 14) Output definitions
    if output_definitions is None:
        output_definitions = {
            "desired_variables": ["Facility Total Electric Demand Power", "Zone Air Temperature"],
            "desired_meters": ["Electricity:Facility"],
            "override_variable_frequency": "Hourly",
            "override_meter_frequency": "Hourly",
            "include_tables": True,
            "include_summary": True
        }
    out_settings = assign_output_settings(
        desired_variables=output_definitions.get("desired_variables", []),
        desired_meters=output_definitions.get("desired_meters", []),
        override_variable_frequency=output_definitions.get("override_variable_frequency", "Hourly"),
        override_meter_frequency=output_definitions.get("override_meter_frequency", "Hourly"),
        include_tables=output_definitions.get("include_tables", True),
        include_summary=output_definitions.get("include_summary", True)
    )
    add_output_definitions(idf, out_settings)
    logger.debug(f"[{building_index}] Output definitions added.")

    # 15) Save final IDF
    os.makedirs(idf_config["output_dir"], exist_ok=True)
    idf_filename = f"building_{building_row.get('ogc_fid', building_index)}.idf"
    idf_filename = idf_filename.replace(" ", "_").replace(":", "_") # Basic sanitization
    out_path = os.path.join(idf_config["output_dir"], idf_filename)
    
    try:
        idf.save(out_path)
        logger.info(f"IDF for building_index {building_index} saved at: {out_path}")
    except Exception as e:
        logger.error(f"Failed to save IDF for building_index {building_index} at {out_path}: {e}", exc_info=True)
        return None

    return out_path


def create_idfs_for_all_buildings(
    df_buildings,
    scenario="scenario1",
    calibration_stage="pre_calibration",
    strategy="B",
    random_seed=42,
    # partial user configs
    user_config_geom=None,
    user_config_lighting=None,
    user_config_equipment=None,
    user_config_dhw=None,
    res_data=None, # Fenestration base data
    nonres_data=None, # Fenestration base data
    user_config_shading=None,
    shading_type_key_for_blinds="my_external_louvers",
    apply_blind_shading=True,
    apply_geometric_shading=False,
    shading_strategy = "A",
    # Other configs
    user_config_hvac=None,
    user_config_vent=None,
    user_config_epw=None,
    output_definitions=None,
    run_simulations=True,
    simulate_config=None,
    post_process=True,
    post_process_config=None,
    logs_base_dir=None
):
    """
    Loops over df_buildings, calls create_idf_for_building for each.
    """
    func_logger = logging.getLogger(f"{__name__}.create_idfs_for_all_buildings")
    func_logger.info(f"Starting to create IDFs for {len(df_buildings)} buildings.")

    assigned_geom_log       = {}
    assigned_lighting_log   = {}
    assigned_equip_log      = {}
    assigned_dhw_log        = {}
    assigned_fenez_log      = {}
    assigned_shading_log    = {}
    assigned_hvac_log       = {}
    assigned_vent_log       = {}
    assigned_epw_log        = {}
    assigned_groundtemp_log = {}
    assigned_setzone_log    = {}

    for idx, row in df_buildings.iterrows():
        building_specific_seed = random_seed + idx 

        # Possibly filter shading overrides from user_config_shading if it’s a list of rules:
        specific_shading_overrides = {}
        if isinstance(user_config_shading, list):
            try:
                from idf_objects.wshading.shading_overrides_from_excel import pick_shading_params_from_rules
                bldg_identifier = row.get("ogc_fid", idx)
                specific_shading_overrides = pick_shading_params_from_rules(
                    building_id=bldg_identifier,
                    shading_type_key=shading_type_key_for_blinds,
                    all_rules=user_config_shading,
                    fallback={}
                )
                if specific_shading_overrides:
                    func_logger.info(f"Found specific Excel shading overrides for building {bldg_identifier}: {specific_shading_overrides}")
            except ImportError:
                func_logger.warning("shading_overrides_from_excel.py not found. Skipping Excel-based shading overrides.")
            except Exception as e_excel_override:
                func_logger.error(f"Error applying Excel shading overrides for building {row.get('ogc_fid', idx)}: {e_excel_override}", exc_info=True)
        elif isinstance(user_config_shading, dict):
            specific_shading_overrides = user_config_shading

        idf_path = create_idf_for_building(
            building_row=row,
            building_index=idx,
            scenario=scenario,
            calibration_stage=calibration_stage,
            strategy=strategy,
            random_seed=building_specific_seed,
            # geometry
            user_config_geom=user_config_geom,
            assigned_geom_log=assigned_geom_log,
            # lighting
            user_config_lighting=user_config_lighting,
            assigned_lighting_log=assigned_lighting_log,
            # electric equipment
            user_config_equipment=user_config_equipment,
            assigned_equip_log=assigned_equip_log,
            # DHW
            user_config_dhw=user_config_dhw,
            assigned_dhw_log=assigned_dhw_log,
            # Fenestration
            res_data=res_data,
            nonres_data=nonres_data,
            assigned_fenez_log=assigned_fenez_log,
            # Window shading
            shading_type_key_for_blinds=shading_type_key_for_blinds,
            user_config_shading=specific_shading_overrides,
            assigned_shading_log=assigned_shading_log,
            apply_blind_shading=apply_blind_shading,
            apply_geometric_shading=apply_geometric_shading,
            shading_strategy=shading_strategy,
            # HVAC
            user_config_hvac=user_config_hvac,
            assigned_hvac_log=assigned_hvac_log,
            # Vent
            user_config_vent=user_config_vent,
            assigned_vent_log=assigned_vent_log,
            # zone sizing
            assigned_setzone_log=assigned_setzone_log,
            # ground temps
            assigned_groundtemp_log=assigned_groundtemp_log,
            # output definitions
            output_definitions=output_definitions
        )
        if idf_path:
            df_buildings.loc[idx, "idf_name"] = os.path.basename(idf_path)
        else:
            df_buildings.loc[idx, "idf_name"] = "ERROR_CREATING_IDF"
            func_logger.error(f"IDF creation failed for building index {idx}. See previous errors.")

    if run_simulations:
        func_logger.info("Proceeding to run simulations for generated IDFs.")
        if simulate_config is None:
            simulate_config = {}

        sim_output_dir = os.path.join(logs_base_dir, "Sim_Results") if logs_base_dir else simulate_config.get("base_output_dir", "output/Sim_Results")
        os.makedirs(sim_output_dir, exist_ok=True)

        idf_directory = idf_config["output_dir"]
        iddfile       = idf_config["iddfile"]

        simulate_all(
            df_buildings=df_buildings[df_buildings["idf_name"] != "ERROR_CREATING_IDF"],
            idf_directory=idf_directory,
            iddfile=iddfile,
            base_output_dir=sim_output_dir,
            user_config_epw=user_config_epw,
            assigned_epw_log=assigned_epw_log,
            num_workers=simulate_config.get("num_workers", 4),
        )
    else:
        func_logger.info("Skipping simulations as per configuration.")

    if post_process:
        func_logger.info("Proceeding to post-process simulation results and write logs.")

        default_post_process_config = {
            "base_output_dir": "output/Sim_Results",
            "outputs": [{
                "convert_to_daily": False, "convert_to_monthly": False,
                "aggregator": "none", "output_csv": "output/results/merged_as_is.csv"
            }]
        }
        current_post_process_config = post_process_config if post_process_config is not None else default_post_process_config
        
        base_sim_dir_for_merge = os.path.join(logs_base_dir, "Sim_Results") if logs_base_dir else current_post_process_config.get("base_output_dir")
        
        multiple_outputs = current_post_process_config.get("outputs", [])

        for proc_item in multiple_outputs:
            out_csv_path = proc_item.get("output_csv", "output/results/merged_default.csv")
            if logs_base_dir and "output/" in out_csv_path:
                rel_filename = out_csv_path.split("output/", 1)[-1] 
                out_csv_path = os.path.join(logs_base_dir, rel_filename)
            
            os.makedirs(os.path.dirname(out_csv_path), exist_ok=True)

            merge_all_results(
                base_output_dir=base_sim_dir_for_merge,
                output_csv=out_csv_path,
                convert_to_daily=proc_item.get("convert_to_daily", False),
                daily_aggregator=proc_item.get("aggregator", "mean"),
                convert_to_monthly=proc_item.get("convert_to_monthly", False)
            )
            func_logger.info(f"Merged results saved to: {out_csv_path}")

        _write_geometry_csv(assigned_geom_log, logs_base_dir)
        _write_lighting_csv(assigned_lighting_log, logs_base_dir)
        _write_equipment_csv(assigned_equip_log, logs_base_dir)
        _write_fenestration_csv(assigned_fenez_log, logs_base_dir)
        _write_dhw_csv(assigned_dhw_log, logs_base_dir)
        # <--- REPLACED: use new specialized _write_hvac_csv(...) --->
        _write_hvac_csv(assigned_hvac_log, logs_base_dir)
        # <--- REPLACED: use new specialized _write_vent_csv(...) --->
        _write_vent_csv(assigned_vent_log, logs_base_dir)

        _write_shading_csv(assigned_shading_log, logs_base_dir)
        _write_groundtemp_csv(assigned_groundtemp_log, logs_base_dir)
        _write_setzone_csv(assigned_setzone_log, logs_base_dir)
        _write_epw_csv(assigned_epw_log, logs_base_dir)

        func_logger.info("Finished post-processing and writing all assigned parameter logs.")
    else:
        func_logger.info("Skipping post-processing as per configuration.")

    return df_buildings


###############################################################################
# Internal Helper Functions to Write Assigned Logs
###############################################################################
def _make_assigned_path(filename, logs_base_dir):
    assigned_dir = os.path.join(logs_base_dir, "assigned") if logs_base_dir else "output/assigned"
    os.makedirs(assigned_dir, exist_ok=True)
    return os.path.join(assigned_dir, filename)


def _write_generic_log_csv(log_dict, filename_base, id_column_name, logs_base_dir):
    rows = []
    if not log_dict:
        logger.debug(f"Log dictionary for {filename_base} is empty. Skipping CSV write.")
        return
    for item_id, params in log_dict.items():
        if isinstance(params, dict):
            for param_name, param_val in params.items():
                row_data = {id_column_name: item_id, "param_name": param_name}
                if isinstance(param_val, dict):
                    row_data.update(param_val)
                else:
                    row_data["assigned_value"] = param_val
                rows.append(row_data)
        else:
            rows.append({id_column_name: item_id, "param_name": "unknown", "assigned_value": str(params)})

    if not rows:
        logger.debug(f"No rows generated for {filename_base} CSV. Skipping write.")
        return
    
    df = pd.DataFrame(rows)
    common_cols = [id_column_name, "object_name", "param_name", "assigned_value", "min_val", "max_val", "shading_type_key_used", "strategy_used"]
    ordered_cols = [col for col in common_cols if col in df.columns]
    remaining_cols = [col for col in df.columns if col not in ordered_cols]
    df = df[ordered_cols + remaining_cols]

    out_path = _make_assigned_path(f"assigned_{filename_base}.csv", logs_base_dir)
    try:
        df.to_csv(out_path, index=False)
        logger.info(f"Successfully wrote {filename_base} log to {out_path}")
    except Exception as e:
        logger.error(f"Error writing {filename_base} log to {out_path}: {e}", exc_info=True)


def _write_geometry_csv(assigned_geom_log, logs_base_dir):
    _write_generic_log_csv(assigned_geom_log, "geometry", "ogc_fid", logs_base_dir)

def _write_lighting_csv(assigned_lighting_log, logs_base_dir):
    rows = []
    for bldg_id, param_dict in assigned_lighting_log.items():
        for param_key, details in param_dict.items():
            rows.append({
               "ogc_fid": bldg_id,
               "object_name": details.get("object_name", param_key),
               "param_name": param_key,
               "assigned_value": details.get("assigned_value"),
               "min_val": details.get("min_val"),
               "max_val": details.get("max_val")
            })
    if not rows:
        return
    df = pd.DataFrame(rows)
    out_path = _make_assigned_path("assigned_lighting.csv", logs_base_dir)
    df.to_csv(out_path, index=False)
    logger.info(f"Lighting log written to {out_path}")


def _write_equipment_csv(assigned_equip_log, logs_base_dir):
    rows = []
    for bldg_id, outer_dict in assigned_equip_log.items():
        param_dict = outer_dict.get("assigned", outer_dict)
        for param_key, details in param_dict.items():
            rows.append({
                "ogc_fid": bldg_id,
                "object_name": details.get("object_name", param_key),
                "param_name": param_key,
                "assigned_value": details.get("assigned_value"),
                "min_val": details.get("min_val"),
                "max_val": details.get("max_val")
            })
    if not rows:
        return
    df = pd.DataFrame(rows)
    out_path = _make_assigned_path("assigned_equipment.csv", logs_base_dir)
    df.to_csv(out_path, index=False)
    logger.info(f"Equipment log written to {out_path}")


def _write_fenestration_csv(assigned_fenez_log, logs_base_dir):
    _write_generic_log_csv(assigned_fenez_log, "fenez_params", "ogc_fid", logs_base_dir)

def _write_dhw_csv(assigned_dhw_log, logs_base_dir):
    _write_generic_log_csv(assigned_dhw_log, "dhw_params", "ogc_fid", logs_base_dir)


###############################################################################
# UPDATED: Specialized code to ensure HVAC CSV has "assigned_value" column
###############################################################################
def _write_hvac_csv(assigned_hvac_log, logs_base_dir):
    """
    Specialized function to flatten building-level vs. zone-level HVAC data
    into rows each containing an 'assigned_value'.
    """
    rows = []
    if not assigned_hvac_log:
        logger.debug("HVAC log is empty. Skipping CSV.")
        return

    for bldg_id, hvac_data in assigned_hvac_log.items():
        # 1) building-level HVAC parameters
        hvac_params = hvac_data.get("hvac_params", {})
        for param_name, param_val in hvac_params.items():
            rows.append({
                "ogc_fid": bldg_id,
                "zone_name": None,
                "param_name": param_name,
                "assigned_value": param_val
            })

        # 2) zone-level HVAC parameters
        if "zones" in hvac_data:
            for zone_name, zone_info in hvac_data["zones"].items():
                for param_name, param_val in zone_info.items():
                    rows.append({
                        "ogc_fid": bldg_id,
                        "zone_name": zone_name,
                        "param_name": param_name,
                        "assigned_value": param_val
                    })

    if not rows:
        logger.debug("No rows generated for assigned_hvac_params.csv. Skipping write.")
        return

    df = pd.DataFrame(rows)
    out_path = _make_assigned_path("assigned_hvac_params.csv", logs_base_dir)
    df.to_csv(out_path, index=False)
    logger.info(f"HVAC log written to {out_path}")


###############################################################################
# UPDATED: Specialized code to ensure Vent CSV has "assigned_value" column
###############################################################################
def _write_vent_csv(assigned_vent_log, logs_base_dir):
    """
    Specialized function to flatten building-level vs. zone-level Vent data
    into rows each containing an 'assigned_value'.
    """
    rows = []
    if not assigned_vent_log:
        logger.debug("Ventilation log is empty. Skipping CSV.")
        return

    for bldg_id, vent_data in assigned_vent_log.items():
        # 1) building-level ventilation parameters
        bld_params = vent_data.get("building_params", {})
        for param_name, param_val in bld_params.items():
            rows.append({
                "ogc_fid": bldg_id,
                "zone_name": None,
                "param_name": param_name,
                "assigned_value": param_val
            })

        # 2) schedule_details
        sched_details = vent_data.get("schedule_details", {})
        for param_name, param_val in sched_details.items():
            rows.append({
                "ogc_fid": bldg_id,
                "zone_name": None,
                "param_name": param_name,
                "assigned_value": param_val
            })

        # 3) zone-level ventilation parameters
        zones_dict = vent_data.get("zones", {})
        for zone_name, zinfo in zones_dict.items():
            for param_name, param_val in zinfo.items():
                rows.append({
                    "ogc_fid": bldg_id,
                    "zone_name": zone_name,
                    "param_name": param_name,
                    "assigned_value": param_val
                })

    if not rows:
        logger.debug("No rows generated for assigned_ventilation.csv. Skipping write.")
        return

    df = pd.DataFrame(rows)
    out_path = _make_assigned_path("assigned_ventilation.csv", logs_base_dir)
    df.to_csv(out_path, index=False)
    logger.info(f"Ventilation log written to {out_path}")


def _write_shading_csv(assigned_shading_log, logs_base_dir):
    rows = []
    for window_id, data_for_window in assigned_shading_log.items():
        shading_params = data_for_window.get("shading_params_picked", {})
        status = data_for_window.get("shading_creation_status", "")
        type_key = data_for_window.get("shading_type_key_used", "")
        strategy = data_for_window.get("strategy_used", "")
        control_name = data_for_window.get("shading_control_name_assigned", "")
        blind_mat_name = data_for_window.get("blind_material_name_used", "")

        if not shading_params and not status:
            continue

        base_row_info = {
            "window_id": window_id,
            "shading_type_key": type_key,
            "strategy": strategy,
            "creation_status": status,
            "control_name": control_name,
            "blind_material_name": blind_mat_name
        }

        if shading_params:
            for param_name, param_val in shading_params.items():
                row = base_row_info.copy()
                row["param_name"] = param_name
                row["assigned_value"] = str(param_val)
                rows.append(row)
        else:
            row = base_row_info.copy()
            row["param_name"] = "N/A"
            row["assigned_value"] = "N/A (No params picked or error)"
            rows.append(row)
            
    if not rows:
        logger.debug("No data to write for assigned_shading_params.csv")
        return
        
    df = pd.DataFrame(rows)
    out_path = _make_assigned_path("assigned_shading_params.csv", logs_base_dir)
    df.to_csv(out_path, index=False)
    logger.info(f"Shading parameters log written to {out_path}")


def _write_groundtemp_csv(assigned_groundtemp_log, logs_base_dir):
    _write_generic_log_csv(assigned_groundtemp_log, "ground_temperatures", "building_id_or_global", logs_base_dir)

def _write_setzone_csv(assigned_setzone_log, logs_base_dir):
    _write_generic_log_csv(assigned_setzone_log, "zone_sizing_outdoor_air", "ogc_fid", logs_base_dir)

def _write_epw_csv(assigned_epw_log, logs_base_dir):
    _write_generic_log_csv(assigned_epw_log, "epw_assignments", "ogc_fid", logs_base_dir)

------------------------------------------------------------

File: D:\Documents\E_Plus_2030_py\idf_objects\HVAC\assign_hvac_values.py
============================================================
# File: HVAC/assign_hvac_values.py

import random
from .hvac_lookup import hvac_lookup

def find_hvac_overrides(
    building_id,
    building_function,
    residential_type,
    non_residential_type,
    age_range,
    scenario,
    calibration_stage,
    user_config
):
    """
    Returns a list of user_config rows that match the specified building criteria.
    Each row might contain override data for setpoints or even schedule definitions.
    For example, a row might say:
      {
        "building_id": 123,
        "param_name": "heating_day_setpoint",
        "fixed_value": 20.5
      }
    or:
      {
        "building_id": 123,
        "param_name": "hvac_availability_weekday",
        "override_blocks": [
          ("06:00",1.0),
          ("22:00",0.3),
          ("24:00",0.0)
        ]
      }
    """
    matches = []
    for row in user_config or []:
        if "building_id" in row and row["building_id"] != building_id:
            continue
        if "building_function" in row and row["building_function"] != building_function:
            continue

        if "residential_type" in row and row["residential_type"] != residential_type:
            continue
        if "non_residential_type" in row and row["non_residential_type"] != non_residential_type:
            continue

        if "age_range" in row and row["age_range"] != age_range:
            continue
        if "scenario" in row and row["scenario"] != scenario:
            continue
        if "calibration_stage" in row and row["calibration_stage"] != calibration_stage:
            continue

        matches.append(row)
    return matches


def pick_val_with_range(rng_tuple, strategy="A", log_dict=None, param_name=None):
    """
    rng_tuple = (min_val, max_val).
    strategy  = "A" => midpoint, 
                "B" => random uniform in [min,max],
                else => pick min_val.

    If log_dict and param_name are given, store both the chosen value and the
    (min_val, max_val) range in log_dict for reference.
    """
    min_v, max_v = rng_tuple

    if strategy == "A":  # midpoint
        chosen = (min_v + max_v) / 2.0
    elif strategy == "B":
        chosen = random.uniform(min_v, max_v)
    else:
        chosen = min_v  # default => pick min

    if log_dict is not None and param_name is not None:
        log_dict[f"{param_name}_range"] = (min_v, max_v)
        log_dict[param_name] = chosen

    return chosen


def override_numeric_range(current_range, row):
    """
    If row has a 'fixed_value' => returns (v, v).
    If row has 'min_val' and 'max_val' => returns (min_val, max_val).
    Otherwise returns current_range unchanged.
    """
    if "fixed_value" in row and row["fixed_value"] is not None:
        v = row["fixed_value"]
        return (v, v)
    if "min_val" in row and "max_val" in row and row["min_val"] is not None and row["max_val"] is not None:
        return (row["min_val"], row["max_val"])
    return current_range


def override_schedule_details(schedule_details, row):
    """
    If row has param_name like "hvac_availability_weekday" or "occupancy_weekend", etc.,
    and 'override_blocks' = a list of (time, value),
    we replace the default schedule details with user-provided blocks.

    For example:
      row = {
        "param_name": "hvac_availability_weekday",
        "override_blocks": [
           ("06:00",1.0), ("22:00",0.2), ("24:00",0.0)
        ]
      }
    We'll do:
      schedule_details["hvac_availability"]["weekday"] = that new list
    """

    pname = row.get("param_name", "").lower()
    override_blocks = row.get("override_blocks")
    if not override_blocks:
        return  # nothing to do

    # Example approach:
    if pname == "hvac_availability_weekday" and "hvac_availability" in schedule_details:
        schedule_details["hvac_availability"]["weekday"] = override_blocks
    elif pname == "hvac_availability_weekend" and "hvac_availability" in schedule_details:
        schedule_details["hvac_availability"]["weekend"] = override_blocks

    elif pname == "occupancy_weekday" and "occupancy" in schedule_details:
        schedule_details["occupancy"]["weekday"] = override_blocks
    elif pname == "occupancy_weekend" and "occupancy" in schedule_details:
        schedule_details["occupancy"]["weekend"] = override_blocks

    elif pname == "setpoints_weekday" and "setpoints" in schedule_details:
        schedule_details["setpoints"]["weekday"] = override_blocks
    elif pname == "setpoints_weekend" and "setpoints" in schedule_details:
        schedule_details["setpoints"]["weekend"] = override_blocks

    # etc. Expand for infiltration, ventilation, or other schedule types
    # if row["param_name"] = "infiltration_weekday" => ...
    return


def assign_hvac_ideal_parameters(
    building_id=None,
    building_function=None,
    residential_type=None,
    non_residential_type=None,
    age_range=None,
    scenario=None,
    calibration_stage="pre_calibration",
    strategy="A",
    random_seed=None,
    user_config_hvac=None,
    assigned_hvac_log=None
):
    """
    1) Look up default parameter ranges + schedule_details from hvac_lookup
       using (calibration_stage, scenario, building_function, subtype, age_range).
    2) Find user_config rows that override numeric setpoints or schedule blocks.
    3) Apply those overrides.
    4) Pick final numeric setpoints with pick_val_with_range(...).
    5) Build a dictionary "final_hvac_params" including:
       - heating_day_setpoint, cooling_day_setpoint, etc.
       - schedule_details (with possibly overridden time blocks).
    6) Optionally store it in assigned_hvac_log[bldg_id]["hvac_params"].
    7) Return final_hvac_params.
    """

    # For reproducibility if desired
    if random_seed is not None:
        random.seed(random_seed)

    # 1) Lookup the base data in hvac_lookup
    if calibration_stage not in hvac_lookup:
        calibration_stage = "pre_calibration"
    stage_block = hvac_lookup[calibration_stage]

    if scenario not in stage_block:
        scenario = next(iter(stage_block.keys()))
    scenario_block = stage_block[scenario]

    if building_function not in scenario_block:
        building_function = next(iter(scenario_block.keys()))
    bf_block = scenario_block[building_function]

    # Decide subtype based on function
    if building_function.lower() == "residential":
        subtype = residential_type or next(iter(bf_block.keys()))
    else:
        subtype = non_residential_type or next(iter(bf_block.keys()))

    if subtype not in bf_block:
        subtype = next(iter(bf_block.keys()))
    sub_block = bf_block[subtype]

    if age_range not in sub_block:
        age_range = next(iter(sub_block.keys()))
    final_block = sub_block[age_range]

    # Extract base setpoint ranges
    heat_day_rng     = final_block.get("heating_day_setpoint_range", (20.0, 20.0))
    heat_night_rng   = final_block.get("heating_night_setpoint_range", (16.0, 16.0))
    cool_day_rng     = final_block.get("cooling_day_setpoint_range", (25.0, 25.0))
    cool_night_rng   = final_block.get("cooling_night_setpoint_range", (27.0, 27.0))
    max_heat_air_rng = final_block.get("max_heating_supply_air_temp_range", (50.0, 50.0))
    min_cool_air_rng = final_block.get("min_cooling_supply_air_temp_range", (13.0, 13.0))

    # Extract default schedule_details (which might contain multi-block definitions)
    schedule_details = final_block.get("schedule_details", {})

    # 2) Gather user_config overrides
    matches = find_hvac_overrides(
        building_id or 0,
        building_function or "",
        residential_type or "",
        non_residential_type or "",
        age_range or "",
        scenario or "",
        calibration_stage,
        user_config_hvac
    )

    # We'll store intermediate picks in local_log
    local_log = {}

    # 3) Apply overrides
    for row in matches:
        pname = row.get("param_name", "")

        # (A) Numeric setpoint overrides
        if pname == "heating_day_setpoint":
            heat_day_rng = override_numeric_range(heat_day_rng, row)
        elif pname == "heating_night_setpoint":
            heat_night_rng = override_numeric_range(heat_night_rng, row)
        elif pname == "cooling_day_setpoint":
            cool_day_rng = override_numeric_range(cool_day_rng, row)
        elif pname == "cooling_night_setpoint":
            cool_night_rng = override_numeric_range(cool_night_rng, row)
        elif pname == "max_heating_supply_air_temp":
            max_heat_air_rng = override_numeric_range(max_heat_air_rng, row)
        elif pname == "min_cooling_supply_air_temp":
            min_cool_air_rng = override_numeric_range(min_cool_air_rng, row)

        # (B) Schedule block overrides
        elif "override_blocks" in row:
            # This means we might override hvac_availability "weekday" or "weekend" blocks, etc.
            override_schedule_details(schedule_details, row)

        # else: skip if we don't recognize param_name

    # 4) Pick final numeric setpoints
    heating_day_setpoint = pick_val_with_range(
        heat_day_rng, strategy, local_log, param_name="heating_day_setpoint"
    )
    heating_night_setpoint = pick_val_with_range(
        heat_night_rng, strategy, local_log, param_name="heating_night_setpoint"
    )
    cooling_day_setpoint = pick_val_with_range(
        cool_day_rng, strategy, local_log, param_name="cooling_day_setpoint"
    )
    cooling_night_setpoint = pick_val_with_range(
        cool_night_rng, strategy, local_log, param_name="cooling_night_setpoint"
    )
    max_heating_supply_air_temp = pick_val_with_range(
        max_heat_air_rng, strategy, local_log, param_name="max_heating_supply_air_temp"
    )
    min_cooling_supply_air_temp = pick_val_with_range(
        min_cool_air_rng, strategy, local_log, param_name="min_cooling_supply_air_temp"
    )

    # 5) Build final dictionary
    final_hvac_params = {
        "heating_day_setpoint": heating_day_setpoint,
        "heating_day_setpoint_range": local_log["heating_day_setpoint_range"],

        "heating_night_setpoint": heating_night_setpoint,
        "heating_night_setpoint_range": local_log["heating_night_setpoint_range"],

        "cooling_day_setpoint": cooling_day_setpoint,
        "cooling_day_setpoint_range": local_log["cooling_day_setpoint_range"],

        "cooling_night_setpoint": cooling_night_setpoint,
        "cooling_night_setpoint_range": local_log["cooling_night_setpoint_range"],

        "max_heating_supply_air_temp": max_heating_supply_air_temp,
        "max_heating_supply_air_temp_range": local_log["max_heating_supply_air_temp_range"],

        "min_cooling_supply_air_temp": min_cooling_supply_air_temp,
        "min_cooling_supply_air_temp_range": local_log["min_cooling_supply_air_temp_range"],

        "schedule_details": schedule_details
    }

    # 6) Optionally store in assigned_hvac_log
    if assigned_hvac_log is not None and building_id is not None:
        if building_id not in assigned_hvac_log:
            assigned_hvac_log[building_id] = {}
        assigned_hvac_log[building_id]["hvac_params"] = final_hvac_params

    # 7) Return
    return final_hvac_params

------------------------------------------------------------

File: D:\Documents\E_Plus_2030_py\idf_objects\HVAC\custom_hvac.py
============================================================
# File: idf_objects/HVAC/custom_hvac.py

from typing import Dict, Optional, Any, List

# Assuming these modules are in the same directory or accessible via path
from .assign_hvac_values import assign_hvac_ideal_parameters
from .schedule_creation import create_schedules_for_building

def add_HVAC_Ideal_to_all_zones(
    idf, # The IDF model object (e.g., from eppy)
    building_row: Optional[Dict[str, Any]] = None,
    calibration_stage: str = "pre_calibration",
    strategy: str = "A",
    random_seed: Optional[int] = None,
    user_config_hvac: Optional[List[Dict[str, Any]]] = None,
    assigned_hvac_log: Optional[Dict] = None
):
    """
    Adds an Ideal Loads Air System to every Zone in the IDF model, along with
    necessary schedules & setpoints derived from hvac_lookup data and user overrides.

    Args:
        idf: The IDF model object.
        building_row: A dictionary containing metadata about the building
                      (e.g., ogc_fid, building_function, age_range).
        calibration_stage: Stage identifier (e.g., "pre_calibration", "calibrated").
        strategy: Strategy for picking values from ranges ('A'=midpoint, 'B'=random).
        random_seed: Optional seed for reproducibility if strategy 'B' is used.
        user_config_hvac: Optional list of override dictionaries for HVAC parameters.
        assigned_hvac_log: Optional dictionary to store assigned parameters and logs.

    Modifies the `idf` object in place and optionally updates `assigned_hvac_log`.
    """

    # --- Basic Setup & Input Handling ---
    if building_row is None:
        building_row = {} # Use empty dict to avoid errors on .get()
        print("[Custom HVAC Warning] add_HVAC_Ideal_to_all_zones called without building_row metadata.")

    bldg_id = building_row.get("ogc_fid", 0) # Default to 0 if no ID provided
    bldg_func = building_row.get("building_function", "residential") # Default func
    res_type = building_row.get("residential_type") # Can be None if not residential
    nonres_type = building_row.get("non_residential_type") # Can be None if residential
    age_range = building_row.get("age_range", "unknown") # Default age
    scenario = building_row.get("scenario", "default_scenario") # Default scenario

    print(f"[Custom HVAC Info] Setting up Ideal Loads for Building ID: {bldg_id}, Function: {bldg_func}, Age: {age_range}")

    # --- 1) Get HVAC Parameters & Schedule Definitions ---
    try:
        hvac_params = assign_hvac_ideal_parameters(
            building_id=bldg_id,
            building_function=bldg_func,
            residential_type=res_type,
            non_residential_type=nonres_type,
            age_range=age_range,
            scenario=scenario,
            calibration_stage=calibration_stage,
            strategy=strategy,
            random_seed=random_seed,
            user_config_hvac=user_config_hvac,
            assigned_hvac_log=assigned_hvac_log # Pass log dict through
        )
    except Exception as e:
        print(f"[Custom HVAC Error] Failed to assign HVAC parameters for building {bldg_id}: {e}")
        return # Cannot proceed without parameters

    if not hvac_params or not isinstance(hvac_params, dict):
         print(f"[Custom HVAC Error] assign_hvac_ideal_parameters returned invalid data for building {bldg_id}. Aborting HVAC setup.")
         return

    # --- 2) Create Schedules in IDF ---
    schedule_details = hvac_params.get("schedule_details")
    if schedule_details and isinstance(schedule_details, dict):
        try:
            create_schedules_for_building(
                idf,
                schedule_details=schedule_details,
                building_id=bldg_id,
                assigned_hvac_log=assigned_hvac_log # Pass log dict through
            )
        except Exception as e:
            print(f"[Custom HVAC Error] Failed during schedule creation for building {bldg_id}: {e}")
            # Decide whether to proceed without custom schedules or abort
            # return
    else:
        print(f"[Custom HVAC Warning] No schedule_details found in hvac_params for building {bldg_id}. Schedules may not be created correctly.")


    # --- 3) Ensure Core IDF Objects Exist (Schedule Limits) ---
    # Check/create SCHEDULETYPELIMITS for Temperature
    if not idf.getobject("SCHEDULETYPELIMITS", "Temperature"):
        print("[Custom HVAC Info] Creating SCHEDULETYPELIMITS: Temperature")
        stl_temp = idf.newidfobject("SCHEDULETYPELIMITS")
        stl_temp.Name = "Temperature"
        stl_temp.Lower_Limit_Value = -100
        stl_temp.Upper_Limit_Value = 200
        stl_temp.Numeric_Type = "CONTINUOUS"
        # stl_temp.Unit_Type = "Temperature" # Optional depending on E+ version

    # Check/create SCHEDULETYPELIMITS for ControlType
    if not idf.getobject("SCHEDULETYPELIMITS", "ControlType"):
        print("[Custom HVAC Info] Creating SCHEDULETYPELIMITS: ControlType")
        stl_ctrl = idf.newidfobject("SCHEDULETYPELIMITS")
        stl_ctrl.Name = "ControlType"
        stl_ctrl.Lower_Limit_Value = 0
        stl_ctrl.Upper_Limit_Value = 4 # 0=Uncontrolled, 1=Thermo, 2=Humid, 3=Staged, 4=DualSetpt
        stl_ctrl.Numeric_Type = "DISCRETE"

    # Check/create SCHEDULETYPELIMITS for Fraction (FIX from previous analysis)
    if not idf.getobject("SCHEDULETYPELIMITS", "Fraction"):
        print("[Custom HVAC Info] Creating SCHEDULETYPELIMITS: Fraction")
        stl_frac = idf.newidfobject("SCHEDULETYPELIMITS")
        stl_frac.Name = "Fraction"
        stl_frac.Lower_Limit_Value = 0.0
        stl_frac.Upper_Limit_Value = 1.0
        stl_frac.Numeric_Type = "CONTINUOUS"
        # stl_frac.Unit_Type = "Dimensionless" # Optional

    # --- 4) Create Default Control Type Schedule (if needed) ---
    # This schedule tells the thermostat to use Dual Setpoints (Control Type 1)
    control_type_sched_name = "ZONE_CONTROL_TYPE_SCHEDULE" # Use underscore for safety
    ctrl_sched = idf.getobject("SCHEDULE:COMPACT", control_type_sched_name.upper())
    if not ctrl_sched:
        print(f"[Custom HVAC Info] Creating SCHEDULE:COMPACT: {control_type_sched_name}")
        ctrl_sched = idf.newidfobject("SCHEDULE:COMPACT")
        ctrl_sched.Name = control_type_sched_name
        ctrl_sched.Schedule_Type_Limits_Name = "ControlType"
        # --- FIX from previous analysis: Correct field format ---
        ctrl_sched.Field_1 = "Through: 12/31"  # No comma
        ctrl_sched.Field_2 = "For: AllDays"     # No comma
        # Control Type 4 corresponds to ThermostatSetpoint:DualSetpoint
        # which matches the thermostat object defined below.
        ctrl_sched.Field_3 = "Until: 24:00,4;"
    else:
        print(f"[Custom HVAC Info] Found existing schedule: {control_type_sched_name}")
        # Optionally verify/update its fields here if needed

    # --- 5) Retrieve Key Parameters for Ideal Loads ---
    try:
        max_heat_temp = hvac_params["max_heating_supply_air_temp"]
        min_cool_temp = hvac_params["min_cooling_supply_air_temp"]
    except KeyError as e:
         print(f"[Custom HVAC Error] Missing required temperature limit '{e}' in hvac_params for building {bldg_id}. Aborting.")
         return

    # Get the name of the HVAC availability schedule created earlier
    # Default to 'AlwaysOn' if not found, but log a warning
    hvac_avail_sched_name = "AlwaysOn" # Fallback default
    if schedule_details:
         hvac_avail_info = schedule_details.get("hvac_availability", {})
         hvac_avail_sched_name = hvac_avail_info.get("schedule_name", "HVAC_Avail_Sched") # Get name from lookup/override
         # Check if the schedule actually exists in the IDF after creation attempt
         if not idf.getobject("SCHEDULE:COMPACT", hvac_avail_sched_name.upper()):
             print(f"[Custom HVAC Warning] HVAC Availability schedule '{hvac_avail_sched_name}' not found in IDF. Defaulting IdealLoads Availability to 'AlwaysOn'.")
             hvac_avail_sched_name = "AlwaysOn" # Revert to default if creation failed/missing

    # Get the names for setpoint schedules (ensure consistency with schedule_creation.py)
    setpoint_sched_heat_name = "ZONE_HEATING_SETPOINTS" # Default name used in schedule_creation
    setpoint_sched_cool_name = "ZONE_COOLING_SETPOINTS" # Default name used in schedule_creation
    if schedule_details:
        setpoints_info = schedule_details.get("setpoints", {})
        setpoint_sched_heat_name = setpoints_info.get("schedule_name_heat", setpoint_sched_heat_name)
        setpoint_sched_cool_name = setpoints_info.get("schedule_name_cool", setpoint_sched_cool_name)
        # Check if these schedules exist
        if not idf.getobject("SCHEDULE:COMPACT", setpoint_sched_heat_name.upper()):
             print(f"[Custom HVAC Warning] Heating setpoint schedule '{setpoint_sched_heat_name}' not found in IDF.")
             # Thermostat object might fail later if schedule is missing
        if not idf.getobject("SCHEDULE:COMPACT", setpoint_sched_cool_name.upper()):
             print(f"[Custom HVAC Warning] Cooling setpoint schedule '{setpoint_sched_cool_name}' not found in IDF.")

    # --- 6) Process Each Zone ---
    zones = idf.idfobjects.get("ZONE", []) # Use .get for safety
    if not zones:
        print(f"[Custom HVAC Error] No ZONE objects found in the IDF model for building {bldg_id}. Cannot add HVAC systems.")
        return

    # Ensure the assigned_hvac_log structure is ready for zone data
    if assigned_hvac_log is not None:
        if bldg_id not in assigned_hvac_log:
            assigned_hvac_log[bldg_id] = {}
        if "zones" not in assigned_hvac_log[bldg_id]:
            assigned_hvac_log[bldg_id]["zones"] = {}

    print(f"[Custom HVAC Info] Processing {len(zones)} zones for Ideal Loads setup...")
    for zone_obj in zones:
        zone_name = zone_obj.Name
        print(f"  - Processing Zone: {zone_name}")

        # Define object names based on zone name
        thermostat_name = f"{zone_name}_CONTROLS"
        dual_setpoint_name = f"{zone_name}_SETPOINTS"
        equip_conn_name = zone_name # Convention often uses zone name
        equip_list_name = f"{zone_name}_EQUIPMENT"
        ideal_loads_name = f"{zone_name}_Ideal_Loads"
        inlet_nodelist_name = f"{zone_name}_INLETS"
        inlet_node_name = f"{zone_name}_INLET" # Single node for the list
        zone_node_name = f"{zone_name}_NODE"
        return_node_name = f"{zone_name}_OUTLET"


        # 6a) Create/Update Thermostat (ZONECONTROL:THERMOSTAT)
        # Use getobject for case-insensitive check if needed
        thermo = idf.getobject("ZONECONTROL:THERMOSTAT", thermostat_name.upper())
        if not thermo:
            thermo = idf.newidfobject("ZONECONTROL:THERMOSTAT")
            thermo.Name = thermostat_name
        # Assign zone name - check attribute based on potential E+ versions
        if hasattr(thermo, "Zone_or_ZoneList_or_Space_or_SpaceList_Name"):
            thermo.Zone_or_ZoneList_or_Space_or_SpaceList_Name = zone_name
        else:
            thermo.Zone_or_ZoneList_Name = zone_name # Older versions
        thermo.Control_Type_Schedule_Name = control_type_sched_name
        thermo.Control_1_Object_Type = "ThermostatSetpoint:DualSetpoint"
        thermo.Control_1_Name = dual_setpoint_name

        # 6b) Create/Update THERMOSTATSETPOINT:DUALSETPOINT
        dualset = idf.getobject("THERMOSTATSETPOINT:DUALSETPOINT", dual_setpoint_name.upper())
        if not dualset:
            dualset = idf.newidfobject("THERMOSTATSETPOINT:DUALSETPOINT")
            dualset.Name = dual_setpoint_name
        # --- Use the consistent schedule names identified earlier ---
        dualset.Heating_Setpoint_Temperature_Schedule_Name = setpoint_sched_heat_name
        dualset.Cooling_Setpoint_Temperature_Schedule_Name = setpoint_sched_cool_name


        # 6c) Create/Update ZONEHVAC:EQUIPMENTCONNECTIONS
        # Note: Name convention might vary; here using Zone Name directly
        eq_conn = idf.getobject("ZONEHVAC:EQUIPMENTCONNECTIONS", zone_name.upper())
        if not eq_conn:
            eq_conn = idf.newidfobject("ZONEHVAC:EQUIPMENTCONNECTIONS")
            eq_conn.Zone_Name = zone_name
        # Update fields
        eq_conn.Zone_Conditioning_Equipment_List_Name = equip_list_name
        eq_conn.Zone_Air_Inlet_Node_or_NodeList_Name = inlet_nodelist_name
        eq_conn.Zone_Air_Exhaust_Node_or_NodeList_Name = "" # Assuming no exhaust for ideal loads
        eq_conn.Zone_Air_Node_Name = zone_node_name
        eq_conn.Zone_Return_Air_Node_or_NodeList_Name = return_node_name


        # 6d) Create/Update ZONEHVAC:EQUIPMENTLIST
        eq_list = idf.getobject("ZONEHVAC:EQUIPMENTLIST", equip_list_name.upper())
        if not eq_list:
            eq_list = idf.newidfobject("ZONEHVAC:EQUIPMENTLIST")
            eq_list.Name = equip_list_name
        # Update fields - add Ideal Loads system
        eq_list.Load_Distribution_Scheme = "SequentialLoad" # Or UniformLoad, etc.
        eq_list.Zone_Equipment_1_Object_Type = "ZoneHVAC:IdealLoadsAirSystem"
        eq_list.Zone_Equipment_1_Name = ideal_loads_name
        eq_list.Zone_Equipment_1_Cooling_Sequence = 1
        eq_list.Zone_Equipment_1_Heating_or_NoLoad_Sequence = 1
        # Clear out other equipment slots if necessary
        # for i in range(2, 5): # Example: Clear eqpt 2, 3, 4
        #    setattr(eq_list, f"Zone_Equipment_{i}_Object_Type", "")
        #    setattr(eq_list, f"Zone_Equipment_{i}_Name", "")


        # 6e) Create/Update ZONEHVAC:IDEALLOADSAIRSYSTEM
        ideal = idf.getobject("ZONEHVAC:IDEALLOADSAIRSYSTEM", ideal_loads_name.upper())
        if not ideal:
            ideal = idf.newidfobject("ZONEHVAC:IDEALLOADSAIRSYSTEM")
            ideal.Name = ideal_loads_name
        # Update fields
        ideal.Availability_Schedule_Name = hvac_avail_sched_name # Use retrieved name
        ideal.Zone_Supply_Air_Node_Name = inlet_nodelist_name # Connects to inlet list
        ideal.Zone_Exhaust_Air_Node_Name = "" # Match connections object
        ideal.System_Inlet_Air_Node_Name = "" # Not connecting to outdoor air here

        # Supply air temperature limits
        ideal.Maximum_Heating_Supply_Air_Temperature = max_heat_temp
        ideal.Minimum_Cooling_Supply_Air_Temperature = min_cool_temp

        # Supply air humidity limits (optional, can be autosized or fixed)
        # ideal.Maximum_Heating_Supply_Air_Humidity_Ratio = 0.0156 # Example
        # ideal.Minimum_Cooling_Supply_Air_Humidity_Ratio = 0.0077 # Example

        # Heating/Cooling capacity limits
        ideal.Heating_Limit = "LimitFlowRateAndCapacity" # Or NoLimit, LimitCapacity, LimitFlowRate
        ideal.Maximum_Heating_Air_Flow_Rate = "Autosize"
        ideal.Maximum_Sensible_Heating_Capacity = "Autosize"

        ideal.Cooling_Limit = "LimitFlowRateAndCapacity"
        ideal.Maximum_Cooling_Air_Flow_Rate = "Autosize"
        ideal.Maximum_Total_Cooling_Capacity = "Autosize"

        # Optional: Dehumidification/Humidification Control
        ideal.Dehumidification_Control_Type = "None" # Or None, Humidistat, etc.
        # ideal.Cooling_Sensible_Heat_Ratio = 0.7 # Example if needed
        ideal.Humidification_Control_Type = "None" # Or None, Humidistat
        # ideal.Design_Specification_Outdoor_Air_Object_Name = "" # Can link DSOA if needed

        # Optional: Economizer / Heat Recovery
        # EnergyPlus 22.x does not accept "FixedDryBulb" for this field
        # (valid options are NoEconomizer, DifferentialDryBulb,
        #  DifferentialEnthalpy). Use NoEconomizer for compatibility.
        ideal.Outdoor_Air_Economizer_Type = "NoEconomizer"
        if hasattr(ideal, "Economizer_Maximum_Limit_Dry_Bulb_Temperature"):
            ideal.Economizer_Maximum_Limit_Dry_Bulb_Temperature = 18  # DegC
        else:
            print(
                f"[Custom HVAC Warning] IdealLoads object '{ideal.Name}' does not "
                "support 'Economizer_Maximum_Limit_Dry_Bulb_Temperature'."
            )
        ideal.Heat_Recovery_Type = "None"


        # 6f) Ensure NODELIST for supply inlets exists
        inlet_nl = idf.getobject("NODELIST", inlet_nodelist_name.upper())
        if not inlet_nl:
            inlet_nl = idf.newidfobject("NODELIST")
            inlet_nl.Name = inlet_nodelist_name
        # Ensure it points to the correct node(s)
        inlet_nl.Node_1_Name = inlet_node_name
        # Clear other nodes if necessary
        # inlet_nl.Node_2_Name = ""


        # 6g) Log zone-level info if requested
        if assigned_hvac_log is not None:
            # Ensure zone entry exists
            if zone_name not in assigned_hvac_log[bldg_id]["zones"]:
                assigned_hvac_log[bldg_id]["zones"][zone_name] = {}
            # Update with HVAC details for this zone
            assigned_hvac_log[bldg_id]["zones"][zone_name].update({
                "hvac_object_name": ideal_loads_name,
                "hvac_object_type": "ZONEHVAC:IDEALLOADSAIRSYSTEM",
                "availability_schedule": hvac_avail_sched_name,
                "thermostat_name": thermostat_name,
                "thermostat_dualsetpoint_name": dual_setpoint_name,
                "heating_setpoint_schedule": setpoint_sched_heat_name,
                "cooling_setpoint_schedule": setpoint_sched_cool_name,
            })

    # --- Final Logging ---
    print(f"[Custom HVAC Info] Completed Ideal Loads HVAC setup for all zones in building {bldg_id}.")

# Example usage (if run standalone for testing):
# if __name__ == '__main__':
#     from eppy.modeleditor import IDF
#     # Load a base IDF file
#     try:
#         idf = IDF("path/to/your/base_model.idf")
#     except FileNotFoundError:
#         print("Error: Base IDF file not found for testing.")
#         exit()
#
#     # Example building metadata
#     test_building_row = {
#         "ogc_fid": 999,
#         "building_function": "residential",
#         "residential_type": "Corner House", # Match a key in hvac_lookup
#         "age_range": "2015 and later",      # Match a key in hvac_lookup
#         "scenario": "scenario1"
#     }
#
#     # Example empty log dict
#     test_log = {}
#
#     # Call the function
#     add_HVAC_Ideal_to_all_zones(
#         idf,
#         building_row=test_building_row,
#         assigned_hvac_log=test_log
#         # Add other parameters as needed for test
#     )
#
#     # Save the modified IDF
#     idf.saveas("path/to/your/output_model_with_hvac.idf")
#     print("Saved modified IDF with Ideal Loads.")
#     # print("Log:", test_log) # Inspect the log if desired
------------------------------------------------------------

File: D:\Documents\E_Plus_2030_py\idf_objects\ventilation\add_ventilation.py
============================================================
# ventilation/add_ventilation.py

import math
from typing import Optional, Dict, Any

# Import specific objects/functions - ensure correct imports
try:
    from geomeppy import IDF
except ImportError:
    IDF = Any

# Import local modules
from idf_objects.ventilation.assign_ventilation_values import (
    assign_ventilation_params_with_overrides
)
from idf_objects.ventilation import schedules # Updated import to get the module
from idf_objects.ventilation.create_ventilation_systems import create_ventilation_system
from idf_objects.ventilation.calc_functions import (
    calc_infiltration_rate_at_1Pa_per_m2,
    calc_required_ventilation_flow
)
from idf_objects.ventilation.mappings import (
    safe_lower,
    map_age_range_to_year_key,
    map_infiltration_key, # This provides the archetype_key
    map_usage_key
)

# Define type alias for clarity
BuildingRow = Dict[str, Any]
ZoneInfoMap = Dict[str, Dict[str, Any]] # e.g., {zone_name: {'area': float, 'is_core': bool}}
AssignedVentLog = Dict[Any, Dict[str, Any]]


def get_zone_floor_area_from_surfaces(idf: IDF, zone_name: str) -> float:
    """
    Calculates the zone's floor area by summing the areas of its 'floor' surfaces.
    This is a fallback if the ZONE object's Floor_Area is 'autocalculate' or not resolved.
    """
    total_floor_surface_area = 0.0
    surfaces = idf.idfobjects.get("BUILDINGSURFACE:DETAILED", [])
    if not surfaces:
        surfaces = idf.idfobjects.get("BuildingSurface:Detailed", [])

    for surface in surfaces:
        try:
            surface_zone_name = getattr(surface, 'Zone_Name', None)
            if surface_zone_name is None:
                continue

            surface_type_attr = getattr(surface, 'Surface_Type', '').lower()

            if surface_zone_name.upper() == zone_name.upper() and surface_type_attr == 'floor':
                surface_area = getattr(surface, 'area', 0.0)
                if surface_area <= 1e-6 and hasattr(surface, 'Gross_Area'):
                    try:
                        surface_area = float(surface.Gross_Area)
                    except (ValueError, TypeError):
                        surface_area = 0.0
                if isinstance(surface_area, (float, int)) and surface_area > 0:
                    total_floor_surface_area += surface_area
        except Exception as e:
            surface_id = getattr(surface, 'Name', 'UnknownSurface')
            print(f"[WARNING] add_ventilation.py: Error accessing properties for surface '{surface_id}' in zone '{zone_name}': {e}")
            continue

    if total_floor_surface_area > 1e-6:
        print(f"[VENT INFO] add_ventilation.py: Calculated floor area for zone '{zone_name}' by summing floor surfaces: {total_floor_surface_area:.2f} m2.")
    return total_floor_surface_area


def add_ventilation_to_idf(
    idf: IDF,
    building_row: BuildingRow,
    calibration_stage: str = "pre_calibration",
    strategy: str = "A", # This is the parameter picking strategy (A, B, C)
    random_seed: Optional[int] = None,
    user_config_vent: Optional[list] = None,
    assigned_vent_log: Optional[AssignedVentLog] = None,
    zone_details: Optional[ZoneInfoMap] = None,
    system_d_infiltration_reduction_factor: float = 1.0,
    infiltration_model: str = "constant",
    typical_delta_t: float = 10.0,
    typical_wind: float = 3.0
):
    """
    Adds infiltration + ventilation to the IDF based on building_row data,
    using archetype-specific schedules.
    """

    # --- 1) Ensure Fallback Schedules Exist ---
    # Only 'AlwaysOnSched' is strictly needed as a global fallback.
    # Archetype-specific schedules will be created dynamically.
    schedules.create_always_on_schedule(idf, "AlwaysOnSched")
    # The generic day/night and workhours schedules are no longer created by default here.
    # They can be created by schedules.get_or_create_archetype_schedule if its lookup points to their patterns.

    # --- 2) Extract building info ---
    bldg_id = building_row.get("ogc_fid", "UnknownBuildingID")
    bldg_func = safe_lower(building_row.get("building_function", "residential"))
    if bldg_func not in ("residential", "non_residential"):
        bldg_func = "residential"

    age_range_str = building_row.get("age_range", "2015 and later")
    scenario = building_row.get("scenario", "scenario1")
    total_bldg_floor_area_m2_input = building_row.get("area", 100.0)

    if not isinstance(total_bldg_floor_area_m2_input, (int, float)) or total_bldg_floor_area_m2_input <= 0:
        print(f"[VENT WARNING] add_ventilation.py: Building {bldg_id}: Invalid total building floor area attribute ('area': {total_bldg_floor_area_m2_input}). Defaulting to 100.0 m2.")
        total_bldg_floor_area_m2_input = 100.0

    # --- 3) Decide lookup keys ---
    # 'infiltration_key' serves as the primary archetype key for schedules as well
    archetype_key = map_infiltration_key(building_row)
    usage_key = map_usage_key(building_row)
    year_key = map_age_range_to_year_key(age_range_str)
    is_res = (bldg_func == "residential")

    # --- 4) Assign building-level ventilation parameters ---
    # This now also determines the *names* for archetype-specific schedules
    assigned_vent = assign_ventilation_params_with_overrides(
        building_id=bldg_id, building_function=bldg_func, age_range=age_range_str,
        scenario=scenario, calibration_stage=calibration_stage, strategy=strategy,
        random_seed=random_seed, user_config_vent=user_config_vent,
        infiltration_key=archetype_key, year_key=year_key, is_residential=is_res,
        default_flow_exponent=0.67
    )

    # --- 5) Unpack chosen building-level parameters ---
    infiltration_base_L_s_m2_10Pa = assigned_vent["infiltration_base_L_s_m2_10Pa"]
    year_factor = assigned_vent["year_factor"]
    fan_pressure_Pa = assigned_vent.get("fan_pressure")
    fan_total_efficiency = assigned_vent.get("fan_total_efficiency")
    f_ctrl = assigned_vent["f_ctrl"]
    hrv_sens_eff = assigned_vent["hrv_eff"]
    hrv_lat_eff = assigned_vent.get("hrv_lat_eff", 0.0)
    # These are now archetype-specific names (or overridden by user_config_vent)
    final_infiltration_sched_name = assigned_vent["infiltration_schedule_name"]
    final_ventilation_sched_name = assigned_vent["ventilation_schedule_name"]
    system_type = assigned_vent["system_type"]
    flow_exponent = assigned_vent["flow_exponent"]
    # The 'strategy' (A, B, C) used for picking parameters will also be used for picking schedule values
    parameter_pick_strategy = strategy # Keep the original strategy name for clarity if needed

    # --- 6) Ensure Archetype-Specific Schedules are Created and Get Chosen Patterns ---
    vent_sched_obj, chosen_vent_wd_pattern, chosen_vent_we_pattern = schedules.get_or_create_archetype_schedule(
        idf=idf,
        target_schedule_name=final_ventilation_sched_name,
        building_function=bldg_func,
        archetype_key=archetype_key,
        purpose="ventilation",
        strategy=parameter_pick_strategy # Use the same strategy for schedule value picking
    )

    infil_sched_obj, chosen_infil_wd_pattern, chosen_infil_we_pattern = schedules.get_or_create_archetype_schedule(
        idf=idf,
        target_schedule_name=final_infiltration_sched_name,
        building_function=bldg_func,
        archetype_key=archetype_key,
        purpose="infiltration",
        strategy=parameter_pick_strategy
    )

    if not vent_sched_obj:
        print(f"[VENT CRITICAL] add_ventilation.py: Failed to create ventilation schedule '{final_ventilation_sched_name}' for Bldg {bldg_id}. Aborting ventilation setup.")
        return
    if not infil_sched_obj:
        print(f"[VENT CRITICAL] add_ventilation.py: Failed to create infiltration schedule '{final_infiltration_sched_name}' for Bldg {bldg_id}. Aborting ventilation setup.")
        return
        
    # Update names in case a fallback name was used by get_or_create_archetype_schedule
    final_ventilation_sched_name = vent_sched_obj.Name
    final_infiltration_sched_name = infil_sched_obj.Name
    
    # --- 7) Debug Print (now includes final schedule names) ---
    print(
        f"[VENT PARAMS] Bldg={bldg_id}, Func={bldg_func}, AgeKey='{year_key}', Sys={system_type}\n"
        f"  LookupKeys: Archetype='{archetype_key}', Usage='{usage_key if usage_key else 'N/A'}'\n"
        f"  InfilParams: Base(L/s/m2@10Pa)={infiltration_base_L_s_m2_10Pa:.4f}, YearFactor={year_factor:.3f}, Exp={flow_exponent}\n"
        f"  VentParams: f_ctrl={f_ctrl:.3f}, HRV_SensEff={hrv_sens_eff:.3f}, HRV_LatEff={hrv_lat_eff:.3f}\n"
        f"  FanParams: Pressure={fan_pressure_Pa if fan_pressure_Pa is not None else 'N/A'} Pa, Efficiency={fan_total_efficiency if fan_total_efficiency is not None else 'N/A'}\n"
        f"  Schedules: Infil='{final_infiltration_sched_name}', Vent='{final_ventilation_sched_name}' (Strategy: {parameter_pick_strategy})"
    )

    # --- 8) Calculate Base Infiltration Rate (@1Pa) per m2 floor area ---
    infiltration_rate_at_1Pa_L_s_per_m2_floor_area = calc_infiltration_rate_at_1Pa_per_m2(
        infiltration_base_at_10Pa_per_m2=infiltration_base_L_s_m2_10Pa,
        year_factor=year_factor,
        flow_exponent=flow_exponent
    )
    if system_type == "D" and system_d_infiltration_reduction_factor != 1.0:
        # ... (infiltration reduction logic remains the same) ...
        effective_rate_before_reduction = infiltration_rate_at_1Pa_L_s_per_m2_floor_area
        infiltration_rate_at_1Pa_L_s_per_m2_floor_area *= system_d_infiltration_reduction_factor
        print(f"  System D: Infiltration rate reduced by factor {system_d_infiltration_reduction_factor:.2f} from {effective_rate_before_reduction:.4f} to {infiltration_rate_at_1Pa_L_s_per_m2_floor_area:.4f} L/s/m2 @ 1Pa")


    # --- 9) Calculate Total Required Mechanical Ventilation Flow for the building ---
    vent_flow_m3_s_total_building = calc_required_ventilation_flow(
        building_function=bldg_func,
        f_ctrl_val=f_ctrl,
        floor_area_m2=total_bldg_floor_area_m2_input,
        usage_key=usage_key
    )

    # --- 10) Determine DSOA object name and ensure it exists for System D ---
    # ... (DSOA logic remains the same) ...
    dsoa_object_name_global = "DSOA_Global" 
    if system_type == "D":
        dsoa_obj = idf.getobject("DESIGNSPECIFICATION:OUTDOORAIR", dsoa_object_name_global.upper())
        if not dsoa_obj:
            print(f"[VENT INFO] add_ventilation.py: Building {bldg_id}: Creating default DesignSpecification:OutdoorAir: {dsoa_object_name_global}")
            try:
                dsoa_obj = idf.newidfobject("DESIGNSPECIFICATION:OUTDOORAIR", Name=dsoa_object_name_global)
                dsoa_obj.Outdoor_Air_Method = "Flow/Area" 
            except Exception as e:
                print(f"[ERROR] add_ventilation.py: Building {bldg_id}: Failed to create {dsoa_object_name_global}: {e}")
                dsoa_obj = None

        if dsoa_obj: 
            base_design_rate_L_s_m2 = 0.0
            if bldg_func == "residential":
                base_design_rate_L_s_m2 = 0.9
            else: 
                usage_flow_map_L_s_m2 = { 
                    "office_area_based": 1.0, "childcare": 4.8, "retail": 0.6,
                    "meeting_function": 1.0, "healthcare_function": 1.2, "sport_function": 1.5,
                    "cell_function": 0.8, "industrial_function": 0.5, "accommodation_function": 0.9,
                    "education_function": 1.1, "other_use_function": 0.6
                }
                base_design_rate_L_s_m2 = usage_flow_map_L_s_m2.get(usage_key, 1.0)
            
            dsoa_flow_per_area_m3_s_m2 = (base_design_rate_L_s_m2 * f_ctrl) / 1000.0
            dsoa_obj.Outdoor_Air_Flow_per_Zone_Floor_Area = dsoa_flow_per_area_m3_s_m2
            dsoa_obj.Outdoor_Air_Flow_per_Person = 0.0 
            dsoa_obj.Outdoor_Air_Flow_per_Zone = 0.0
            dsoa_obj.Outdoor_Air_Flow_Air_Changes_per_Hour = 0.0
            print(f"  System D: Set DSOA '{dsoa_obj.Name}' Outdoor_Air_Flow_per_Zone_Floor_Area to {dsoa_flow_per_area_m3_s_m2:.6f} m3/s-m2 (base {base_design_rate_L_s_m2:.2f} L/s/m2, f_ctrl {f_ctrl:.3f})")


    # --- 11) Get Zones and Prepare Zone Information Map ---
    # ... (Zone area calculation logic remains the same) ...
    zones_in_idf = idf.idfobjects.get("ZONE", [])
    if not zones_in_idf: zones_in_idf = idf.idfobjects.get("Zone", [])
    if not zones_in_idf:
        print(f"[VENT ERROR] add_ventilation.py: Building {bldg_id}: No ZONE objects found. Cannot proceed."); return

    num_zones = len(zones_in_idf)
    effective_zone_info_map: ZoneInfoMap = {}
    sum_of_individual_zone_areas = 0.0

    if zone_details: 
        valid_zone_details = True; temp_total_area = 0.0
        for zd_name, zd_props in zone_details.items():
            if not (isinstance(zd_props, dict) and 'area' in zd_props and 
                    isinstance(zd_props['area'], (float, int)) and zd_props['area'] >= 0 and 
                    'is_core' in zd_props and isinstance(zd_props['is_core'], bool)):
                valid_zone_details = False; break
            temp_total_area += zd_props['area']
        if valid_zone_details and temp_total_area > 1e-6:
            effective_zone_info_map = zone_details
            sum_of_individual_zone_areas = temp_total_area
        else: effective_zone_info_map = {}; sum_of_individual_zone_areas = 0.0 
    
    if not effective_zone_info_map or sum_of_individual_zone_areas <= 1e-6: 
        print(f"[VENT INFO] add_ventilation.py: Bldg {bldg_id}: Calculating zone areas/core status from IDF.")
        sum_of_individual_zone_areas = 0.0; effective_zone_info_map = {} 
        for zone_obj in zones_in_idf:
            # ... (detailed area calculation logic as before) ...
            zone_name_key = zone_obj.Name; area_val = 0.0; raw_field_value_str = ""
            try: area_val = getattr(zone_obj, 'floor_area', 0.0) 
            except Exception: area_val = 0.0
            if not isinstance(area_val, (float, int)) or area_val < 0: area_val = 0.0
            
            if area_val < 1e-6: 
                area_val = 0.0 
                try: 
                    raw_field_value_str = str(getattr(zone_obj, 'Floor_Area', "")).strip().lower()
                    if raw_field_value_str == "autocalculate":
                        area_val = get_zone_floor_area_from_surfaces(idf, zone_name_key)
                        if area_val < 1e-6: area_val = 0.0
                    elif raw_field_value_str: area_val = float(raw_field_value_str)
                    if area_val < 0: area_val = 0.0 
                except Exception: area_val = 0.0
            effective_zone_info_map[zone_name_key] = {'area': area_val, 'is_core': "_core" in safe_lower(zone_name_key)}
            sum_of_individual_zone_areas += area_val

    use_equal_split_fallback = False
    final_total_area_for_proportions = sum_of_individual_zone_areas

    if sum_of_individual_zone_areas <= 1e-6:
        print(f"[VENT ERROR] add_ventilation.py: Bldg {bldg_id}: Sum of individual zone areas is {sum_of_individual_zone_areas}. Fallback active.")
        if total_bldg_floor_area_m2_input > 0 and num_zones > 0:
            use_equal_split_fallback = True
            average_zone_area_for_fallback = total_bldg_floor_area_m2_input / num_zones
            # ... (fallback logic as before) ...
            temp_map_for_fallback = {}
            for zone_obj_fb in zones_in_idf:
                temp_map_for_fallback[zone_obj_fb.Name] = {'area': average_zone_area_for_fallback, 'is_core': "_core" in safe_lower(zone_obj_fb.Name)}
            effective_zone_info_map = temp_map_for_fallback
            final_total_area_for_proportions = total_bldg_floor_area_m2_input 
        else:
            print(f"[VENT CRITICAL] add_ventilation.py: Bldg {bldg_id}: Cannot distribute flows. Sum of zone areas is zero and input building area is zero or no zones. Aborting."); return


    # --- 12) Log Building-Level Parameters & Chosen Schedule Patterns ---
    if assigned_vent_log is not None:
        if bldg_id not in assigned_vent_log: assigned_vent_log[bldg_id] = {}
        log_building_params = assigned_vent.copy()
        # ... (existing log_building_params assignments) ...
        log_building_params["infiltration_rate_at_1Pa_L_s_per_m2_EFFECTIVE"] = infiltration_rate_at_1Pa_L_s_per_m2_floor_area
        log_building_params["ventilation_total_required_m3_s_building"] = vent_flow_m3_s_total_building
        log_building_params["total_bldg_floor_area_m2_input_attr"] = total_bldg_floor_area_m2_input
        log_building_params["sum_of_individual_zone_areas_derived"] = sum_of_individual_zone_areas 
        log_building_params["final_total_area_used_for_proportions"] = final_total_area_for_proportions
        log_building_params["flow_distribution_method"] = "EqualSplitFallbackLogicActive" if use_equal_split_fallback else "ProportionalToIndividualZoneArea"
        log_building_params["system_d_infiltration_reduction_factor_applied"] = system_d_infiltration_reduction_factor if system_type == "D" and system_d_infiltration_reduction_factor != 1.0 else None

        assigned_vent_log[bldg_id]["building_params"] = log_building_params

        # ---- NEW LOGGING SECTION for SCHEDULES ----
        schedule_log_details = {}
        if vent_sched_obj:
            schedule_log_details["ventilation_schedule_name"] = vent_sched_obj.Name
            if chosen_vent_wd_pattern: # Log chosen patterns only if newly created or returned by creator
                schedule_log_details["ventilation_chosen_weekday_pattern"] = chosen_vent_wd_pattern
            if chosen_vent_we_pattern:
                schedule_log_details["ventilation_chosen_weekend_pattern"] = chosen_vent_we_pattern
        if infil_sched_obj:
            schedule_log_details["infiltration_schedule_name"] = infil_sched_obj.Name
            if chosen_infil_wd_pattern:
                schedule_log_details["infiltration_chosen_weekday_pattern"] = chosen_infil_wd_pattern
            if chosen_infil_we_pattern:
                schedule_log_details["infiltration_chosen_weekend_pattern"] = chosen_infil_we_pattern
        assigned_vent_log[bldg_id]["schedule_details"] = schedule_log_details
        # --- END NEW LOGGING SECTION ---

        assigned_vent_log[bldg_id]["zones"] = {}

    print(
        f"[VENT FLOWS] add_ventilation.py: Bldg={bldg_id}: BaseInfilRate(@1Pa,Eff)={infiltration_rate_at_1Pa_L_s_per_m2_floor_area:.4f} L/s/m2, "
        f"TotalMechVentReq={vent_flow_m3_s_total_building:.4f} m3/s, "
        f"DistMethod={'EqualSplitFallbackLogicActive' if use_equal_split_fallback else 'ProportionalToIndividualZoneArea'}"
    )

    # --- 13) Loop Through Zones: Calculate Zone Flows & Create IDF Objects ---
    for zone_obj_loopvar in zones_in_idf:
        # ... (zone loop logic remains the same, using final_infiltration_sched_name and final_ventilation_sched_name) ...
        zone_name_curr = zone_obj_loopvar.Name
        zone_info_curr = effective_zone_info_map.get(zone_name_curr)
        if not zone_info_curr: 
            print(f"[VENT CRITICAL ERROR] add_ventilation.py: Zone '{zone_name_curr}' not found in effective map. Skipping."); continue 
            
        zone_floor_area_curr_m2 = zone_info_curr.get('area', 0.0) 
        is_core_zone_curr = zone_info_curr.get('is_core', False)

        infiltration_for_this_zone_m3_s = 0.0
        ventilation_for_this_zone_m3_s = 0.0

        if is_core_zone_curr:
            infiltration_for_this_zone_m3_s = 0.0
        else: 
            if zone_floor_area_curr_m2 > 1e-6:
                infiltration_L_s = infiltration_rate_at_1Pa_L_s_per_m2_floor_area * zone_floor_area_curr_m2
                infiltration_for_this_zone_m3_s = infiltration_L_s / 1000.0
        
        if final_total_area_for_proportions > 1e-6 and zone_floor_area_curr_m2 >= 0:
            proportion = zone_floor_area_curr_m2 / final_total_area_for_proportions if final_total_area_for_proportions > 0 else 0
            ventilation_for_this_zone_m3_s = vent_flow_m3_s_total_building * proportion
        elif num_zones > 0 :
             ventilation_for_this_zone_m3_s = vent_flow_m3_s_total_building / num_zones
        
        fan_param_overrides = {}
        if fan_pressure_Pa is not None: fan_param_overrides["fan_pressure_override_Pa"] = fan_pressure_Pa
        if fan_total_efficiency is not None: fan_param_overrides["fan_efficiency_override"] = fan_total_efficiency
            
        iobj, vobj = create_ventilation_system(
            idf=idf,
            building_function=bldg_func,
            system_type=system_type,
            zone_name=zone_name_curr,
            infiltration_m3_s=infiltration_for_this_zone_m3_s,
            vent_flow_m3_s=ventilation_for_this_zone_m3_s,
            zone_floor_area_m2=zone_floor_area_curr_m2,
            infiltration_sched_name=final_infiltration_sched_name, # Use the dynamically determined name
            ventilation_sched_name=final_ventilation_sched_name,   # Use the dynamically determined name
            infiltration_model=infiltration_model,
            typical_delta_t=typical_delta_t,
            typical_wind=typical_wind,
            pick_strategy=parameter_pick_strategy, # Use consistent strategy
            dsoa_object_name=dsoa_object_name_global if system_type == "D" else None,
            hrv_sensible_effectiveness=hrv_sens_eff if system_type == "D" else 0.0,
            hrv_latent_effectiveness=hrv_lat_eff if system_type == "D" else 0.0,
            **fan_param_overrides,
        )

        if assigned_vent_log is not None and bldg_id in assigned_vent_log: 
            if "zones" not in assigned_vent_log[bldg_id]:
                assigned_vent_log[bldg_id]["zones"] = {}
            assigned_vent_log[bldg_id]["zones"][zone_name_curr] = {
                "infiltration_object_name": iobj.Name if iobj else "N/A", 
                "infiltration_object_type": iobj.key if iobj else "N/A",
                "infiltration_flow_m3_s_DESIGN_TOTAL_ZONE": infiltration_for_this_zone_m3_s,
                "infiltration_flow_m3_s_m2_DESIGN_ZONE": (infiltration_for_this_zone_m3_s / zone_floor_area_curr_m2) if zone_floor_area_curr_m2 > 1e-6 else 0.0,
                "infiltration_schedule_name": final_infiltration_sched_name,
                "ventilation_object_name": vobj.Name if vobj else "N/A",
                "ventilation_object_type": vobj.key if vobj else "N/A",
                "ventilation_flow_m3_s_DESIGN_TOTAL_ZONE": ventilation_for_this_zone_m3_s if vobj else 0.0,
                "ventilation_flow_m3_s_m2_DESIGN_ZONE": (ventilation_for_this_zone_m3_s / zone_floor_area_curr_m2) if vobj and zone_floor_area_curr_m2 > 1e-6 else 0.0,
                "ventilation_schedule_name": final_ventilation_sched_name,
                "zone_floor_area_m2_used_for_dist": zone_floor_area_curr_m2, 
                "is_core_zone": is_core_zone_curr
            }
        elif assigned_vent_log is not None: 
            print(f"[VENT WARNING] add_ventilation.py: Building ID {bldg_id} not in log for zone {zone_name_curr}")

    print(f"[VENTILATION] add_ventilation.py: Completed ventilation setup for Building {bldg_id}.")
------------------------------------------------------------

File: D:\Documents\E_Plus_2030_py\idf_objects\ventilation\assign_ventilation_values.py
============================================================
# ventilation/assign_ventilation_values.py

import random
import math # Import math for isnan check
from .ventilation_lookup import ventilation_lookup # Assuming ventilation_lookup is in the same directory

def find_vent_overrides(
    building_id,
    building_function,
    age_range,
    scenario,
    calibration_stage,
    user_config
):
    """
    Searches a user_config list/dict for any override entries matching the
    building_id, building_function, age_range, scenario, and calibration_stage.
    Returns a list of matching dict rows.
    """
    matches = []
    if not user_config: # Handle empty or None user_config
        return matches

    for row in user_config:
        if not isinstance(row, dict):
            continue

        if "building_id" in row and row["building_id"] != building_id:
            continue
        if "building_function" in row and row["building_function"] != building_function:
            continue
        if "age_range" in row and row["age_range"] != age_range:
            continue
        if "scenario" in row and row["scenario"] != scenario:
            continue
        if "calibration_stage" in row and row["calibration_stage"] != calibration_stage:
            continue
        matches.append(row)
    return matches


def pick_val_with_range(
    rng_tuple,
    strategy="A",
    log_dict=None,
    param_name=None
):
    """
    Selects a value from a (min_val, max_val) range based on the strategy.
    """
    chosen = 0.0
    min_v, max_v = 0.0, 0.0

    if rng_tuple is not None and len(rng_tuple) == 2:
        try:
            min_v_in, max_v_in = rng_tuple
            min_v = float(min_v_in)
            max_v = float(max_v_in)
            if math.isnan(min_v) or math.isnan(max_v):
                raise ValueError("NaN in range tuple")
        except (ValueError, TypeError, IndexError):
            print(f"[WARNING] assign_ventilation_values.py: Invalid range values in tuple {rng_tuple} for {param_name}. Defaulting to (0.0, 0.0).")
            min_v, max_v = 0.0, 0.0
    elif rng_tuple is not None:
        print(f"[WARNING] assign_ventilation_values.py: Invalid range tuple format: {rng_tuple} for {param_name}. Defaulting to (0.0, 0.0).")

    if min_v > max_v:
        print(f"[WARNING] assign_ventilation_values.py: Min value > Max value in range ({min_v}, {max_v}) for {param_name}. Using min value for both.")
        max_v = min_v

    if strategy == "A":
        chosen = (min_v + max_v) / 2.0
    elif strategy == "B":
        chosen = random.uniform(min_v, max_v)
    elif strategy == "C":
        chosen = min_v
    else:
        print(f"[WARNING] assign_ventilation_values.py: Unknown pick strategy '{strategy}' for {param_name}. Defaulting to Minimum.")
        chosen = min_v

    if log_dict is not None and param_name:
        log_dict[f"{param_name}_range"] = (min_v, max_v)
        log_dict[param_name] = chosen
    return chosen


def override_range(current_range, row_override_spec, param_name_for_warning="parameter"):
    """
    Applies overrides from a user config row to a parameter's (min, max) range.
    """
    if "fixed_value" in row_override_spec:
        val = row_override_spec["fixed_value"]
        try:
            f_val = float(val)
            if math.isnan(f_val):
                print(f"[WARNING] assign_ventilation_values.py: Override 'fixed_value' {val} is NaN for {param_name_for_warning}. Using current range {current_range}.")
                return current_range
            return (f_val, f_val)
        except (ValueError, TypeError): # If it's not a number, it's likely a string (e.g. schedule name)
            # For non-numeric fixed_value (like schedule names), this function isn't the right place.
            # That override is handled directly in the main assign_ventilation_params_with_overrides.
            # This function is for numeric ranges. So, if fixed_value is non-numeric, we don't change the range here.
            # Let the main logic handle string fixed_values.
            if not isinstance(val, (int,float)): # if it's truly a string override for a range param, that's an error
                 print(f"[ERROR] assign_ventilation_values.py: 'fixed_value' {val} for numeric range parameter {param_name_for_warning} is not a number. Using current range {current_range}.")
            return current_range

    elif "min_val" in row_override_spec and "max_val" in row_override_spec:
        try:
            min_ovr = float(row_override_spec["min_val"])
            max_ovr = float(row_override_spec["max_val"])
            if math.isnan(min_ovr) or math.isnan(max_ovr):
                raise ValueError("Override range contains NaN")
            if min_ovr > max_ovr:
                print(f"[WARNING] assign_ventilation_values.py: Override min_val > max_val for {param_name_for_warning}. Using min_val for both.")
                return (min_ovr, min_ovr)
            return (min_ovr, max_ovr)
        except (ValueError, TypeError):
            print(f"[WARNING] assign_ventilation_values.py: Could not convert override range ({row_override_spec.get('min_val')}, {row_override_spec.get('max_val')}) to floats for {param_name_for_warning}. Using current range {current_range}.")
            return current_range
    return current_range


def assign_ventilation_params_with_overrides(
    building_id=None,
    building_function="residential",
    age_range="2015 and later",
    scenario="scenario1",
    calibration_stage="pre_calibration",
    strategy="A",
    random_seed=None,
    user_config_vent=None,
    assigned_vent_log=None, # This argument is passed but not used within this function for logging. Logging happens in add_ventilation.py
    infiltration_key=None,  # This is the archetype_key
    year_key=None,
    is_residential=True,
    default_flow_exponent=0.67
):
    """
    Determines ventilation parameters, including archetype-specific schedule names,
    based on lookups and user overrides.
    """
    if random_seed is not None:
        random.seed(random_seed)

    # --- 1) Get Base Parameter Ranges from ventilation_lookup ---
    try:
        scenario_data = ventilation_lookup.get(scenario)
        if not scenario_data:
            print(f"[WARNING] assign_ventilation_values.py: Scenario '{scenario}' not found in ventilation_lookup. Defaulting to first available or 'scenario1'.")
            scenario = next(iter(ventilation_lookup)) if ventilation_lookup else "scenario1"
            scenario_data = ventilation_lookup.get(scenario, {})
        
        stage_dict = scenario_data.get(calibration_stage)
        if not stage_dict:
            print(f"[WARNING] assign_ventilation_values.py: Calibration stage '{calibration_stage}' not found for scenario '{scenario}'. Defaulting to first available or 'pre_calibration'.")
            calibration_stage = next(iter(scenario_data)) if scenario_data else "pre_calibration"
            stage_dict = scenario_data.get(calibration_stage, {})
    except Exception as e:
         raise ValueError(f"Error accessing ventilation_lookup for scenario='{scenario}', stage='{calibration_stage}'. Problem: {e}. Ensure lookup structure is correct.")

    # Initialize parameter ranges
    infiltration_base_rng = (0.0, 0.0)
    year_factor_rng = (1.0, 1.0)
    fan_pressure_rng = (0.0, 0.0)
    fan_total_efficiency_rng = (0.5, 0.7)
    f_ctrl_rng = (1.0, 1.0)
    hrv_sens_eff_rng = (0.0, 0.0)
    hrv_latent_eff_rng = (0.0, 0.0)

    # Determine system type (A/B/C/D) from ventilation_lookup
    system_type_final = "A" if is_residential else "D" # Default
    try:
        system_type_map_lookup = stage_dict.get("system_type_map", {})
        func_key_for_map = "residential" if is_residential else "non_residential"
        year_map = system_type_map_lookup.get(func_key_for_map, {}).get(year_key, {})
        system_type_final = year_map.get(infiltration_key, system_type_final) # infiltration_key is archetype
    except Exception as e:
         print(f"[WARNING] assign_ventilation_values.py: Error looking up system_type from map: {e}. Using default '{system_type_final}'.")

    # ---- NEW: Define Archetype-Specific Schedule Names ----
    # infiltration_key serves as the archetype identifier.
    # Clean the key for use in a schedule name.
    clean_archetype_key = infiltration_key.replace(" ", "_").replace("-", "") if infiltration_key else "default"
    
    # Default names, these will be used by add_ventilation.py to call schedule creation
    ventilation_sched_name = f"VentSched_{clean_archetype_key}"
    infiltration_sched_name = f"InfilSched_{clean_archetype_key}"
    # Fallback for infiltration often is AlwaysOn, but we make it archetype-specific by default.
    # User can override to "AlwaysOnSched" via user_config_vent if needed.
    # ---- END NEW Schedule Name Definition ----

    # Lookup other parameter ranges
    try:
        if is_residential:
            res_infil_lookup = stage_dict.get("residential_infiltration_range", {})
            infiltration_base_rng = res_infil_lookup.get(infiltration_key, (0.8, 1.2))
            sys_ctrl_ranges_lookup = stage_dict.get("system_control_range_res", {})
        else:
            nonres_infil_lookup = stage_dict.get("non_res_infiltration_range", {})
            infiltration_base_rng = nonres_infil_lookup.get(infiltration_key, (0.4, 0.6))
            sys_ctrl_ranges_lookup = stage_dict.get("system_control_range_nonres", {})
    except Exception as e:
         print(f"[WARNING] assign_ventilation_values.py: Error getting infiltration/control ranges from lookup: {e}. Using defaults.")
         sys_ctrl_ranges_lookup = {}

    year_factor_lookup = stage_dict.get("year_factor_range", {})
    year_factor_rng = year_factor_lookup.get(year_key, (1.0, 1.0))

    hrv_sens_eff_rng = stage_dict.get("hrv_sensible_eff_range", hrv_sens_eff_rng)
    hrv_latent_eff_rng = stage_dict.get("hrv_latent_eff_range", hrv_latent_eff_rng)
    fan_total_efficiency_rng = stage_dict.get("fan_total_efficiency_range", fan_total_efficiency_rng)
    # Example for fan_pressure_range if it were structured like others:
    # fan_pressure_data = stage_dict.get("fan_pressure_range", {})
    # fan_pressure_rng = fan_pressure_data.get("res_mech" if is_residential else "nonres_intake", fan_pressure_rng)


    if isinstance(sys_ctrl_ranges_lookup, dict):
        system_specific_ctrl_entry = sys_ctrl_ranges_lookup.get(system_type_final, {})
        if isinstance(system_specific_ctrl_entry, dict):
            f_ctrl_rng = system_specific_ctrl_entry.get("f_ctrl_range", f_ctrl_rng)
        else:
            print(f"[WARNING] assign_ventilation_values.py: Expected dict for sys_ctrl_ranges['{system_type_final}'], got {type(system_specific_ctrl_entry)}. Using default f_ctrl_range {f_ctrl_rng}.")
    else:
        print(f"[WARNING] assign_ventilation_values.py: System control ranges not found or invalid. Using default f_ctrl_range {f_ctrl_rng}.")

    # --- 2) Apply User Overrides from user_config_vent ---
    matches = find_vent_overrides(
        building_id or 0, building_function, age_range,
        scenario, calibration_stage, user_config_vent
    )

    for row in matches:
        pname = row.get("param_name", "")
        if pname == "infiltration_base":
            infiltration_base_rng = override_range(infiltration_base_rng, row, "infiltration_base")
        elif pname == "year_factor":
            year_factor_rng = override_range(year_factor_rng, row, "year_factor")
        elif pname == "system_type": # Overriding system_type
            if "fixed_value" in row:
                system_type_final = str(row["fixed_value"])
                # Re-lookup f_ctrl range based on potentially overridden system_type
                if isinstance(sys_ctrl_ranges_lookup, dict):
                    system_entry = sys_ctrl_ranges_lookup.get(system_type_final, {})
                    if isinstance(system_entry, dict):
                        f_ctrl_rng = system_entry.get("f_ctrl_range", (1.0, 1.0))
                    else: f_ctrl_rng = (1.0, 1.0)
                else: f_ctrl_rng = (1.0, 1.0)
        elif pname == "fan_pressure":
            fan_pressure_rng = override_range(fan_pressure_rng, row, "fan_pressure")
        elif pname == "fan_total_efficiency":
            fan_total_efficiency_rng = override_range(fan_total_efficiency_rng, row, "fan_total_efficiency")
        elif pname == "f_ctrl":
            f_ctrl_rng = override_range(f_ctrl_rng, row, "f_ctrl")
        elif pname == "hrv_eff":
            hrv_sens_eff_rng = override_range(hrv_sens_eff_rng, row, "hrv_eff")
        elif pname == "hrv_latent_eff":
            hrv_latent_eff_rng = override_range(hrv_latent_eff_rng, row, "hrv_latent_eff")
        # ---- Schedule Name Overrides ----
        elif pname == "infiltration_schedule_name":
            if "fixed_value" in row:
                infiltration_sched_name = str(row["fixed_value"]) # Direct string override
        elif pname == "ventilation_schedule_name":
            if "fixed_value" in row:
                ventilation_sched_name = str(row["fixed_value"]) # Direct string override

    # --- 3) Pick Final Values from Ranges ---
    local_log_for_params = {} # For storing picked values and their source ranges for parameters

    infiltration_base_val = pick_val_with_range(infiltration_base_rng, strategy, local_log_for_params, "infiltration_base_L_s_m2_10Pa")
    year_factor_val       = pick_val_with_range(year_factor_rng, strategy, local_log_for_params, "year_factor")
    fan_pressure_val      = pick_val_with_range(fan_pressure_rng, strategy, local_log_for_params, "fan_pressure")
    fan_total_efficiency_val = pick_val_with_range(fan_total_efficiency_rng, strategy, local_log_for_params, "fan_total_efficiency")
    f_ctrl_val            = pick_val_with_range(f_ctrl_rng, strategy, local_log_for_params, "f_ctrl")

    if system_type_final == "D":
        hrv_sens_eff_val = pick_val_with_range(hrv_sens_eff_rng, strategy, local_log_for_params, "hrv_eff")
        hrv_latent_eff_val = pick_val_with_range(hrv_latent_eff_rng, strategy, local_log_for_params, "hrv_lat_eff")
    else:
        hrv_sens_eff_val = 0.0
        hrv_latent_eff_val = 0.0
        local_log_for_params["hrv_eff_range"] = (0.0, 0.0)
        local_log_for_params["hrv_eff"] = 0.0
        local_log_for_params["hrv_lat_eff_range"] = (0.0, 0.0)
        local_log_for_params["hrv_lat_eff"] = 0.0

    # --- 4) Assemble Final Dictionary to be Returned ---
    # This dictionary includes the picked parameters and the determined schedule names.
    # The actual schedule objects are created later in add_ventilation.py.
    assigned_params = {
        "infiltration_base_L_s_m2_10Pa": local_log_for_params["infiltration_base_L_s_m2_10Pa"],
        "infiltration_base_L_s_m2_10Pa_range": local_log_for_params["infiltration_base_L_s_m2_10Pa_range"],
        "year_factor": local_log_for_params["year_factor"],
        "year_factor_range": local_log_for_params["year_factor_range"],
        "fan_pressure": local_log_for_params["fan_pressure"],
        "fan_pressure_range": local_log_for_params["fan_pressure_range"],
        "fan_total_efficiency": local_log_for_params["fan_total_efficiency"],
        "fan_total_efficiency_range": local_log_for_params["fan_total_efficiency_range"],
        "f_ctrl": local_log_for_params["f_ctrl"],
        "f_ctrl_range": local_log_for_params["f_ctrl_range"],
        "hrv_eff": local_log_for_params["hrv_eff"],
        "hrv_eff_range": local_log_for_params["hrv_eff_range"],
        "hrv_lat_eff": local_log_for_params["hrv_lat_eff"],
        "hrv_lat_eff_range": local_log_for_params["hrv_lat_eff_range"],
        "infiltration_schedule_name": infiltration_sched_name, # Final determined name
        "ventilation_schedule_name": ventilation_sched_name,   # Final determined name
        "system_type": system_type_final,
        "flow_exponent": default_flow_exponent,
        "strategy_letter": strategy # Pass the strategy letter for consistency if needed by other functions
    }
    
    # Logging of these 'assigned_params' happens in add_ventilation.py's 'assigned_vent_log'

    return assigned_params
------------------------------------------------------------

File: D:\Documents\E_Plus_2030_py\modification\main_modifi.py
============================================================
"""
main_modifi.py

Handles the generation of scenario-based IDFs for sensitivity, surrogate, 
calibration, or any parametric runs.

Workflow Outline:
  1) Loads "assigned"/"structured" CSV data for HVAC, DHW, Vent, Elec, Fenestration.
  2) Generates multiple scenario picks (random or other) -> scenario_params_*.csv
  3) Loads scenario CSVs, loops over scenario_index, applies them to IDFs
  4) Optionally runs simulations, post-processes, and does validation
"""

import os
import pandas as pd

# ---------------------------------------------------------------------------
# Local modules
# ---------------------------------------------------------------------------
from modification.common_utils import (
    load_assigned_csv,
    load_scenario_csv,
    load_idf,
    save_idf,
    generate_multiple_param_sets,
    save_param_scenarios_to_csv
)

# HVAC
from modification.hvac_functions import (
    create_hvac_scenarios,
    apply_building_level_hvac,
    apply_zone_level_hvac
)

# DHW
from modification.dhw_functions import apply_dhw_params_to_idf

# Elec
from modification.elec_functions import (
    create_elec_scenarios,
    apply_building_level_elec,
    apply_object_level_elec
)

# Equipment
from modification.equipment_functions import (
    create_equipment_scenarios,
    apply_building_level_equipment,
    apply_object_level_equipment,
)

# Fenestration
from modification.fenez_functions2 import (
    create_fenez_scenarios,
    apply_object_level_fenez
)

# Vent
from modification.vent_functions import (
    create_vent_scenarios,
    apply_building_level_vent,
    apply_zone_level_vent
)


def run_modification_workflow(config):
    """
    Main orchestration function that:
      - Loads assigned/structured CSV data for each system (HVAC, DHW, Vent, Elec, Fenez)
      - Creates multiple scenario picks
      - Applies them to a base IDF, generating scenario IDFs
      - Optionally runs simulations, post-process, and validation.
    """
    # -----------------------------------------------------------------------
    # 1) Extract from config
    # -----------------------------------------------------------------------
    base_idf_path   = config["base_idf_path"]
    idd_path        = config["idd_path"]
    assigned_csvs   = config["assigned_csv"]
    scenario_csvs   = config["scenario_csv"]
    building_id     = config["building_id"]
    num_scenarios   = config["num_scenarios"]
    picking_method  = config["picking_method"]
    scale_factor    = config.get("picking_scale_factor", 1.0)
    output_idf_dir  = config["output_idf_dir"]

    os.makedirs(output_idf_dir, exist_ok=True)

    # -----------------------------------------------------------------------
    # 2) HVAC CSV (either building+zones or single 'hvac')
    # -----------------------------------------------------------------------
    df_hvac_bld_sub = pd.DataFrame()
    df_hvac_zn_sub  = pd.DataFrame()
    has_hvac_data   = False

    if "hvac_building" in assigned_csvs and "hvac_zones" in assigned_csvs:
        path_bld = assigned_csvs["hvac_building"]
        path_zn  = assigned_csvs["hvac_zones"]

        df_hvac_bld_all = load_assigned_csv(path_bld)
        df_hvac_zn_all  = load_assigned_csv(path_zn)
        df_hvac_bld_sub = df_hvac_bld_all[df_hvac_bld_all["ogc_fid"] == building_id].copy()
        df_hvac_zn_sub  = df_hvac_zn_all[df_hvac_zn_all["ogc_fid"] == building_id].copy()
        has_hvac_data = True
    elif "hvac" in assigned_csvs:
        # single CSV
        path_hvac_single = assigned_csvs["hvac"]
        df_hvac_all = load_assigned_csv(path_hvac_single)
        df_hvac_bld_sub = df_hvac_all[df_hvac_all["ogc_fid"] == building_id].copy()
        has_hvac_data = True

    # -----------------------------------------------------------------------
    # 3) DHW
    # -----------------------------------------------------------------------
    df_dhw_all = load_assigned_csv(assigned_csvs["dhw"])
    df_dhw_sub = df_dhw_all[df_dhw_all["ogc_fid"] == building_id].copy()

    # -----------------------------------------------------------------------
    # 4) Vent (either building+zones or single 'vent')
    # -----------------------------------------------------------------------
    df_vent_bld_sub = pd.DataFrame()
    df_vent_zn_sub  = pd.DataFrame()
    has_vent_data   = False

    if "vent_building" in assigned_csvs and "vent_zones" in assigned_csvs:
        vent_bld_path = assigned_csvs["vent_building"]
        vent_zn_path  = assigned_csvs["vent_zones"]
        df_vent_bld_all = load_assigned_csv(vent_bld_path)
        df_vent_zn_all  = load_assigned_csv(vent_zn_path)
        df_vent_bld_sub = df_vent_bld_all[df_vent_bld_all["ogc_fid"] == building_id].copy()
        df_vent_zn_sub  = df_vent_zn_all[df_vent_zn_all["ogc_fid"] == building_id].copy()
        has_vent_data = True
    elif "vent" in assigned_csvs:
        vent_single_path = assigned_csvs["vent"]
        df_vent_all = load_assigned_csv(vent_single_path)
        df_vent_bld_sub = df_vent_all[df_vent_all["ogc_fid"] == building_id].copy()
        has_vent_data = True

    # -----------------------------------------------------------------------
    # 5) Elec
    # -----------------------------------------------------------------------
    df_elec_all = load_assigned_csv(assigned_csvs["elec"])
    df_elec_sub = df_elec_all[df_elec_all["ogc_fid"] == building_id].copy()

    # -----------------------------------------------------------------------
    # 6) Equipment
    # -----------------------------------------------------------------------
    df_equip_all = load_assigned_csv(assigned_csvs["equip"])
    df_equip_sub = df_equip_all[df_equip_all["ogc_fid"] == building_id].copy()

    # -----------------------------------------------------------------------
    # 7) Fenestration
    # -----------------------------------------------------------------------
    df_fenez_all = load_assigned_csv(assigned_csvs["fenez"])
    df_fenez_sub = df_fenez_all[df_fenez_all["ogc_fid"] == building_id].copy()

    # -----------------------------------------------------------------------
    # 7) Generate scenario picks for each system
    # -----------------------------------------------------------------------

    # 7A) HVAC
    if has_hvac_data and (not df_hvac_bld_sub.empty or not df_hvac_zn_sub.empty):
        from modification.hvac_functions import create_hvac_scenarios
        df_scen_hvac = create_hvac_scenarios(
            df_building=df_hvac_bld_sub,
            df_zones=df_hvac_zn_sub,
            building_id=building_id,
            num_scenarios=num_scenarios,
            picking_method=picking_method,
            random_seed=42,
            scenario_csv_out=scenario_csvs["hvac"]
        )
    else:
        df_scen_hvac = pd.DataFrame()
        if "hvac" in assigned_csvs and not df_hvac_bld_sub.empty:
            hvac_scenarios = generate_multiple_param_sets(
                df_main_sub=df_hvac_bld_sub,
                num_sets=num_scenarios,
                picking_method=picking_method,
                scale_factor=scale_factor
            )
            save_param_scenarios_to_csv(hvac_scenarios, building_id, scenario_csvs["hvac"])

    # 7B) DHW
    # If you prefer a param_min/param_max approach, import create_dhw_scenarios. 
    # Otherwise, fallback to generate_multiple_param_sets:
    from modification.dhw_functions import create_dhw_scenarios
    df_scen_dhw = create_dhw_scenarios(
        df_dhw_input=df_dhw_sub,
        building_id=building_id,
        num_scenarios=num_scenarios,
        picking_method=picking_method,
        random_seed=42,
        scenario_csv_out=scenario_csvs["dhw"]
    )

    # If older approach:
    # dhw_scenarios = generate_multiple_param_sets(df_main_sub=df_dhw_sub, ...)
    # save_param_scenarios_to_csv(dhw_scenarios, building_id, scenario_csvs["dhw"])

    # 7C) Vent
    if has_vent_data and (not df_vent_bld_sub.empty or not df_vent_zn_sub.empty):
        df_scen_vent = create_vent_scenarios(
            df_building=df_vent_bld_sub,
            df_zones=df_vent_zn_sub,
            building_id=building_id,
            num_scenarios=num_scenarios,
            picking_method=picking_method,
            random_seed=42,
            scenario_csv_out=scenario_csvs["vent"]
        )
    else:
        df_scen_vent = pd.DataFrame()

    # 7D) Elec => "create_elec_scenarios" for param_min/param_max approach
    df_scen_elec = create_elec_scenarios(
        df_lighting=df_elec_sub,
        building_id=building_id,
        num_scenarios=num_scenarios,
        picking_method=picking_method,
        random_seed=42,
        scenario_csv_out=scenario_csvs["elec"]
    )

    # 7E) Equipment
    df_scen_equip = create_equipment_scenarios(
        df_equipment=df_equip_sub,
        building_id=building_id,
        num_scenarios=num_scenarios,
        picking_method=picking_method,
        random_seed=42,
        scenario_csv_out=scenario_csvs["equip"]
    )

    # 7F) Fenestration
    df_scen_fenez = create_fenez_scenarios(
        df_struct_fenez=df_fenez_sub,
        building_id=building_id,
        num_scenarios=num_scenarios,
        picking_method=picking_method,
        random_seed=42,
        scenario_csv_out=scenario_csvs["fenez"]
    )

    # -----------------------------------------------------------------------
    # 8) Load scenario CSVs back, group by scenario_index
    # -----------------------------------------------------------------------
    df_hvac_scen  = load_scenario_csv(scenario_csvs["hvac"])  if os.path.isfile(scenario_csvs["hvac"])  else pd.DataFrame()
    df_dhw_scen   = load_scenario_csv(scenario_csvs["dhw"])   if os.path.isfile(scenario_csvs["dhw"])   else pd.DataFrame()
    df_vent_scen  = load_scenario_csv(scenario_csvs["vent"])  if os.path.isfile(scenario_csvs["vent"])  else pd.DataFrame()
    df_elec_scen  = load_scenario_csv(scenario_csvs["elec"])  if os.path.isfile(scenario_csvs["elec"])  else pd.DataFrame()
    df_equip_scen = load_scenario_csv(scenario_csvs["equip"]) if os.path.isfile(scenario_csvs["equip"]) else pd.DataFrame()
    df_fenez_scen = load_scenario_csv(scenario_csvs["fenez"]) if os.path.isfile(scenario_csvs["fenez"]) else pd.DataFrame()

    hvac_groups  = df_hvac_scen.groupby("scenario_index")  if not df_hvac_scen.empty  else None
    dhw_groups   = df_dhw_scen.groupby("scenario_index")   if not df_dhw_scen.empty   else None
    vent_groups  = df_vent_scen.groupby("scenario_index")  if not df_vent_scen.empty  else None
    elec_groups  = df_elec_scen.groupby("scenario_index")  if not df_elec_scen.empty  else None
    equip_groups = df_equip_scen.groupby("scenario_index") if not df_equip_scen.empty else None
    fenez_groups = df_fenez_scen.groupby("scenario_index") if not df_fenez_scen.empty else None

    # -----------------------------------------------------------------------
    # 9) For each scenario, load base IDF, apply parameters, save new IDF
    # -----------------------------------------------------------------------
    for i in range(num_scenarios):
        print(f"\n--- Creating scenario #{i} for building {building_id} ---")

        # 9A) Pull sub-DataFrames
        hvac_df = hvac_groups.get_group(i) if hvac_groups and i in hvac_groups.groups else pd.DataFrame()
        dhw_df  = dhw_groups.get_group(i)  if dhw_groups  and i in dhw_groups.groups  else pd.DataFrame()
        vent_df = vent_groups.get_group(i) if vent_groups and i in vent_groups.groups else pd.DataFrame()
        elec_df = elec_groups.get_group(i) if elec_groups and i in elec_groups.groups else pd.DataFrame()
        equip_df = equip_groups.get_group(i) if equip_groups and i in equip_groups.groups else pd.DataFrame()
        fenez_df= fenez_groups.get_group(i)if fenez_groups and i in fenez_groups.groups else pd.DataFrame()

        # 9B) For HVAC: building-level vs. zone-level
        hvac_bld_df  = hvac_df[hvac_df["zone_name"].isna()]   if not hvac_df.empty else pd.DataFrame()
        hvac_zone_df = hvac_df[hvac_df["zone_name"].notna()]  if not hvac_df.empty else pd.DataFrame()
        hvac_params  = _make_param_dict(hvac_bld_df)

        # 9C) Convert to param dict for DHW
        dhw_params = _make_param_dict(dhw_df)

        # 9D) Vent building vs. zone
        vent_bld_df  = vent_df[vent_df["zone_name"].isnull()] if not vent_df.empty else pd.DataFrame()
        vent_zone_df = vent_df[vent_df["zone_name"].notnull()]if not vent_df.empty else pd.DataFrame()
        vent_params  = _make_param_dict(vent_bld_df)

        # 9E) Elec => building-level approach or object-level
        elec_params = _make_param_dict(elec_df)

        # 9F) Equipment
        equip_params = _make_param_dict(equip_df)

        # 9G) Load base IDF
        idf = load_idf(base_idf_path, idd_path)

        # 9H) Apply building-level + zone-level HVAC
        apply_building_level_hvac(idf, hvac_params)
        apply_zone_level_hvac(idf, hvac_zone_df)

        # 9I) Apply DHW
        apply_dhw_params_to_idf(idf, dhw_params, suffix=f"Scenario_{i}")

        # 9J) Apply Vent
        if not vent_bld_df.empty or not vent_zone_df.empty:
            apply_building_level_vent(idf, vent_params)
            apply_zone_level_vent(idf, vent_zone_df)

        # 9K) Apply Elec => building-level approach
        #    Or if you prefer object-level, do apply_object_level_elec(idf, elec_df)
        if not elec_df.empty:
            apply_building_level_elec(idf, elec_params, zonelist_name="ALL_ZONES")

        # 9L) Apply Equipment
        if not equip_df.empty:
            apply_building_level_equipment(idf, equip_params, zonelist_name="ALL_ZONES")

        # 9M) Apply Fenestration (object-level)
        apply_object_level_fenez(idf, fenez_df)

        # 9N) Save scenario IDF
        scenario_idf_name = f"building_{building_id}_scenario_{i}.idf"
        scenario_idf_path = os.path.join(output_idf_dir, scenario_idf_name)
        save_idf(idf, scenario_idf_path)
        print(f"[INFO] Saved scenario IDF: {scenario_idf_path}")

    print("[INFO] All scenario IDFs generated successfully.")

    # -----------------------------------------------------------------------
    # 10) (Optional) Run Simulations
    # -----------------------------------------------------------------------
    if config.get("run_simulations", False):
        print("\n[INFO] Running simulations for scenario IDFs...")
        sim_cfg = config.get("simulation_config", {})
        # your simulate_all(...) or E+ runner here
        print("[INFO] Simulations complete (placeholder).")

    # -----------------------------------------------------------------------
    # 11) (Optional) Post-processing
    # -----------------------------------------------------------------------
    if config.get("perform_post_process", False):
        print("[INFO] Performing post-processing merges (placeholder).")
        ppcfg = config.get("post_process_config", {})
        # merge_all_results(...)
        print("[INFO] Post-processing step complete (placeholder).")

    # -----------------------------------------------------------------------
    # 12) (Optional) Validation
    # -----------------------------------------------------------------------
    if config.get("perform_validation", False):
        print("[INFO] Performing validation on scenario results (placeholder).")
        val_cfg = config["validation_config"]
        # run_validation_process(val_cfg)
        print("[INFO] Validation step complete (placeholder).")


def _make_param_dict(df_scenario):
    """
    Builds a dict {param_name: value} from a subset DataFrame, handling both
    'assigned_value' or 'param_value' columns in the scenario CSV.

    We check which column is present. If neither is found, we raise an error.
    """
    if df_scenario.empty:
        return {}

    possible_cols = list(df_scenario.columns)
    if "assigned_value" in possible_cols:
        val_col = "assigned_value"
    elif "param_value" in possible_cols:
        val_col = "param_value"
    else:
        raise AttributeError(
            "No 'assigned_value' or 'param_value' column found in scenario dataframe! "
            f"Columns are: {possible_cols}"
        )

    param_dict = {}
    for row in df_scenario.itertuples():
        p_name = row.param_name
        val    = getattr(row, val_col)
        # Attempt float
        try:
            param_dict[p_name] = float(val)
        except (ValueError, TypeError):
            param_dict[p_name] = val
    return param_dict

------------------------------------------------------------

File: D:\Documents\E_Plus_2030_py\modification\hvac_functions.py
============================================================
"""
hvac_functions.py

Provides functions for:
  1) create_hvac_scenarios(...)
     - Generates scenario data (potentially random) for building-level and zone-level HVAC,
       merging assigned_hvac_building.csv and assigned_hvac_zones.csv.
  2) apply_building_level_hvac(...)
     - Updates building-wide HVAC schedules (heating/cooling setpoints) and
       Ideal Loads supply air temps.
  3) apply_zone_level_hvac(...)
     - For each zone, applies or creates zone-level HVAC objects (like IdealLoads),
       sets setpoint schedules or thermostats, etc.

In this revised version, `_modify_schedule_compact()` is updated to parse 
existing "Until: HH:MM, value" lines and only update the numeric portion, 
preserving the schedule's time blocks. 
"""

import os
import random
import pandas as pd

from eppy.modeleditor import IDF  # or adapt as needed


##############################################################################
# 1) CREATE HVAC SCENARIOS
##############################################################################
def create_hvac_scenarios(
    df_building,
    df_zones,
    building_id,
    num_scenarios=5,
    picking_method="random_uniform",
    random_seed=42,
    scenario_csv_out=None
):
    """
    Generates a scenario-level DataFrame for HVAC from two CSVs:
      - df_building => assigned_hvac_building.csv
      - df_zones    => assigned_hvac_zones.csv

    Each of these has columns like:
      df_building: [ogc_fid, param_name, param_value] plus any param_name_range rows
      df_zones:    [ogc_fid, zone_name, param_name, param_value]

    We parse param_min / param_max from "xxx_range" lines for building-level, then
    produce a "long" DataFrame with columns:
      [scenario_index, ogc_fid, zone_name, param_name, param_value, param_min, param_max, picking_method]

    Example:
      heating_day_setpoint = 10.64 (with range (10.0, 11.0))
      => param_value might be a random pick in [10.0, 11.0] if picking_method=="random_uniform".

    Finally, we optionally save to scenario_csv_out (e.g. "scenario_params_hvac.csv").

    Return:
      df_scen (pd.DataFrame)
    """
    if random_seed is not None:
        random.seed(random_seed)

    # 1) Filter for this building
    df_bldg = df_building[df_building["ogc_fid"] == building_id].copy()
    df_zone = df_zones[df_zones["ogc_fid"] == building_id].copy()

    if df_bldg.empty and df_zone.empty:
        print(f"[create_hvac_scenarios] No HVAC data found for ogc_fid={building_id}.")
        return pd.DataFrame()

    # 2) Parse building-level HVAC params (with param_min/param_max)
    bldg_params = parse_building_hvac_params(df_bldg)

    # 3) Parse zone-level HVAC params (generally no param_min/param_max)
    zone_params = parse_zone_hvac_params(df_zone)

    scenario_rows = []

    # 4) For each scenario, pick new param_value for each building-level param
    for scenario_i in range(num_scenarios):
        # (A) Building-level
        for p in bldg_params:
            p_name = p["param_name"]
            base_val = p["param_value"]
            p_min = p["param_min"]
            p_max = p["param_max"]

            new_val = pick_value(base_val, p_min, p_max, picking_method)

            scenario_rows.append({
                "scenario_index": scenario_i,
                "ogc_fid": building_id,
                "zone_name": None,  # building-level
                "param_name": p_name,
                "param_value": new_val,
                "param_min": p_min,
                "param_max": p_max,
                "picking_method": picking_method
            })

        # (B) zone-level
        for z in zone_params:
            z_name  = z["zone_name"]
            p_name  = z["param_name"]
            base_val = z["param_value"]
            new_val = pick_value(base_val, None, None, picking_method)

            scenario_rows.append({
                "scenario_index": scenario_i,
                "ogc_fid": building_id,
                "zone_name": z_name,
                "param_name": p_name,
                "param_value": new_val,
                "param_min": None,
                "param_max": None,
                "picking_method": picking_method
            })

    # 5) Convert to DataFrame
    df_scen = pd.DataFrame(scenario_rows)

    # 6) Optionally write to CSV
    if scenario_csv_out:
        os.makedirs(os.path.dirname(scenario_csv_out), exist_ok=True)
        df_scen.to_csv(scenario_csv_out, index=False)
        print(f"[create_hvac_scenarios] Wrote scenario HVAC params => {scenario_csv_out}")

    return df_scen


def parse_building_hvac_params(df_bldg):
    """
    Helper to parse building-level HVAC parameters from assigned_hvac_building.csv
    into a list of dict with:
      [
        {
          "param_name": "heating_day_setpoint",
          "param_value": 10.64,
          "param_min": 10.0,
          "param_max": 11.0
        }, ...
      ]
    If we see "heating_day_setpoint_range" => store param_min/param_max in the dict
    for "heating_day_setpoint", etc.
    """
    param_map = {}  # "heating_day_setpoint" => {value: X, min: Y, max: Z}

    for row in df_bldg.itertuples():
        name = row.param_name
        # The CSV from the assignment step uses 'assigned_value'
        # rather than 'param_value'.  We read that column here so
        # the function works with the raw assigned CSV.
        # Support both raw assigned CSVs (assigned_value column)
        # and structured CSVs (param_value column)
        if hasattr(row, "assigned_value"):
            val = row.assigned_value
        else:
            val = row.param_value

        if name.endswith("_range"):
            base_name = name.replace("_range", "")
            if base_name not in param_map:
                param_map[base_name] = {"param_value": None, "param_min": None, "param_max": None}

            t = parse_tuple(val)
            if t and len(t) == 2:
                param_map[base_name]["param_min"] = t[0]
                param_map[base_name]["param_max"] = t[1]
        else:
            if name not in param_map:
                param_map[name] = {"param_value": None, "param_min": None, "param_max": None}
            param_map[name]["param_value"] = val

    # Convert to list
    result = []
    for p_name, dct in param_map.items():
        result.append({
            "param_name":  p_name,
            "param_value": dct["param_value"],
            "param_min":   dct["param_min"],
            "param_max":   dct["param_max"]
        })
    return result


def parse_zone_hvac_params(df_zone):
    """
    Helper for zone-level HVAC: typically no range columns, so param_min/param_max = None.
    Returns a list of dicts => 
      [
        {"zone_name": "Zone1", "param_name": "hvac_object_name", "param_value": "Zone1 Ideal Loads"},
        ...
      ]
    """
    results = []
    for row in df_zone.itertuples():
        zname = row.zone_name
        pname = row.param_name
        # The assigned HVAC CSV stores the picked values under
        # the column 'assigned_value'.  Use that column here so we
        # don't raise an AttributeError when parsing the raw file.
        if hasattr(row, "assigned_value"):
            val = row.assigned_value
        else:
            val = row.param_value

        results.append({
            "zone_name": zname,
            "param_name": pname,
            "param_value": val
        })
    return results


def parse_tuple(val):
    """
    If val is like "(10.0, 11.0)", parse to (10.0, 11.0). Otherwise None.
    """
    if not isinstance(val, str):
        return None
    val_str = val.strip()
    if not (val_str.startswith("(") and val_str.endswith(")")):
        return None
    try:
        inner = val_str[1:-1]
        parts = inner.split(",")
        if len(parts) != 2:
            return None
        p1 = float(parts[0])
        p2 = float(parts[1])
        return (p1, p2)
    except:
        return None


def pick_value(base_val, p_min, p_max, picking_method):
    """
    If picking_method=="random_uniform" and p_min/p_max are numeric,
    pick randomly in [p_min, p_max]. Otherwise keep base_val as is.
    """
    # Attempt float
    try:
        base_f = float(base_val)
    except:
        base_f = None

    if picking_method == "random_uniform" and p_min is not None and p_max is not None:
        try:
            fmin = float(p_min)
            fmax = float(p_max)
            if fmax >= fmin:
                return random.uniform(fmin, fmax)
        except:
            pass
        return base_val

    return base_val


##############################################################################
# 2) APPLY BUILDING-LEVEL HVAC
##############################################################################

def apply_building_level_hvac(idf, param_dict):
    """
    param_dict is a dictionary of building-level HVAC parameters, e.g.:
      {
        "heating_day_setpoint": 10.64,
        "heating_night_setpoint": 15.02,
        "cooling_day_setpoint": 25.6,
        "cooling_night_setpoint": 26.44,
        "max_heating_supply_air_temp": 52.36,
        "min_cooling_supply_air_temp": 13.35,
        ...
      }

    This function:
      1) Updates "ZONE HEATING SETPOINTS" schedule (if day/night keys exist).
      2) Updates "ZONE COOLING SETPOINTS" schedule (if day/night keys exist).
      3) For each ZONEHVAC:IDEALLOADSAIRSYSTEM, sets supply air temps if present.
    """

    # (1) Heating Setpoint Schedules
    if "heating_day_setpoint" in param_dict or "heating_night_setpoint" in param_dict:
        h_day = param_dict.get("heating_day_setpoint", 20.0)
        h_night = param_dict.get("heating_night_setpoint", 15.0)
        _modify_schedule_compact(
            idf,
            schedule_name="ZONE HEATING SETPOINTS",
            day_value=h_day,
            night_value=h_night,
            day_start="07:00",
            day_end="19:00"
        )

    # (2) Cooling Setpoint Schedules
    if "cooling_day_setpoint" in param_dict or "cooling_night_setpoint" in param_dict:
        c_day = param_dict.get("cooling_day_setpoint", 24.0)
        c_night = param_dict.get("cooling_night_setpoint", 27.0)
        _modify_schedule_compact(
            idf,
            schedule_name="ZONE COOLING SETPOINTS",
            day_value=c_day,
            night_value=c_night,
            day_start="07:00",
            day_end="19:00"
        )

    # (3) Ideal Loads Supply Temps
    max_heat = param_dict.get("max_heating_supply_air_temp", None)
    min_cool = param_dict.get("min_cooling_supply_air_temp", None)
    if (max_heat is not None) or (min_cool is not None):
        _set_ideal_loads_supply_temps_all_zones(
            idf,
            max_heating_temp=max_heat,
            min_cooling_temp=min_cool
        )


def _set_ideal_loads_supply_temps_all_zones(idf, max_heating_temp=None, min_cooling_temp=None):
    """
    Loops over all ZONEHVAC:IDEALLOADSAIRSYSTEM objects, sets:
      Maximum_Heating_Supply_Air_Temperature = max_heating_temp
      Minimum_Cooling_Supply_Air_Temperature = min_cooling_temp
    if provided.
    """
    if "ZONEHVAC:IDEALLOADSAIRSYSTEM" not in idf.idfobjects:
        return

    ideal_objs = idf.idfobjects["ZONEHVAC:IDEALLOADSAIRSYSTEM"]
    for ideal in ideal_objs:
        if max_heating_temp is not None:
            ideal.Maximum_Heating_Supply_Air_Temperature = max_heating_temp
        if min_cooling_temp is not None:
            ideal.Minimum_Cooling_Supply_Air_Temperature = min_cooling_temp

        print(f"[HVAC] Updated '{ideal.Name}' => MaxHeat={max_heating_temp}, MinCool={min_cooling_temp}")


##############################################################################
# PARTIAL SCHEDULE EDITING: UPDATED _modify_schedule_compact
##############################################################################

def parse_schedule_until_line(line_str: str):
    """
    Parses a single line like "Until: 07:00, 15.0".
    Returns (time_str, float_value).
    If parsing fails, returns (None, None).
    """
    if not isinstance(line_str, str):
        return (None, None)
    line_str = line_str.strip()
    if not line_str.lower().startswith("until:"):
        return (None, None)

    # Remove "Until:"
    try:
        remainder = line_str.split("Until:", 1)[1].strip()  # e.g. "07:00, 15.0"
        # Split on comma
        time_part, val_str = remainder.split(",", 1)
        time_str = time_part.strip()
        val_float = float(val_str.strip())
        return (time_str, val_float)
    except:
        return (None, None)


def _modify_schedule_compact(
    idf,
    schedule_name,
    day_value,
    night_value,
    day_start="07:00",
    day_end="19:00"
):
    """
    Partially modifies an existing SCHEDULE:COMPACT by parsing each 'Until:' field,
    preserving its time range, but swapping out the numeric value for day_value or
    night_value based on whether time < day_start, time < day_end, or beyond day_end.

    If the schedule does not exist, we log a warning and skip.

    NOTE: This is a simplistic approach to day vs. night assignment:
      - If the field's 'Until' time is < day_start => night_value
      - Else if < day_end => day_value
      - Else => night_value again

    That way we preserve however many time blocks the schedule had—only numeric values
    get replaced. If you want a different approach, adapt the logic below.
    """
    sched_obj = idf.getobject("SCHEDULE:COMPACT", schedule_name.upper())
    if not sched_obj:
        print(f"[WARN] schedule '{schedule_name}' not found; skipping.")
        return

    # We'll parse day_start/day_end into HH:MM integer comparisons for convenience
    def time_to_minutes(tstr):
        # "07:00" => 7*60 + 0 = 420
        parts = tstr.split(":")
        h = int(parts[0])
        m = int(parts[1]) if len(parts) > 1 else 0
        return h * 60 + m

    day_start_mins = time_to_minutes(day_start)
    day_end_mins   = time_to_minutes(day_end)

    # FieldValues is the raw list of all fields after the object name & type
    # Typically, Field_1 might be "Through: 12/31", Field_2 = "For: AllDays",
    # then subsequent fields are "Until: HH:MM, Value".
    for i in range(len(sched_obj.fieldvalues)):
        field_str = sched_obj.fieldvalues[i]

        # parse "Until:" lines
        time_str, old_val = parse_schedule_until_line(field_str)
        if time_str is None:
            # Not an "Until:" line or parse failed, skip
            continue

        # Convert time_str -> minutes
        mins = 9999
        try:
            mins = time_to_minutes(time_str)
        except:
            pass

        # Now pick day or night
        if mins < day_start_mins:
            new_val = night_value
        elif mins < day_end_mins:
            new_val = day_value
        else:
            new_val = night_value

        # Overwrite the field with the same time, new numeric
        sched_obj.fieldvalues[i] = f"Until: {time_str},{new_val:.2f}"

    print(f"[HVAC] Updated schedule '{schedule_name}' with day={day_value}, night={night_value}")


##############################################################################
# 3) APPLY ZONE-LEVEL HVAC
##############################################################################
def apply_zone_level_hvac(idf, df_zone_scen):
    """
    Accepts a DataFrame with columns [zone_name, param_name, param_value] 
    for each zone. E.g.:
      zone_name=Zone1_FrontPerimeter, param_name=hvac_object_name, 
        param_value=Zone1_FrontPerimeter Ideal Loads
      zone_name=Zone1_FrontPerimeter, param_name=heating_setpoint_schedule, 
        param_value=ZONE HEATING SETPOINTS
      ...

    Then you can create or update the zone's HVAC objects accordingly.
    """

    grouped = df_zone_scen.groupby("zone_name")

    for z_name, z_df in grouped:
        print(f"[HVAC] => Zone={z_name}, {len(z_df)} param rows")

        zone_params = {}
        for row in z_df.itertuples():
            pname = row.param_name
            pval  = row.param_value
            zone_params[pname] = pval

        hvac_obj_name  = zone_params.get("hvac_object_name")
        hvac_obj_type  = zone_params.get("hvac_object_type", "ZONEHVAC:IDEALLOADSAIRSYSTEM")

        # Example for schedules:
        heating_sched  = zone_params.get("heating_setpoint_schedule")
        cooling_sched  = zone_params.get("cooling_setpoint_schedule")

        # Create or find the zone hvac system
        if hvac_obj_name:
            hvac_obj = find_or_create_object(idf, hvac_obj_type, hvac_obj_name)
            if hasattr(hvac_obj, "Zone_Name"):
                hvac_obj.Zone_Name = z_name
            print(f"[HVAC] Created or found {hvac_obj_type} => '{hvac_obj_name}' for zone {z_name}")

        # If you want to manipulate thermostats or schedules:
        if heating_sched or cooling_sched:
            print(f"[HVAC] For zone={z_name}, link heating_sched={heating_sched}, cooling_sched={cooling_sched}")
            # Implement or skip

##############################################################################
# Utility: find/create an object
##############################################################################
def find_or_create_object(idf, obj_type_upper, obj_name):
    """
    Utility to find an existing object in IDF by type & name, or create a new one.
    E.g.: find_or_create_object(idf, "ZONEHVAC:IDEALLOADSAIRSYSTEM", "Zone1_Core Ideal Loads")
    """
    obj_type_upper = obj_type_upper.upper()
    if obj_type_upper not in idf.idfobjects:
        # If IDF doesn't have that object class, attempt creation
        new_obj = idf.newidfobject(obj_type_upper)
        new_obj.Name = obj_name
        return new_obj

    existing = [
        o for o in idf.idfobjects[obj_type_upper]
        if hasattr(o, "Name") and str(o.Name) == str(obj_name)
    ]
    if existing:
        return existing[0]
    else:
        new_obj = idf.newidfobject(obj_type_upper)
        new_obj.Name = obj_name
        return new_obj

------------------------------------------------------------

File: D:\Documents\E_Plus_2030_py\modification\vent_functions.py
============================================================
"""
vent_functions.py

Provides:
  1) create_vent_scenarios(...) 
     - Generates scenario-level data for both building-level and zone-level vent parameters,
       possibly incorporating min/max ranges for infiltration_base, infiltration_flow_m3_s, etc.
     - Writes or returns a single scenario DataFrame (or multiple, if you prefer them separate).

  2) apply_building_level_vent(idf, vent_params)
     - Applies building-level infiltration/vent parameters to the IDF.

  3) apply_zone_level_vent(idf, df_zone_scen)
     - Applies zone-level infiltration/vent parameters to the IDF by creating or updating
       ZoneInfiltration:DesignFlowRate and ZoneVentilation:DesignFlowRate objects.
"""

import os
import random
import pandas as pd

# For weather-dependent infiltration coefficients
from idf_objects.ventilation.create_ventilation_systems import (
    apply_weather_coefficients,
)


# ---------------------------------------------------------------------------
# 1) CREATE VENTILATION SCENARIOS
# ---------------------------------------------------------------------------
def create_vent_scenarios(
    df_building,
    df_zones,
    building_id,
    num_scenarios=5,
    picking_method="random_uniform",
    random_seed=42,
    scenario_csv_out=None
):
    """
    Generate a scenario-level DataFrame that combines building-level vent parameters
    (from df_building) and zone-level vent parameters (from df_zones), including
    param_min/param_max if present. Then optionally randomizes or adjusts them
    for each scenario.

    Args:
      df_building (pd.DataFrame): building-level vent data, typically from
        "assigned_vent_building.csv" with columns like:
         - ogc_fid, param_name, param_value
         - param_name might also have a "_range" companion row with min/max in parentheses
      df_zones (pd.DataFrame): zone-level vent data from
        "assigned_vent_zones.csv" with columns like:
         - ogc_fid, zone_name, param_name, param_value
      building_id: int or str, the building ID to filter on
      num_scenarios: int, how many scenario sets to create
      picking_method: str, e.g. "random_uniform", "fixed", etc.
      random_seed: int, for reproducible random picks
      scenario_csv_out: str or None, if not None we write final DataFrame to CSV

    Returns:
      df_scen (pd.DataFrame): A "long" DataFrame with columns like:
        [scenario_index, ogc_fid, zone_name, param_name, param_value,
         param_min, param_max, picking_method, ...]
      - zone_name may be empty/None for building-level params
      - param_min/param_max extracted from e.g. infiltration_base_range
      - param_value potentially randomized if picking_method == "random_uniform"
    """
    if random_seed is not None:
        random.seed(random_seed)

    # 1) Filter for this building
    df_bldg = df_building[df_building["ogc_fid"] == building_id].copy()
    df_zone = df_zones[df_zones["ogc_fid"] == building_id].copy()

    if df_bldg.empty and df_zone.empty:
        print(f"[create_vent_scenarios] No ventilation data found for ogc_fid={building_id}.")
        return pd.DataFrame()

    # 2) Build building param list, with param_min/param_max if they appear in "xxx_range" rows
    bldg_params = parse_building_vent_params(df_bldg)

    # 3) Build zone param list
    zone_params = parse_zone_vent_params(df_zone)

    scenario_rows = []

    # 4) For each scenario, we pick new param_value for each building-level param
    for scenario_i in range(num_scenarios):
        # A) Building-level
        for p in bldg_params:
            p_name = p["param_name"]
            p_val  = p["param_value"]  # base
            p_min  = p["param_min"]
            p_max  = p["param_max"]

            new_val = pick_value(p_val, p_min, p_max, picking_method)
            scenario_rows.append({
                "scenario_index": scenario_i,
                "ogc_fid": building_id,
                "zone_name": None,  # building-level param
                "param_name": p_name,
                "param_value": new_val,
                "param_min": p_min,
                "param_max": p_max,
                "picking_method": picking_method
            })

        # B) Zone-level
        for z in zone_params:
            z_name  = z["zone_name"]
            p_name  = z["param_name"]
            p_val   = z["param_value"]
            p_min   = z["param_min"]
            p_max   = z["param_max"]

            new_val = pick_value(p_val, p_min, p_max, picking_method)
            scenario_rows.append({
                "scenario_index": scenario_i,
                "ogc_fid": building_id,
                "zone_name": z_name,
                "param_name": p_name,
                "param_value": new_val,
                "param_min": p_min,
                "param_max": p_max,
                "picking_method": picking_method
            })

    # 5) Convert to DataFrame
    df_scen = pd.DataFrame(scenario_rows)

    # 6) Optionally write to CSV
    if scenario_csv_out:
        os.makedirs(os.path.dirname(scenario_csv_out), exist_ok=True)
        df_scen.to_csv(scenario_csv_out, index=False)
        print(f"[create_vent_scenarios] Wrote scenario vent params => {scenario_csv_out}")

    return df_scen


def parse_building_vent_params(df_bldg):
    """
    Helper to parse building-level vent parameters from
    assigned_vent_building.csv into a list of dicts with
      [param_name, param_value, param_min, param_max]

    - If param_name == "infiltration_base_range" => store param_min/param_max from the tuple
      matched with param_name="infiltration_base".
    - Similarly for "year_factor_range", "fan_pressure_range", etc.
    """
    # We read rows like:
    #   infiltration_base, 100.3639
    #   infiltration_base_range, (100.3, 100.4)
    # We'll store them in a dictionary keyed by the "base param"
    param_map = {}  # e.g. "infiltration_base" => {value:..., min:..., max:...}

    for row in df_bldg.itertuples():
        name = row.param_name
        val  = row.param_value

        # e.g. name="infiltration_base_range"
        if name.endswith("_range"):
            base_name = name.replace("_range", "")
            if base_name not in param_map:
                param_map[base_name] = {"param_value": None, "param_min": None, "param_max": None}

            # parse (min,max)
            t = parse_tuple(val)
            if t and len(t) == 2:
                param_map[base_name]["param_min"] = t[0]
                param_map[base_name]["param_max"] = t[1]
        else:
            # normal param
            if name not in param_map:
                param_map[name] = {"param_value": None, "param_min": None, "param_max": None}
            param_map[name]["param_value"] = val

    # Now produce a list of dict
    result = []
    for p_name, dct in param_map.items():
        result.append({
            "param_name":  p_name,
            "param_value": dct["param_value"],
            "param_min":   dct["param_min"],
            "param_max":   dct["param_max"]
        })
    return result


def parse_zone_vent_params(df_zone):
    """
    Helper to parse zone-level vent params from assigned_vent_zones.csv
    into a list of dicts with zone_name, param_name, param_value, param_min, param_max.

    Typically, assigned_vent_zones doesn't store param_min/param_max,
    so we might keep them as None. But if your code logs them, parse them similarly.
    """
    results = []
    # e.g. row: zone_name="Zone1_FrontPerimeter", param_name="infiltration_flow_m3_s", param_value=0.255
    for row in df_zone.itertuples():
        zname = row.zone_name
        pname = row.param_name
        val   = row.param_value

        # If you store param ranges in zone CSV, parse them similarly to parse_tuple
        # For now, we assume no range => None
        results.append({
            "zone_name": zname,
            "param_name": pname,
            "param_value": val,
            "param_min": None,
            "param_max": None
        })
    return results


def parse_tuple(val):
    """
    If val is like "(100.3, 100.4)", parse to (100.3, 100.4). Otherwise None.
    """
    if not isinstance(val, str):
        return None
    val_str = val.strip()
    if not (val_str.startswith("(") and val_str.endswith(")")):
        return None
    try:
        # e.g. ast.literal_eval could do this too, but let's do a simple approach:
        inner = val_str[1:-1]  # remove parens
        parts = inner.split(",")
        if len(parts) != 2:
            return None
        p1 = float(parts[0])
        p2 = float(parts[1])
        return (p1, p2)
    except:
        return None


def pick_value(base_val, p_min, p_max, picking_method):
    """
    Given a base_val (float or str), optional p_min/p_max, and a method,
    return a new value. Example:
      - If picking_method == "random_uniform" and p_min/p_max are not None,
        pick random in [p_min, p_max].
      - Else keep base_val as is.

    Adjust as needed for more complex logic (like scale factors).
    """
    # Attempt float conversion
    try:
        base_f = float(base_val)
    except:
        base_f = None

    if picking_method == "random_uniform" and p_min is not None and p_max is not None:
        try:
            fmin = float(p_min)
            fmax = float(p_max)
            if fmax >= fmin:
                return random.uniform(fmin, fmax)
        except:
            pass
        # fallback => base_val
        return base_val

    # if picking_method == "fixed", or no range, just return base_val
    return base_val


# ---------------------------------------------------------------------------
# 2) APPLY BUILDING-LEVEL VENT
# ---------------------------------------------------------------------------
def apply_building_level_vent(idf, vent_params):
    """
    Applies building-level infiltration/vent parameters (like
    ``infiltration_base_L_s_m2_10Pa`` and ``year_factor`` together with
    schedule names) to the IDF in a "coarse" manner.

    Example usage:
        vent_params = {
            "infiltration_base_L_s_m2_10Pa": 0.5,
            "year_factor": 1.0,
            "infiltration_schedule_name": "AlwaysOnSched",
            "ventilation_schedule_name": "VentSched",
            ...
        }
        apply_building_level_vent(my_idf, vent_params)
    """
    # This is a placeholder approach, as building-level infiltration often
    # needs to be distributed to zones. But let's assume you have a
    # "global infiltration" or "HVAC system infiltration" object in IDF
    # that you can set.

    infiltration_base = vent_params.get("infiltration_base_L_s_m2_10Pa")
    year_factor = vent_params.get("year_factor")
    infiltration_sched = vent_params.get("infiltration_schedule_name")
    ventilation_sched = vent_params.get("ventilation_schedule_name")
    # etc.

    print(
        f"[VENT] Applying building-level infiltration_base_L_s_m2_10Pa={infiltration_base}, year_factor={year_factor},"
        f" infiltration_sched={infiltration_sched}, ventilation_sched={ventilation_sched}"
    )

    # If you have a top-level infiltration object or design object, you can find it:
    # e.g. "ZoneInfiltration:DesignFlowRate" object named "GlobalInfil" or similar
    # This is just a pseudo-illustration:
    # infiltration_obj = find_or_create_infiltration_object(idf, name="GlobalInfil")
    # infiltration_obj.Design_Flow_Rate = infiltration_base
    # infiltration_obj.Schedule_Name = infiltration_sched

    # If you want to store infiltration_total_m3_s, etc. do similarly
    # ...
    pass


# ---------------------------------------------------------------------------
# 3) APPLY ZONE-LEVEL VENT
# ---------------------------------------------------------------------------
def apply_zone_level_vent(idf, df_zone_scen):
    """
    Applies zone-level infiltration/vent parameters to each zone. The DataFrame
    is expected to have columns:
       [zone_name, param_name, param_value, ...]
    derived from scenario-based picks or from assigned_vent_zones.csv.

    Each zone might have ``infiltration_object_name`` and associated values such
    as ``infiltration_base_L_s_m2_10Pa`` or ``infiltration_flow_m3_s`` along with
    ``year_factor``.  Schedule names (``infiltration_schedule_name`` and
    ``ventilation_schedule_name``) can also be provided.

    We'll group by zone_name, create or update the infiltration/vent objects
    in IDF.
    """
    # group by zone_name
    grouped = df_zone_scen.groupby("zone_name")

    for z_name, z_df in grouped:
        print(f"[VENT] => Zone={z_name}, {len(z_df)} param rows")

        # We can parse infiltrationX / ventilationX from param_name => param_value
        # A simple approach is to build a dict
        z_params = {}
        for row in z_df.itertuples():
            pname = row.param_name
            pval  = row.param_value
            z_params[pname] = pval

        # infiltration
        infil_obj_name  = z_params.get("infiltration_object_name")
        infil_obj_type  = z_params.get(
            "infiltration_object_type", "ZONEINFILTRATION:DESIGNFLOWRATE"
        )
        infil_base      = z_params.get("infiltration_base_L_s_m2_10Pa")
        year_factor     = z_params.get("year_factor")
        infil_flow      = z_params.get("infiltration_flow_m3_s", 0.0)
        infil_schedule  = z_params.get("infiltration_schedule_name", "AlwaysOnSched")
        infil_model     = z_params.get("infiltration_model", "constant")
        typical_delta_t = float(z_params.get("typical_delta_t", 10.0))
        typical_wind    = float(z_params.get("typical_wind", 3.0))
        zone_area       = z_params.get("zone_floor_area_m2_used_for_dist")
        if zone_area is None:
            zone_area = z_params.get("zone_floor_area_m2")

        print(
            f"    Infiltration params: base_L_s_m2_10Pa={infil_base}, year_factor={year_factor}, flow={infil_flow}, schedule={infil_schedule}"
        )

        # find or create infiltration object
        if infil_obj_name:
            infil_obj = find_or_create_object(idf, infil_obj_type, infil_obj_name)
            # set infiltration fields (some fields vary by object type)
            if hasattr(infil_obj, "Name"):
                infil_obj.Name = infil_obj_name
            if hasattr(infil_obj, "Zone_or_ZoneList_Name"):
                infil_obj.Zone_or_ZoneList_Name = z_name
            if hasattr(infil_obj, "Design_Flow_Rate_Calculation_Method"):
                infil_obj.Design_Flow_Rate_Calculation_Method = "Flow/Area"

            flow_per_area = 0.0
            try:
                flow_val = float(infil_flow)
                if zone_area and float(zone_area) > 0:
                    flow_per_area = flow_val / float(zone_area)
            except Exception:
                pass

            if hasattr(infil_obj, "Schedule_Name"):
                infil_obj.Schedule_Name = infil_schedule

            if infil_model.lower() == "weather":
                apply_weather_coefficients(
                    infil_obj,
                    max(0.0, flow_per_area),
                    typical_delta_t=typical_delta_t,
                    typical_wind=typical_wind,
                )
            else:
                if hasattr(infil_obj, "Design_Flow_Rate"):
                    infil_obj.Design_Flow_Rate = max(0.0, flow_per_area)
                if hasattr(infil_obj, "Constant_Term_Coefficient"):
                    infil_obj.Constant_Term_Coefficient = 1.0
                if hasattr(infil_obj, "Temperature_Term_Coefficient"):
                    infil_obj.Temperature_Term_Coefficient = 0.0
                if hasattr(infil_obj, "Velocity_Term_Coefficient"):
                    infil_obj.Velocity_Term_Coefficient = 0.0
                if hasattr(infil_obj, "Velocity_Squared_Term_Coefficient"):
                    infil_obj.Velocity_Squared_Term_Coefficient = 0.0

        # ventilation
        vent_obj_name   = z_params.get("ventilation_object_name")
        vent_obj_type   = z_params.get(
            "ventilation_object_type", "ZONEVENTILATION:DESIGNFLOWRATE"
        )
        vent_flow       = z_params.get("ventilation_flow_m3_s", 0.0)
        vent_schedule   = z_params.get("ventilation_schedule_name", "AlwaysOnSched")

        print(
            f"    Ventilation params: flow={vent_flow}, schedule={vent_schedule}"
        )

        if vent_obj_name:
            vent_obj = find_or_create_object(idf, vent_obj_type, vent_obj_name)
            if hasattr(vent_obj, "Name"):
                vent_obj.Name = vent_obj_name
            if hasattr(vent_obj, "Zone_or_ZoneList_Name"):
                vent_obj.Zone_or_ZoneList_Name = z_name
            if hasattr(vent_obj, "Design_Flow_Rate_Calculation_Method"):
                vent_obj.Design_Flow_Rate_Calculation_Method = "Flow/Area"

            flow_per_area_v = 0.0
            try:
                vflow = float(vent_flow)
                if zone_area and float(zone_area) > 0:
                    flow_per_area_v = vflow / float(zone_area)
            except Exception:
                pass
            if hasattr(vent_obj, "Design_Flow_Rate"):
                vent_obj.Design_Flow_Rate = max(0.0, flow_per_area_v)
            if hasattr(vent_obj, "Schedule_Name"):
                vent_obj.Schedule_Name = vent_schedule


def find_or_create_object(idf, obj_type_upper, obj_name):
    """
    Utility to find an existing object in IDF by type & name, or create a new one.
    e.g. find_or_create_object(idf, "ZONEINFILTRATION:DESIGNFLOWRATE", "Infil_Zone1")
    """
    if not obj_type_upper:
        return None
    if obj_type_upper not in idf.idfobjects:
        # If IDF doesn't have that object class, attempt creation
        new_obj = idf.newidfobject(obj_type_upper)
        return new_obj

    # try to find by name
    existing = [
        o for o in idf.idfobjects[obj_type_upper]
        if hasattr(o, "Name") and str(o.Name) == str(obj_name)
    ]
    if existing:
        return existing[0]
    else:
        # create new
        new_obj = idf.newidfobject(obj_type_upper)
        return new_obj

------------------------------------------------------------

File: D:\Documents\E_Plus_2030_py\modification\common_utils.py
============================================================
# common_utils.py

import os
import random
import pandas as pd

# =============================================================================
# 1) CHOOSE EITHER EPPY OR GEOMEPPY:
# -----------------------------------------------------------------------------
# If using Eppy, uncomment these and comment out the Geomeppy lines:
# from eppy.modeleditor import IDF

# If using Geomeppy (to allow set_wwr, getsurfaces, etc.), uncomment these:
from geomeppy import IDF as GeomIDF

# =============================================================================
# 2) "Assigned" CSV Loading
# -----------------------------------------------------------------------------
def load_assigned_csv(csv_path):
    """
    Loads a generic CSV file containing assigned parameters for a building or zone.
    For example:
      D:/Documents/E_Plus_2030_py/output/assigned/assigned_dhw_params.csv
      D:/Documents/E_Plus_2030_py/output/assigned/assigned_hvac_building.csv
      etc.

    Returns:
      A Pandas DataFrame with the file contents.
    """
    if not os.path.isfile(csv_path):
        raise FileNotFoundError(f"Cannot find CSV at: {csv_path}")
    df = pd.read_csv(csv_path)
    return df


def filter_for_building(df_main, df_zone=None, building_id=None):
    """
    If 'df_zone' is provided, filters both dataframes by ogc_fid == building_id.
    If building_id is None, returns df_main (and df_zone) unfiltered.

    This is useful if you have a building-level CSV and a zone-level CSV
    and want to isolate data for a particular ogc_fid.
    """
    if building_id is not None:
        df_main_sub = df_main[df_main["ogc_fid"] == building_id].copy()
        if df_zone is not None:
            df_zone_sub = df_zone[df_zone["ogc_fid"] == building_id].copy()
        else:
            df_zone_sub = None
    else:
        df_main_sub = df_main.copy()
        df_zone_sub = df_zone.copy() if df_zone is not None else None

    return df_main_sub, df_zone_sub


# =============================================================================
# 3) Helpers for Generating Scenario Parameter Sets
# -----------------------------------------------------------------------------
def to_float_or_none(x):
    """
    Attempts to convert x to float. If it fails (or is NaN), returns None.
    """
    try:
        return float(x)
    except (ValueError, TypeError):
        return None


def pick_value_in_range(base_val, param_min, param_max,
                        method="random_uniform", scale_factor=0.5):
    """
    Picks a new value given:
      - base_val: original numeric value (fallback if range is invalid)
      - param_min, param_max: numeric range
      - method: 
         "random_uniform" => uniform in [param_min, param_max]
         "scale_around_base" => base_val * random(1 - scale_factor, 1 + scale_factor)
         "offset_half" => base_val +/- up to 50% of half the total range
      - scale_factor: used if method="scale_around_base"

    Returns a float. If range invalid, returns base_val.
    """
    base_val_f = to_float_or_none(base_val)
    if base_val_f is None:
        base_val_f = 0.0

    min_f = to_float_or_none(param_min)
    max_f = to_float_or_none(param_max)

    if method == "random_uniform":
        if min_f is not None and max_f is not None and min_f < max_f:
            return random.uniform(min_f, max_f)
        else:
            return base_val_f

    elif method == "scale_around_base":
        low_factor = 1.0 - scale_factor
        high_factor = 1.0 + scale_factor
        factor = random.uniform(low_factor, high_factor)
        return base_val_f * factor

    elif method == "offset_half":
        if min_f is not None and max_f is not None:
            half_span = (max_f - min_f) / 2.0 * 0.5
            offset = random.uniform(-half_span, half_span)
            return base_val_f + offset
        else:
            return base_val_f

    # Default => return base_val
    return base_val_f


# If some params are always strings (like schedule names), define them here:
STRING_PARAMS = [
    "system_type",
    "fan_control_mode",
    "schedule_name",
    "ventilation_schedule_name",
    "infiltration_schedule_name",
    # Add more as needed...
]


def define_building_param_strategy(df_main_sub,
                                   picking_method="random_uniform",
                                   scale_factor=0.5):
    """
    Loops over rows in df_main_sub to build {param_name -> new_value}.
    For each row, we call pick_value_in_range(...) only if param_name is numeric.

    If param_name is in STRING_PARAMS, we do NOT randomly pick numeric ranges.
    Instead, we keep the original value as str.
    """
    final_param_dict = {}

    for idx, row in df_main_sub.iterrows():
        param_name = row.get("param_name", None)
        if not param_name:
            continue

        base_val = row.get("param_value", None)  # or row.get("assigned_value", None)
        p_min = row.get("param_min", None)
        p_max = row.get("param_max", None)

        # 1) If param_name is in STRING_PARAMS => keep as string
        if param_name.lower() in STRING_PARAMS:
            final_param_dict[param_name] = str(base_val)
            continue

        # 2) If param_name is numeric => pick from range
        new_val = pick_value_in_range(
            base_val=base_val,
            param_min=p_min,
            param_max=p_max,
            method=picking_method,
            scale_factor=scale_factor
        )
        final_param_dict[param_name] = new_val

    return final_param_dict


def generate_multiple_param_sets(df_main_sub, num_sets=5,
                                 picking_method="random_uniform",
                                 scale_factor=0.5):
    """
    Calls define_building_param_strategy(...) multiple times to create 
    'num_sets' scenario dicts, e.g. for random draws in [param_min, param_max].

    Returns: list of dicts => each dict is {param_name -> new_value}
    """
    all_scenarios = []
    for _ in range(num_sets):
        scenario = define_building_param_strategy(
            df_main_sub=df_main_sub,
            picking_method=picking_method,
            scale_factor=scale_factor
        )
        all_scenarios.append(scenario)
    return all_scenarios


def save_param_scenarios_to_csv(all_scenarios, building_id,
                                out_csv="scenario_params.csv"):
    """
    Writes each scenario's picks to CSV with columns:
      [scenario_index, ogc_fid, param_name, assigned_value]

    This is how we form the "scenario_index" concept for grouping later.
    """
    rows = []
    for i, scenario_dict in enumerate(all_scenarios):
        for p_name, val in scenario_dict.items():
            rows.append({
                "scenario_index": i,
                "ogc_fid": building_id,
                "param_name": p_name,
                "assigned_value": val
            })

    df_out = pd.DataFrame(rows)
    os.makedirs(os.path.dirname(out_csv), exist_ok=True)
    df_out.to_csv(out_csv, index=False)
    print(f"[INFO] Saved scenario picks => {out_csv}")


# =============================================================================
# 4) IDF Load/Save with Geomeppy
# -----------------------------------------------------------------------------
def load_idf(base_idf_path, idd_path):
    """
    Loads an existing IDF file from disk using Geomeppy (or Eppy, if desired).
    Adjust path as needed.
    """
    if not os.path.isfile(idd_path):
        raise FileNotFoundError(f"IDD file not found at: {idd_path}")
    if not os.path.isfile(base_idf_path):
        raise FileNotFoundError(f"IDF file not found at: {base_idf_path}")

    # With Geomeppy:
    GeomIDF.setiddname(idd_path)
    idf = GeomIDF(base_idf_path)
    return idf


def save_idf(idf, out_path):
    """
    Saves the modified IDF to out_path, creating directories as needed.
    """
    os.makedirs(os.path.dirname(out_path), exist_ok=True)
    idf.saveas(out_path)
    print(f"[INFO] Saved modified IDF => {out_path}")


# =============================================================================
# 5) Loading a "Scenario" CSV (already-defined picks)
# -----------------------------------------------------------------------------
def load_scenario_csv(scenario_csv):
    """
    Reads a CSV that presumably has columns:
      - scenario_index
      - ogc_fid
      - param_name
      - assigned_value
    or something similar.

    The caller can then do: df.groupby("scenario_index") to iterate over scenarios.
    """
    if not os.path.isfile(scenario_csv):
        raise FileNotFoundError(f"Cannot find scenario CSV at: {scenario_csv}")
    df = pd.read_csv(scenario_csv)
    return df

------------------------------------------------------------

File: D:\Documents\E_Plus_2030_py\idf_objects\structuring\flatten_assigned_vent.py
============================================================
"""
flatten_assigned_vent.py

Transforms the "assigned_ventilation.csv" file (which has [ogc_fid, param_name, assigned_value])
into two structured CSVs:
  1) assigned_vent_building.csv
  2) assigned_vent_zones.csv

Usage:
    python flatten_assigned_vent.py
or
    from idf_objects.structuring.flatten_assigned_vent import main
    main()
"""

import pandas as pd
import ast
import os

def parse_assigned_value(value_str):
    """
    Safely convert the string in 'assigned_value' into a Python dict,
    e.g. literal_eval("{'infiltration_base': 1.23}")
    """
    try:
        return ast.literal_eval(str(value_str))
    except (SyntaxError, ValueError):
        return {}
    

    
def flatten_ventilation_data(df_input, out_build_csv, out_zone_csv):
    """
    UPDATED: Takes a pre-flattened DataFrame and splits it into two files.
    - Rows where 'zone_name' is NaN/None are considered building-level.
    - Rows with a 'zone_name' are considered zone-level.

    :param df_input: pd.DataFrame
        Must contain columns => "ogc_fid", "zone_name", "param_name", "assigned_value"
    :param out_build_csv: str
        File path for building-level CSV output.
    :param out_zone_csv: str
        File path for zone-level CSV output.
    """
    # Ensure the 'zone_name' column exists, even if all values are NaN
    if 'zone_name' not in df_input.columns:
        df_input['zone_name'] = pd.NA

    # 1. Building-level data is where zone_name is null/NaN
    df_build = df_input[df_input['zone_name'].isnull()].copy()

    # 2. Zone-level data is where zone_name has a value
    df_zone = df_input[df_input['zone_name'].notnull()].copy()
    
    # Rename 'assigned_value' to 'param_value' for consistency with downstream scripts
    if 'assigned_value' in df_build.columns:
        df_build.rename(columns={'assigned_value': 'param_value'}, inplace=True)

    if 'assigned_value' in df_zone.columns:
        df_zone.rename(columns={'assigned_value': 'param_value'}, inplace=True)

    # Select and ensure columns exist, even if DataFrame is empty
    build_cols = ["ogc_fid", "param_name", "param_value"]
    zone_cols = ["ogc_fid", "zone_name", "param_name", "param_value"]

    df_build = df_build.reindex(columns=build_cols)
    df_zone = df_zone.reindex(columns=zone_cols)


    # Write them to CSV
    os.makedirs(os.path.dirname(out_build_csv), exist_ok=True)
    df_build.to_csv(out_build_csv, index=False)

    os.makedirs(os.path.dirname(out_zone_csv), exist_ok=True)
    df_zone.to_csv(out_zone_csv, index=False)

    print(f"[INFO] Wrote building-level ventilation picks to {out_build_csv} ({len(df_build)} rows).")
    print(f"[INFO] Wrote zone-level ventilation picks to {out_zone_csv} ({len(df_zone)} rows).")


def main():
    """
    Example CLI entry point.
    """
    csv_in = r"D:\Documents\E_Plus_2030_py\output\assigned\assigned_ventilation.csv"
    csv_build_out = r"D:\Documents\E_Plus_2030_py\output\assigned\assigned_vent_building.csv"
    csv_zone_out = r"D:\Documents\E_Plus_2030_py\output\assigned\assigned_vent_zones.csv"

    if not os.path.exists(csv_in):
        print(f"Error: Input file not found at {csv_in}")
        return
        
    df_assigned = pd.read_csv(csv_in)

    flatten_ventilation_data(
        df_input=df_assigned,
        out_build_csv=csv_build_out,
        out_zone_csv=csv_zone_out
    )


if __name__ == "__main__":
    main()

------------------------------------------------------------

File: D:\Documents\E_Plus_2030_py\idf_objects\structuring\flatten_hvac.py
============================================================
"""
flatten_hvac.py

Transforms the "assigned_hvac_params.csv" file (with columns [ogc_fid, param_name, assigned_value])
into two structured CSVs:
  1) assigned_hvac_building.csv
  2) assigned_hvac_zones.csv

Usage (standalone):
    python flatten_hvac.py

Or import and call 'main()' or the lower-level functions.
"""

import pandas as pd
import ast
import os

def parse_assigned_value(value_str):
    """
    Safely convert the string in 'assigned_value' into a Python dict.
    Uses ast.literal_eval to parse e.g.:
      "{'heating_day_setpoint': 20.28, 'cooling_day_setpoint': 24.55, ...}"
    into a real Python dictionary.
    """
    try:
        return ast.literal_eval(str(value_str))
    except (SyntaxError, ValueError):
        return {}


def flatten_hvac_data(df_input, out_build_csv, out_zone_csv):
    """
    UPDATED: Takes a pre-flattened DataFrame and splits it into two files.
    - Rows where 'zone_name' is NaN/None are considered building-level.
    - Rows with a 'zone_name' are considered zone-level.

    :param df_input: pd.DataFrame
        Must contain columns => "ogc_fid", "zone_name", "param_name", "assigned_value"
    :param out_build_csv: str
        File path for building-level CSV output.
    :param out_zone_csv: str
        File path for zone-level CSV output.
    """

    # Ensure the 'zone_name' column exists, even if all values are NaN
    if 'zone_name' not in df_input.columns:
        df_input['zone_name'] = pd.NA

    # 1. Building-level data is where zone_name is null/NaN
    df_build = df_input[df_input['zone_name'].isnull()].copy()
    
    # 2. Zone-level data is where zone_name has a value
    df_zone = df_input[df_input['zone_name'].notnull()].copy()

    # Rename 'assigned_value' to 'param_value' for consistency with downstream scripts
    if 'assigned_value' in df_build.columns:
        df_build.rename(columns={'assigned_value': 'param_value'}, inplace=True)
    
    if 'assigned_value' in df_zone.columns:
        df_zone.rename(columns={'assigned_value': 'param_value'}, inplace=True)

    # Select and ensure columns exist, even if DataFrame is empty
    build_cols = ["ogc_fid", "param_name", "param_value"]
    zone_cols = ["ogc_fid", "zone_name", "param_name", "param_value"]

    df_build = df_build.reindex(columns=build_cols)
    df_zone = df_zone.reindex(columns=zone_cols)

    # Write to CSV
    os.makedirs(os.path.dirname(out_build_csv), exist_ok=True)
    df_build.to_csv(out_build_csv, index=False)

    os.makedirs(os.path.dirname(out_zone_csv), exist_ok=True)
    df_zone.to_csv(out_zone_csv, index=False)

    print(f"[INFO] Wrote building-level HVAC picks to {out_build_csv} ({len(df_build)} rows).")
    print(f"[INFO] Wrote zone-level HVAC picks to {out_zone_csv} ({len(df_zone)} rows).")


def main():
    """
    Example CLI entry point.
    """
    csv_in = r"D:\Documents\E_Plus_2030_py\output\assigned\assigned_hvac_params.csv"
    csv_build_out = r"D:\Documents\E_Plus_2030_py\output\assigned\assigned_hvac_building.csv"
    csv_zone_out  = r"D:\Documents\E_Plus_2030_py\output\assigned\assigned_hvac_zones.csv"

    # Check if the input file exists
    if not os.path.exists(csv_in):
        print(f"Error: Input file not found at {csv_in}")
        return

    df_assigned = pd.read_csv(csv_in)

    flatten_hvac_data(
        df_input=df_assigned,
        out_build_csv=csv_build_out,
        out_zone_csv=csv_zone_out
    )

if __name__ == "__main__":
    main()
------------------------------------------------------------

File: D:\Documents\E_Plus_2030_py\orchestrator.py
============================================================
"""
orchestrator.py

Orchestrates the entire EnergyPlus workflow using a job-specific subfolder
for config files and a job-specific folder in /output for results.

Steps:
  1. Retrieve 'job_id' from job_config (set by job_manager or app).
  2. Form an output directory: <OUTPUT_DIR>/<job_id>.
  3. Load main_config.json from user_configs/<job_id>.
  4. Merge with posted_data["main_config"] if present.
  5. Apply Excel overrides, JSON overrides, create IDFs, run sims, etc.
  6. If scenario modification is enabled, override paths so scenario IDFs/results
     stay in the same job folder, then run scenario-based modifications.
  7. Perform structuring (e.g., flatten assigned CSVs) if requested.
  8. Perform global validation, sensitivity, surrogate, calibration if requested;
     patch any relative CSV paths to be inside the job folder (unless "data/").
  9. Zip & email final results if mail_user.json is present.
  10. Respect any cancel_event from job_manager.
"""

import os
import json
import logging
import threading
import time
from contextlib import contextmanager
import pandas as pd

# Splitting / deep-merge
from splitter import deep_merge_dicts

# DB loading if needed
from database_handler import load_buildings_from_db

# Excel overrides
from excel_overrides import (
    override_dhw_lookup_from_excel_file,
    override_epw_lookup_from_excel_file,
    override_lighting_lookup_from_excel_file,
    override_hvac_lookup_from_excel_file,
    override_vent_lookup_from_excel_file
)

# Fenestration config
from idf_objects.fenez.fenez_config_manager import build_fenez_config

# IDF creation
import idf_creation
from idf_creation import create_idfs_for_all_buildings

# Scenario modification
from main_modifi import run_modification_workflow

# Validation
from validation.main_validation import run_validation_process

# Sensitivity / Surrogate / Calibration
from cal.unified_sensitivity import run_sensitivity_analysis
from cal.unified_surrogate import (
    load_scenario_params as sur_load_scenario_params,
    pivot_scenario_params,
    load_sim_results,
    aggregate_results,
    merge_params_with_results,
    build_and_save_surrogate
)
from cal.unified_calibration import run_unified_calibration

# Zip & email
from zip_and_mail import zip_user_output, send_results_email

from cleanup_old_jobs import cleanup_old_results


class WorkflowCanceled(Exception):
    """Custom exception used to stop the workflow if a cancel_event is set."""
    pass


@contextmanager
def step_timer(logger, name: str):
    """Context manager to log step durations."""
    logger.info(f"[STEP] Starting {name} ...")
    start = time.perf_counter()
    try:
        yield
    finally:
        elapsed = time.perf_counter() - start
        logger.info(f"[STEP] Finished {name} in {elapsed:.2f} seconds.")


def orchestrate_workflow(job_config: dict, cancel_event: threading.Event = None):
    """
    Orchestrates the entire E+ workflow using a job-specific subfolder for config JSON,
    plus a job-specific output folder for results.

    Args:
        job_config (dict): includes:
            {
              "job_id": "<unique_id_for_this_job>",
              "job_subfolder": "user_configs/<job_id>",
              "posted_data": {...} (optional),
              ...
            }
        cancel_event (threading.Event): If set, we gracefully exit early.

    Raises:
        WorkflowCanceled if cancel_event is set mid-way.

    Returns:
        None (logs extensively, optionally zips/emails final results).
    """
    logger = logging.getLogger(__name__)
    logger.info("=== Starting orchestrate_workflow ===")
    overall_start = time.perf_counter()

    # -------------------------------------------------------------------------
    # 0) Identify job_id, define check_canceled
    # -------------------------------------------------------------------------
    job_id = job_config.get("job_id", "unknown_job_id")
    logger.info(f"[INFO] Orchestrator for job_id={job_id}")

    def check_canceled():
        """Raise WorkflowCanceled if cancel_event is set."""
        if cancel_event and cancel_event.is_set():
            logger.warning("=== CANCEL event detected. Stopping workflow. ===")
            raise WorkflowCanceled("Workflow was canceled by user request.")

    # -------------------------------------------------------------------------
    # 1) Identify the user_configs folder (where main_config.json resides)
    # -------------------------------------------------------------------------
    user_configs_folder = job_config.get("job_subfolder")
    if not user_configs_folder or not os.path.isdir(user_configs_folder):
        logger.error(f"[ERROR] job_subfolder not found or invalid => {user_configs_folder}")
        return

    # -------------------------------------------------------------------------
    # 2) Build an output directory for this job under OUTPUT_DIR
    #    e.g. /usr/src/app/output/<job_id>
    # -------------------------------------------------------------------------
    env_out_dir = os.environ.get("OUTPUT_DIR", "/usr/src/app/output")
    job_output_dir = os.path.join(env_out_dir, job_id)
    os.makedirs(job_output_dir, exist_ok=True)
    logger.info(f"[INFO] Using job-specific output folder: {job_output_dir}")

    # -------------------------------------------------------------------------
    # 3) Load main_config.json from user_configs/<job_id>
    # -------------------------------------------------------------------------
    main_config_path = os.path.join(user_configs_folder, "main_config.json")
    if not os.path.isfile(main_config_path):
        logger.error(f"[ERROR] Cannot find main_config.json at {main_config_path}")
        return

    with open(main_config_path, "r") as f:
        existing_config_raw = json.load(f)
    main_config = existing_config_raw.get("main_config", {})
    logger.info(f"[INFO] Loaded existing main_config from {main_config_path}.")

    # Merge posted_data["main_config"] if present
    posted_data = job_config.get("posted_data", {})
    if "main_config" in posted_data:
        logger.info("[INFO] Deep merging posted_data['main_config'] into main_config.")
        deep_merge_dicts(main_config, posted_data["main_config"])
        # optionally re-save
        with open(main_config_path, "w") as f:
            json.dump({"main_config": main_config}, f, indent=2)

    # -------------------------------------------------------------------------
    # 4) Extract sub-sections from main_config
    # -------------------------------------------------------------------------
    check_canceled()
    paths_dict       = main_config.get("paths", {})
    excel_flags      = main_config.get("excel_overrides", {})
    user_flags       = main_config.get("user_config_overrides", {})
    def_dicts        = main_config.get("default_dicts", {})
    structuring_cfg  = main_config.get("structuring", {})
    modification_cfg = main_config.get("modification", {})
    validation_cfg   = main_config.get("validation", {})
    sens_cfg         = main_config.get("sensitivity", {})
    sur_cfg          = main_config.get("surrogate", {})
    cal_cfg          = main_config.get("calibration", {})

    # IDF creation block
    idf_cfg = main_config.get("idf_creation", {})
    perform_idf_creation = idf_cfg.get("perform_idf_creation", False)
    scenario             = idf_cfg.get("scenario", "scenario1")
    calibration_stage    = idf_cfg.get("calibration_stage", "pre_calibration")
    strategy             = idf_cfg.get("strategy", "B")
    random_seed          = idf_cfg.get("random_seed", 42)
    run_simulations      = idf_cfg.get("run_simulations", True)
    simulate_config      = idf_cfg.get("simulate_config", {})
    post_process         = idf_cfg.get("post_process", True)
    post_process_config  = idf_cfg.get("post_process_config", {})
    output_definitions   = idf_cfg.get("output_definitions", {})
    use_database         = main_config.get("use_database", False)
    db_filter            = main_config.get("db_filter", {})
    filter_by            = main_config.get("filter_by")  # if using DB

    # Summarize which major steps will run
    steps_to_run = []
    if perform_idf_creation:
        steps_to_run.append("IDF creation")
        if run_simulations:
            steps_to_run.append("simulations")
    if structuring_cfg.get("perform_structuring", False):
        steps_to_run.append("structuring")
    if modification_cfg.get("perform_modification", False):
        steps_to_run.append("modification")
    if main_config.get("validation_base", {}).get("perform_validation", False):
        steps_to_run.append("base validation")
    if main_config.get("validation_scenarios", {}).get("perform_validation", False):
        steps_to_run.append("scenario validation")
    if sens_cfg.get("perform_sensitivity", False):
        steps_to_run.append("sensitivity analysis")
    if sur_cfg.get("perform_surrogate", False):
        steps_to_run.append("surrogate modeling")
    if cal_cfg.get("perform_calibration", False):
        steps_to_run.append("calibration")

    if steps_to_run:
        logger.info("[INFO] Steps to execute: " + ", ".join(steps_to_run))
    else:
        logger.info("[INFO] No major steps are enabled in configuration.")

    # -------------------------------------------------------------------------
    # 5) Possibly override idf_creation.idf_config from env, then force IDFs
    #    to go in <job_output_dir>/output_IDFs
    # -------------------------------------------------------------------------
    check_canceled()

    env_idd_path = os.environ.get("IDD_PATH")
    if env_idd_path:
        idf_creation.idf_config["iddfile"] = env_idd_path
    env_base_idf = os.environ.get("BASE_IDF_PATH")
    if env_base_idf:
        idf_creation.idf_config["idf_file_path"] = env_base_idf

    job_idf_dir = os.path.join(job_output_dir, "output_IDFs")
    os.makedirs(job_idf_dir, exist_ok=True)
    idf_creation.idf_config["output_dir"] = job_idf_dir

    # If user explicitly set these in main_config, override again
    if "iddfile" in idf_cfg:
        idf_creation.idf_config["iddfile"] = idf_cfg["iddfile"]
    if "idf_file_path" in idf_cfg:
        idf_creation.idf_config["idf_file_path"] = idf_cfg["idf_file_path"]

    if "output_idf_dir" in idf_cfg:
        subfolder = idf_cfg["output_idf_dir"]  # e.g. "output_IDFs"
        full_dir = os.path.join(job_output_dir, subfolder)
        idf_creation.idf_config["output_dir"] = full_dir
    else:
        idf_creation.idf_config["output_dir"] = os.path.join(job_output_dir, "output_IDFs")

    # -------------------------------------------------------------------------
    # 6) Setup default dictionaries
    # -------------------------------------------------------------------------
    base_res_data    = def_dicts.get("res_data", {})
    base_nonres_data = def_dicts.get("nonres_data", {})
    dhw_lookup       = def_dicts.get("dhw", {})
    epw_lookup       = def_dicts.get("epw", [])
    lighting_lookup  = def_dicts.get("lighting", {})
    hvac_lookup      = def_dicts.get("hvac", {})
    vent_lookup      = def_dicts.get("vent", {})

    # -------------------------------------------------------------------------
    # 7) Apply Excel overrides if flags are set
    # -------------------------------------------------------------------------
    check_canceled()

    updated_res_data, updated_nonres_data = build_fenez_config(
        base_res_data=base_res_data,
        base_nonres_data=base_nonres_data,
        excel_path=paths_dict.get("fenez_excel", ""),
        do_excel_override=excel_flags.get("override_fenez_excel", False),
        user_fenez_overrides=[]
    )

    if excel_flags.get("override_dhw_excel", False):
        dhw_lookup = override_dhw_lookup_from_excel_file(
            dhw_excel_path=paths_dict.get("dhw_excel", ""),
            default_dhw_lookup=dhw_lookup,
            override_dhw_flag=True
        )

    if excel_flags.get("override_epw_excel", False):
        epw_lookup = override_epw_lookup_from_excel_file(
            epw_excel_path=paths_dict.get("epw_excel", ""),
            epw_lookup=epw_lookup,
            override_epw_flag=True
        )

    if excel_flags.get("override_lighting_excel", False):
        lighting_lookup = override_lighting_lookup_from_excel_file(
            lighting_excel_path=paths_dict.get("lighting_excel", ""),
            lighting_lookup=lighting_lookup,
            override_lighting_flag=True
        )

    if excel_flags.get("override_hvac_excel", False):
        hvac_lookup = override_hvac_lookup_from_excel_file(
            hvac_excel_path=paths_dict.get("hvac_excel", ""),
            hvac_lookup=hvac_lookup,
            override_hvac_flag=True
        )

    if excel_flags.get("override_vent_excel", False):
        vent_lookup = override_vent_lookup_from_excel_file(
            vent_excel_path=paths_dict.get("vent_excel", ""),
            vent_lookup=vent_lookup,
            override_vent_flag=True
        )

    # -------------------------------------------------------------------------
    # 8) JSON overrides from user_configs/<job_id> if user_flags are set
    # -------------------------------------------------------------------------
    check_canceled()

    def safe_load_subjson(fname, key):
        """
        Loads user_configs/<job_id>/fname if it exists, returns data.get(key).
        """
        full_path = os.path.join(user_configs_folder, fname)
        if os.path.isfile(full_path):
            try:
                with open(full_path, "r") as ff:
                    data = json.load(ff)
                return data.get(key)
            except Exception as e:
                logger.error(f"[ERROR] loading {fname} => {e}")
        return None

    # Fenestration
    user_fenez_data = []
    if user_flags.get("override_fenez_json", False):
        loaded = safe_load_subjson("fenestration.json", "fenestration")
        if loaded:
            user_fenez_data = loaded

    updated_res_data, updated_nonres_data = build_fenez_config(
        base_res_data=updated_res_data,
        base_nonres_data=updated_nonres_data,
        excel_path="",
        do_excel_override=False,
        user_fenez_overrides=user_fenez_data
    )

    # DHW
    user_config_dhw = None
    if user_flags.get("override_dhw_json", False):
        user_config_dhw = safe_load_subjson("dhw.json", "dhw")

    # EPW
    user_config_epw = []
    if user_flags.get("override_epw_json", False):
        e = safe_load_subjson("epw.json", "epw")
        if e:
            user_config_epw = e

    # Lighting
    user_config_lighting = None
    if user_flags.get("override_lighting_json", False):
        user_config_lighting = safe_load_subjson("lighting.json", "lighting")

    # HVAC
    user_config_hvac = None
    if user_flags.get("override_hvac_json", False):
        user_config_hvac = safe_load_subjson("hvac.json", "hvac")

    # Vent
    user_config_vent = []
    if user_flags.get("override_vent_json", False):
        v = safe_load_subjson("vent.json", "vent")
        if v:
            user_config_vent = v

    # Geometry
    geom_data = {}
    if user_flags.get("override_geometry_json", False):
        g = safe_load_subjson("geometry.json", "geometry")
        if g:
            geom_data["geometry"] = g

    # Shading
    shading_data = {}
    if user_flags.get("override_shading_json", False):
        s = safe_load_subjson("shading.json", "shading")
        if s:
            shading_data["shading"] = s

    # -------------------------------------------------------------------------
    # 9) IDF creation
    # -------------------------------------------------------------------------
    check_canceled()
    df_buildings = pd.DataFrame()

    if perform_idf_creation:
        logger.info("[INFO] IDF creation is ENABLED.")
        with step_timer(logger, "IDF creation and simulations"):
            # a) Load building data
            if use_database:
                logger.info("[INFO] Loading building data from DB.")
                if not filter_by:
                    raise ValueError("[ERROR] 'filter_by' must be specified when 'use_database' is True.")
                df_buildings = load_buildings_from_db(db_filter, filter_by)

                # Optionally save the raw DB buildings
                extracted_csv_path = os.path.join(job_output_dir, "extracted_buildings.csv")
                df_buildings.to_csv(extracted_csv_path, index=False)
                logger.info(f"[INFO] Saved extracted buildings to {extracted_csv_path}")

            else:
                bldg_data_path = paths_dict.get("building_data", "")
                if os.path.isfile(bldg_data_path):
                    df_buildings = pd.read_csv(bldg_data_path)
                else:
                    logger.warning(f"[WARN] building_data CSV not found => {bldg_data_path}")

            logger.info(f"[INFO] Number of buildings to simulate: {len(df_buildings)}")

            # b) Create IDFs & (optionally) run sims in job folder
            df_buildings = create_idfs_for_all_buildings(
                df_buildings=df_buildings,
                scenario=scenario,
                calibration_stage=calibration_stage,
                strategy=strategy,
                random_seed=random_seed,
                user_config_geom=geom_data.get("geometry", []),
                user_config_lighting=user_config_lighting,
                user_config_dhw=user_config_dhw,
                res_data=updated_res_data,
                nonres_data=updated_nonres_data,
                user_config_hvac=user_config_hvac,
                user_config_vent=user_config_vent,
                user_config_epw=user_config_epw,
                output_definitions=output_definitions,
                run_simulations=run_simulations,
                simulate_config=simulate_config,
                post_process=post_process,
                post_process_config=post_process_config,
                logs_base_dir=job_output_dir
            )

            # === Store the mapping (ogc_fid -> idf_name) so we can look it up later ===
            idf_map_csv = os.path.join(job_output_dir, "extracted_idf_buildings.csv")
            df_buildings.to_csv(idf_map_csv, index=False)
            logger.info(f"[INFO] Wrote building -> IDF map to {idf_map_csv}")

    else:
        logger.info("[INFO] Skipping IDF creation.")

    # -------------------------------------------------------------------------
    # 10) Perform structuring if requested
    # -------------------------------------------------------------------------
    check_canceled()
    if structuring_cfg.get("perform_structuring", False):
        with step_timer(logger, "structuring"):
            logger.info("[INFO] Performing structuring ...")

            # --- Fenestration -------------------------------------------------
            from idf_objects.structuring.fenestration_structuring import transform_fenez_log_to_structured_with_ranges
            fenez_conf = structuring_cfg.get("fenestration", {})
            fenez_in = fenez_conf.get("csv_in", "assigned/assigned_fenez_params.csv")
            fenez_out = fenez_conf.get("csv_out", "assigned/structured_fenez_params.csv")
            if not os.path.isabs(fenez_in):
                fenez_in = os.path.join(job_output_dir, fenez_in)
            if not os.path.isabs(fenez_out):
                fenez_out = os.path.join(job_output_dir, fenez_out)
            if os.path.isfile(fenez_in):
                transform_fenez_log_to_structured_with_ranges(csv_input=fenez_in, csv_output=fenez_out)
            else:
                logger.warning(f"[STRUCTURING] Fenestration input CSV not found => {fenez_in}")

            # --- DHW ---------------------------------------------------------
            from idf_objects.structuring.dhw_structuring import transform_dhw_log_to_structured
            dhw_conf = structuring_cfg.get("dhw", {})
            dhw_in = dhw_conf.get("csv_in", "assigned/assigned_dhw_params.csv")
            dhw_out = dhw_conf.get("csv_out", "assigned/structured_dhw_params.csv")
            if not os.path.isabs(dhw_in):
                dhw_in = os.path.join(job_output_dir, dhw_in)
            if not os.path.isabs(dhw_out):
                dhw_out = os.path.join(job_output_dir, dhw_out)
            if os.path.isfile(dhw_in):
                transform_dhw_log_to_structured(dhw_in, dhw_out)
            else:
                logger.warning(f"[STRUCTURING] DHW input CSV not found => {dhw_in}")

            # --- HVAC flatten -----------------------------------------------
            from idf_objects.structuring.flatten_hvac import flatten_hvac_data, parse_assigned_value as parse_hvac
            hvac_conf = structuring_cfg.get("hvac", {})
            hvac_in = hvac_conf.get("csv_in", "assigned/assigned_hvac_params.csv")
            hvac_bld = hvac_conf.get("build_out", "assigned/assigned_hvac_building.csv")
            hvac_zone = hvac_conf.get("zone_out", "assigned/assigned_hvac_zones.csv")
            if not os.path.isabs(hvac_in):
                hvac_in = os.path.join(job_output_dir, hvac_in)
            if not os.path.isabs(hvac_bld):
                hvac_bld = os.path.join(job_output_dir, hvac_bld)
            if not os.path.isabs(hvac_zone):
                hvac_zone = os.path.join(job_output_dir, hvac_zone)
            if os.path.isfile(hvac_in):
                df_hvac = pd.read_csv(hvac_in)
                if "assigned_value" in df_hvac.columns:
                    df_hvac["assigned_value"] = df_hvac["assigned_value"].apply(parse_hvac)
                    flatten_hvac_data(
                        df_input=df_hvac,
                        out_build_csv=hvac_bld,
                        out_zone_csv=hvac_zone,
                    )
                else:
                    logger.warning(
                        f"[STRUCTURING] 'assigned_value' column missing in {hvac_in}. Skipping HVAC flatten."
                    )
            else:
                logger.warning(f"[STRUCTURING] HVAC input CSV not found => {hvac_in}")

            # --- Vent flatten -----------------------------------------------
            from idf_objects.structuring.flatten_assigned_vent import flatten_ventilation_data, parse_assigned_value as parse_vent
            vent_conf = structuring_cfg.get("vent", {})
            vent_in = vent_conf.get("csv_in", "assigned/assigned_ventilation.csv")
            vent_bld = vent_conf.get("build_out", "assigned/assigned_vent_building.csv")
            vent_zone = vent_conf.get("zone_out", "assigned/assigned_vent_zones.csv")
            if not os.path.isabs(vent_in):
                vent_in = os.path.join(job_output_dir, vent_in)
            if not os.path.isabs(vent_bld):
                vent_bld = os.path.join(job_output_dir, vent_bld)
            if not os.path.isabs(vent_zone):
                vent_zone = os.path.join(job_output_dir, vent_zone)
            if os.path.isfile(vent_in):
                df_vent = pd.read_csv(vent_in)
                if "assigned_value" in df_vent.columns:
                    df_vent["assigned_value"] = df_vent["assigned_value"].apply(parse_vent)
                    flatten_ventilation_data(
                        df_input=df_vent,
                        out_build_csv=vent_bld,
                        out_zone_csv=vent_zone,
                    )
                else:
                    logger.warning(
                        f"[STRUCTURING] 'assigned_value' column missing in {vent_in}. Skipping ventilation flatten."
                    )
            else:
                logger.warning(f"[STRUCTURING] Vent input CSV not found => {vent_in}")
    else:
        logger.info("[INFO] Skipping structuring.")

    # -------------------------------------------------------------------------
    # 11) Scenario Modification
    # -------------------------------------------------------------------------
    check_canceled()
    if modification_cfg.get("perform_modification", False):
        with step_timer(logger, "modification"):
            logger.info("[INFO] Scenario modification is ENABLED.")

            mod_cfg = modification_cfg["modify_config"]

            # 1) Ensure scenario IDFs go to <job_output_dir>/scenario_idfs
            scenario_idf_dir = os.path.join(job_output_dir, "scenario_idfs")
            os.makedirs(scenario_idf_dir, exist_ok=True)
            mod_cfg["output_idf_dir"] = scenario_idf_dir

            # 2) Ensure scenario sims => <job_output_dir>/Sim_Results/Scenarios
            if "simulation_config" in mod_cfg:
                sim_out = os.path.join(job_output_dir, "Sim_Results", "Scenarios")
                os.makedirs(sim_out, exist_ok=True)
                mod_cfg["simulation_config"]["output_dir"] = sim_out

            # 3) Post-process => <job_output_dir>/results_scenarioes
            if "post_process_config" in mod_cfg:
                ppcfg = mod_cfg["post_process_config"]
                as_is_csv = os.path.join(job_output_dir, "results_scenarioes", "merged_as_is_scenarios.csv")
                daily_csv = os.path.join(job_output_dir, "results_scenarioes", "merged_daily_mean_scenarios.csv")
                os.makedirs(os.path.dirname(as_is_csv), exist_ok=True)
                os.makedirs(os.path.dirname(daily_csv), exist_ok=True)
                ppcfg["output_csv_as_is"] = as_is_csv
                ppcfg["output_csv_daily_mean"] = daily_csv

            # 4) Fix assigned_csv paths
            assigned_csv_dict = mod_cfg.get("assigned_csv", {})
            for key, rel_path in assigned_csv_dict.items():
                assigned_csv_dict[key] = os.path.join(job_output_dir, rel_path)

            # 5) Fix scenario_csv paths
            scenario_csv_dict = mod_cfg.get("scenario_csv", {})
            for key, rel_path in scenario_csv_dict.items():
                scenario_csv_dict[key] = os.path.join(job_output_dir, rel_path)

            # ----------------------------------------------------------------------
            # NEW LOGIC: pick the base_idf_path from building_id automatically
            # ----------------------------------------------------------------------
            # The user sets "building_id" in the config, e.g. 20233330
            building_id = mod_cfg["building_id"]

            # We need the CSV that was saved right after create_idfs_for_all_buildings(...)
            idf_map_csv = os.path.join(job_output_dir, "extracted_idf_buildings.csv")
            if not os.path.isfile(idf_map_csv):
                raise FileNotFoundError(
                    f"Cannot find building->IDF map CSV at {idf_map_csv}. "
                    f"Did you skip 'perform_idf_creation'?"
                )

        # Read the mapping: each row has "ogc_fid" and "idf_name"
            df_idf_map = pd.read_csv(idf_map_csv)
            row_match = df_idf_map.loc[df_idf_map["ogc_fid"] == building_id]

            if row_match.empty:
                raise ValueError(
                    f"No building found for building_id={building_id} in {idf_map_csv}"
                )

        # e.g. "building_0.idf", "building_16.idf", "building_16_ba62d0.idf", etc.
            idf_filename = row_match.iloc[0]["idf_name"]

        # Build the full path to that IDF in output_IDFs
            base_idf_path = os.path.join(job_output_dir, "output_IDFs", idf_filename)
            mod_cfg["base_idf_path"] = base_idf_path
            logger.info(f"[INFO] Auto-selected base IDF => {base_idf_path}")
        # ----------------------------------------------------------------------

            # Finally, run the scenario workflow
            run_modification_workflow(mod_cfg)
    else:
        logger.info("[INFO] Skipping scenario modification.")


    # -------------------------------------------------------------------------
    # 12) Helper to handle patching CSVs that are "relative" but not "data/".
    # -------------------------------------------------------------------------
    def patch_if_relative(csv_path: str):
        """
        1) If absolute, return as-is.
        2) If starts with 'data/', interpret as /usr/src/app/data/... (no job folder).
        3) Else, join with job_output_dir.
        """
        if not csv_path:
            return csv_path
        if os.path.isabs(csv_path):
            return csv_path
        if csv_path.startswith("data/"):
            return os.path.join("/usr/src/app", csv_path)
        return os.path.join(job_output_dir, csv_path)

    # -------------------------------------------------------------------------
    # 13) Global Validation
    # -------------------------------------------------------------------------
        # (A) Validation - BASE
    # -------------------------------------------------------------------------
    check_canceled()
    base_validation_cfg = main_config.get("validation_base", {})
    if base_validation_cfg.get("perform_validation", False):
        with step_timer(logger, "base validation"):
            logger.info("[INFO] BASE Validation is ENABLED.")
            val_conf = base_validation_cfg["config"]

            # Patch relative paths
            sim_csv = val_conf.get("sim_data_csv")
            if sim_csv:
                val_conf["sim_data_csv"] = patch_if_relative(sim_csv)

            real_csv = val_conf.get("real_data_csv")
            if real_csv:
                val_conf["real_data_csv"] = patch_if_relative(real_csv)

            out_csv = val_conf.get("output_csv")
            if out_csv:
                val_conf["output_csv"] = patch_if_relative(out_csv)

            # Now run the validation
            run_validation_process(val_conf)
    else:
        logger.info("[INFO] Skipping BASE validation or not requested.")

    # (B) Validation - SCENARIOS
    # -------------------------------------------------------------------------
    check_canceled()
    scenario_validation_cfg = main_config.get("validation_scenarios", {})
    if scenario_validation_cfg.get("perform_validation", False):
        with step_timer(logger, "scenario validation"):
            logger.info("[INFO] SCENARIO Validation is ENABLED.")
            val_conf = scenario_validation_cfg["config"]

            # Patch relative paths
            sim_csv = val_conf.get("sim_data_csv")
            if sim_csv:
                val_conf["sim_data_csv"] = patch_if_relative(sim_csv)

            real_csv = val_conf.get("real_data_csv")
            if real_csv:
                val_conf["real_data_csv"] = patch_if_relative(real_csv)

            out_csv = val_conf.get("output_csv")
            if out_csv:
                val_conf["output_csv"] = patch_if_relative(out_csv)

            # Now run the validation
            run_validation_process(val_conf)
    else:
        logger.info("[INFO] Skipping SCENARIO validation or not requested.")


    # -------------------------------------------------------------------------
    # 14) Sensitivity Analysis
    # -------------------------------------------------------------------------
    check_canceled()
    if sens_cfg.get("perform_sensitivity", False):
        with step_timer(logger, "sensitivity analysis"):
            logger.info("[INFO] Sensitivity Analysis is ENABLED.")

            scenario_folder = sens_cfg.get("scenario_folder", "")
            sens_cfg["scenario_folder"] = patch_if_relative(scenario_folder)

            results_csv = sens_cfg.get("results_csv", "")
            sens_cfg["results_csv"] = patch_if_relative(results_csv)

            out_csv = sens_cfg.get("output_csv", "sensitivity_output.csv")
            sens_cfg["output_csv"] = patch_if_relative(out_csv)

            run_sensitivity_analysis(
                scenario_folder=sens_cfg["scenario_folder"],
                method=sens_cfg["method"],
                results_csv=sens_cfg.get("results_csv", ""),
                target_variable=sens_cfg.get("target_variable", []),
                output_csv=sens_cfg.get("output_csv", "sensitivity_output.csv"),
                n_morris_trajectories=sens_cfg.get("n_morris_trajectories", 10),
                num_levels=sens_cfg.get("num_levels", 4),
                n_sobol_samples=sens_cfg.get("n_sobol_samples", 128)
            )
    else:
        logger.info("[INFO] Skipping sensitivity analysis.")

    # -------------------------------------------------------------------------
    # 15) Surrogate Modeling
    # -------------------------------------------------------------------------
    check_canceled()
    if sur_cfg.get("perform_surrogate", False):
        with step_timer(logger, "surrogate modeling"):
            logger.info("[INFO] Surrogate Modeling is ENABLED.")

            scenario_folder = sur_cfg.get("scenario_folder", "")
            sur_cfg["scenario_folder"] = patch_if_relative(scenario_folder)

            results_csv = sur_cfg.get("results_csv", "")
            sur_cfg["results_csv"] = patch_if_relative(results_csv)

            model_out = sur_cfg.get("model_out", "")
            sur_cfg["model_out"] = patch_if_relative(model_out)

            cols_out = sur_cfg.get("cols_out", "")
            sur_cfg["cols_out"] = patch_if_relative(cols_out)

            target_var = sur_cfg["target_variable"]
            test_size  = sur_cfg["test_size"]

            df_scen = sur_load_scenario_params(sur_cfg["scenario_folder"])
            pivot_df = pivot_scenario_params(df_scen)

            df_sim = load_sim_results(sur_cfg["results_csv"])
            df_agg = aggregate_results(df_sim)
            merged_df = merge_params_with_results(pivot_df, df_agg, target_var)

            rf_model, trained_cols = build_and_save_surrogate(
                df_data=merged_df,
                target_col=target_var,
                model_out_path=sur_cfg["model_out"],
                columns_out_path=sur_cfg["cols_out"],
                test_size=test_size,
                random_state=42
            )
            if rf_model:
                logger.info("[INFO] Surrogate model built & saved.")
            else:
                logger.warning("[WARN] Surrogate modeling failed or insufficient data.")
    else:
        logger.info("[INFO] Skipping surrogate modeling.")

    # -------------------------------------------------------------------------
    # 16) Calibration
    # -------------------------------------------------------------------------
    check_canceled()
    if cal_cfg.get("perform_calibration", False):
        with step_timer(logger, "calibration"):
            logger.info("[INFO] Calibration is ENABLED.")

            scen_folder = cal_cfg.get("scenario_folder", "")
            cal_cfg["scenario_folder"] = patch_if_relative(scen_folder)

            real_csv = cal_cfg.get("real_data_csv", "")
            cal_cfg["real_data_csv"] = patch_if_relative(real_csv)

            sur_model_path = cal_cfg.get("surrogate_model_path", "")
            cal_cfg["surrogate_model_path"] = patch_if_relative(sur_model_path)

            sur_cols_path = cal_cfg.get("surrogate_columns_path", "")
            cal_cfg["surrogate_columns_path"] = patch_if_relative(sur_cols_path)

            hist_csv = cal_cfg.get("output_history_csv", "")
            cal_cfg["output_history_csv"] = patch_if_relative(hist_csv)

            best_params_folder = cal_cfg.get("best_params_folder", "")
            cal_cfg["best_params_folder"] = patch_if_relative(best_params_folder)

            run_unified_calibration(cal_cfg)
    else:
        logger.info("[INFO] Skipping calibration.")

    # -------------------------------------------------------------------------
    # 17) Zip & Email final results, if mail_user.json present
    # -------------------------------------------------------------------------
    try:
        with step_timer(logger, "zipping and email"):
            mail_user_path = os.path.join(user_configs_folder, "mail_user.json")
            mail_info = {}
            if os.path.isfile(mail_user_path):
                with open(mail_user_path, "r") as f:
                    mail_info = json.load(f)

                mail_user_list = mail_info.get("mail_user", [])
                if len(mail_user_list) > 0:
                    first_user = mail_user_list[0]
                    recipient_email = first_user.get("email", "")
                    if recipient_email:
                        zip_path = zip_user_output(job_output_dir)
                        send_results_email(zip_path, recipient_email)
                        logger.info(f"[INFO] Emailed zip {zip_path} to {recipient_email}")
                    else:
                        logger.warning("[WARN] mail_user.json => missing 'email'")
                else:
                    logger.warning("[WARN] mail_user.json => 'mail_user' list is empty.")
            else:
                logger.info("[INFO] No mail_user.json found, skipping email.")
    except Exception as e:
        logger.error(f"[ERROR] Zipping/Emailing results failed => {e}")

    # -------------------------------------------------------------------------
    # LAST STEP: (Optional) Call the cleanup function
    # -------------------------------------------------------------------------
    try:
        cleanup_old_results()  # This will remove any job folder older than MAX_AGE_HOURS
    except Exception as e:
        logger.error(f"[CLEANUP ERROR] => {e}")

    total_time = time.perf_counter() - overall_start
    logger.info(f"=== End of orchestrate_workflow (took {total_time:.2f} seconds) ===")

------------------------------------------------------------

--- Displaying first 10 rows of each CSV file ---

================================================================================
Processing file: D:\Documents\E_Plus_2030_py\output\c0da7070-d688-4d49-b1ac-c3a9e56a9fac\assigned\assigned_ventilation.csv
================================================================================
   ogc_fid zone_name                           param_name      assigned_value
0  4136733       NaN        infiltration_base_L_s_m2_10Pa  1.2278853596915769
1  4136733       NaN  infiltration_base_L_s_m2_10Pa_range          (1.1, 1.3)
2  4136733       NaN                          year_factor  1.1050021510445334
3  4136733       NaN                    year_factor_range          (1.1, 1.3)
4  4136733       NaN                         fan_pressure                 0.0
5  4136733       NaN                   fan_pressure_range          (0.0, 0.0)
6  4136733       NaN                 fan_total_efficiency  0.5446421476297645
7  4136733       NaN           fan_total_efficiency_range          (0.5, 0.7)
8  4136733       NaN                               f_ctrl  1.0236471214164014
9  4136733       NaN                         f_ctrl_range        (0.95, 1.05)

================================================================================
Processing file: D:\Documents\E_Plus_2030_py\output\c0da7070-d688-4d49-b1ac-c3a9e56a9fac\assigned\assigned_vent_zones.csv
================================================================================
   ogc_fid             zone_name                                param_name  \
0  4136733  Zone1_FrontPerimeter                  infiltration_object_name   
1  4136733  Zone1_FrontPerimeter                  infiltration_object_type   
2  4136733  Zone1_FrontPerimeter  infiltration_flow_m3_s_DESIGN_TOTAL_ZONE   
3  4136733  Zone1_FrontPerimeter     infiltration_flow_m3_s_m2_DESIGN_ZONE   
4  4136733  Zone1_FrontPerimeter                infiltration_schedule_name   
5  4136733  Zone1_FrontPerimeter                   ventilation_object_name   
6  4136733  Zone1_FrontPerimeter                   ventilation_object_type   
7  4136733  Zone1_FrontPerimeter   ventilation_flow_m3_s_DESIGN_TOTAL_ZONE   
8  4136733  Zone1_FrontPerimeter      ventilation_flow_m3_s_m2_DESIGN_ZONE   
9  4136733  Zone1_FrontPerimeter                 ventilation_schedule_name   

             param_value  
0                     {}  
1                     {}  
2  0.0024764919250233677  
3  0.0002900821092811951  
4                     {}  
5                     {}  
6                     {}  
7   0.004980039248806627  
8  0.0005833333333333334  
9                     {}  

================================================================================
Processing file: D:\Documents\E_Plus_2030_py\output\c0da7070-d688-4d49-b1ac-c3a9e56a9fac\assigned\assigned_vent_building.csv
================================================================================
   ogc_fid                           param_name         param_value
0  4136733        infiltration_base_L_s_m2_10Pa  1.2278853596915769
1  4136733  infiltration_base_L_s_m2_10Pa_range          (1.1, 1.3)
2  4136733                          year_factor  1.1050021510445334
3  4136733                    year_factor_range          (1.1, 1.3)
4  4136733                         fan_pressure                 0.0
5  4136733                   fan_pressure_range          (0.0, 0.0)
6  4136733                 fan_total_efficiency  0.5446421476297645
7  4136733           fan_total_efficiency_range          (0.5, 0.7)
8  4136733                               f_ctrl  1.0236471214164014
9  4136733                         f_ctrl_range        (0.95, 1.05)

================================================================================
Processing file: D:\Documents\E_Plus_2030_py\output\c0da7070-d688-4d49-b1ac-c3a9e56a9fac\assigned\assigned_hvac_zones.csv
================================================================================
   ogc_fid             zone_name                    param_name param_value
0  4136733  Zone1_FrontPerimeter              hvac_object_name          {}
1  4136733  Zone1_FrontPerimeter              hvac_object_type          {}
2  4136733  Zone1_FrontPerimeter         availability_schedule          {}
3  4136733  Zone1_FrontPerimeter               thermostat_name          {}
4  4136733  Zone1_FrontPerimeter  thermostat_dualsetpoint_name          {}
5  4136733  Zone1_FrontPerimeter     heating_setpoint_schedule          {}
6  4136733  Zone1_FrontPerimeter     cooling_setpoint_schedule          {}
7  4136733  Zone1_RightPerimeter              hvac_object_name          {}
8  4136733  Zone1_RightPerimeter              hvac_object_type          {}
9  4136733  Zone1_RightPerimeter         availability_schedule          {}

================================================================================
Processing file: D:\Documents\E_Plus_2030_py\output\c0da7070-d688-4d49-b1ac-c3a9e56a9fac\assigned\assigned_hvac_params.csv
================================================================================
   ogc_fid zone_name                         param_name      assigned_value
0  4136733       NaN               heating_day_setpoint  19.639426798457883
1  4136733       NaN         heating_day_setpoint_range        (19.0, 20.0)
2  4136733       NaN             heating_night_setpoint  15.025010755222667
3  4136733       NaN       heating_night_setpoint_range        (15.0, 16.0)
4  4136733       NaN               cooling_day_setpoint  32.550058636738235
5  4136733       NaN         cooling_day_setpoint_range        (32.0, 34.0)
6  4136733       NaN             cooling_night_setpoint  32.446421476297644
7  4136733       NaN       cooling_night_setpoint_range        (32.0, 34.0)
8  4136733       NaN        max_heating_supply_air_temp   53.68235607082006
9  4136733       NaN  max_heating_supply_air_temp_range        (50.0, 55.0)

================================================================================
Processing file: D:\Documents\E_Plus_2030_py\output\c0da7070-d688-4d49-b1ac-c3a9e56a9fac\assigned\assigned_hvac_building.csv
================================================================================
   ogc_fid                         param_name         param_value
0  4136733               heating_day_setpoint  19.639426798457883
1  4136733         heating_day_setpoint_range        (19.0, 20.0)
2  4136733             heating_night_setpoint  15.025010755222667
3  4136733       heating_night_setpoint_range        (15.0, 16.0)
4  4136733               cooling_day_setpoint  32.550058636738235
5  4136733         cooling_day_setpoint_range        (32.0, 34.0)
6  4136733             cooling_night_setpoint  32.446421476297644
7  4136733       cooling_night_setpoint_range        (32.0, 34.0)
8  4136733        max_heating_supply_air_temp   53.68235607082006
9  4136733  max_heating_supply_air_temp_range        (50.0, 55.0)

Successfully stored all 6 file paths in 'file_paths.txt'