File: D:\Documents\E_Plus_2030_py\orchestrator.py
============================================================
"""
orchestrator.py

Orchestrates the entire EnergyPlus workflow using a job-specific subfolder
for config files and a job-specific folder in /output for results.

Steps:
  1. Retrieve 'job_id' from job_config (set by job_manager or app).
  2. Form an output directory: <OUTPUT_DIR>/<job_id>.
  3. Load main_config.json from user_configs/<job_id>.
  4. Merge with posted_data["main_config"] if present.
  5. Apply Excel overrides, JSON overrides, create IDFs, run sims, etc.
  6. If scenario modification is enabled, override paths so scenario IDFs/results
     stay in the same job folder, then run scenario-based modifications.
  7. Perform structuring (e.g., flatten assigned CSVs) if requested.
  8. Perform global validation, sensitivity, surrogate, calibration if requested;
     patch any relative CSV paths to be inside the job folder (unless "data/").
  9. Zip & email final results if mail_user.json is present.
  10. Respect any cancel_event from job_manager.
"""

import os
import json
import logging
import threading
import time
from contextlib import contextmanager
import pandas as pd

# Splitting / deep-merge
from splitter import deep_merge_dicts

# DB loading if needed
from database_handler import load_buildings_from_db

# Excel overrides
from excel_overrides import (
    override_dhw_lookup_from_excel_file,
    override_epw_lookup_from_excel_file,
    override_lighting_lookup_from_excel_file,
    override_hvac_lookup_from_excel_file,
    override_vent_lookup_from_excel_file
)

# Fenestration config
from idf_objects.fenez.fenez_config_manager import build_fenez_config


# IDF creation
import idf_creation
from idf_creation import create_idfs_for_all_buildings

# Scenario modification
from main_modifi import run_modification_workflow

# Validation
from validation.main_validation import run_validation_process

# Sensitivity / Surrogate / Calibration
from cal.unified_sensitivity import run_sensitivity_analysis
from cal.unified_surrogate import (
    load_scenario_params as sur_load_scenario_params,
    pivot_scenario_params,
    load_sim_results,
    aggregate_results,
    merge_params_with_results,
    build_and_save_surrogate
)
from cal.unified_calibration import run_unified_calibration

# Zip & email
from zip_and_mail import zip_user_output, send_results_email

from cleanup_old_jobs import cleanup_old_results


class WorkflowCanceled(Exception):
    """Custom exception used to stop the workflow if a cancel_event is set."""
    pass


@contextmanager
def step_timer(logger, name: str):
    """Context manager to log step durations."""
    logger.info(f"[STEP] Starting {name} ...")
    start = time.perf_counter()
    try:
        yield
    finally:
        elapsed = time.perf_counter() - start
        logger.info(f"[STEP] Finished {name} in {elapsed:.2f} seconds.")


def orchestrate_workflow(job_config: dict, cancel_event: threading.Event = None):
    """
    Orchestrates the entire E+ workflow using a job-specific subfolder for config JSON,
    plus a job-specific output folder for results.

    Args:
        job_config (dict): includes:
            {
              "job_id": "<unique_id_for_this_job>",
              "job_subfolder": "user_configs/<job_id>",
              "posted_data": {...} (optional),
              ...
            }
        cancel_event (threading.Event): If set, we gracefully exit early.

    Raises:
        WorkflowCanceled if cancel_event is set mid-way.

    Returns:
        None (logs extensively, optionally zips/emails final results).
    """
    logger = logging.getLogger(__name__)
    logger.info("=== Starting orchestrate_workflow ===")
    overall_start = time.perf_counter()

    # -------------------------------------------------------------------------
    # 0) Identify job_id, define check_canceled
    # -------------------------------------------------------------------------
    job_id = job_config.get("job_id", "unknown_job_id")
    logger.info(f"[INFO] Orchestrator for job_id={job_id}")

    def check_canceled():
        """Raise WorkflowCanceled if cancel_event is set."""
        if cancel_event and cancel_event.is_set():
            logger.warning("=== CANCEL event detected. Stopping workflow. ===")
            raise WorkflowCanceled("Workflow was canceled by user request.")


    # -------------------------------------------------------------------------
    # 12) Helper to handle patching CSVs that are "relative" but not "data/".
    # -------------------------------------------------------------------------
    def patch_if_relative(csv_path: str):
        """
        1) If absolute, return as-is.
        2) If starts with 'data/', interpret as /usr/src/app/data/... (no job folder).
        3) Else, join with job_output_dir.
        """
        if not csv_path:
            return csv_path
        if os.path.isabs(csv_path):
            return csv_path
        if csv_path.startswith("data/"):
            return os.path.join("/usr/src/app", csv_path)
        return os.path.join(job_output_dir, csv_path)





    # -------------------------------------------------------------------------
    # 1) Identify the user_configs folder (where main_config.json resides)
    # -------------------------------------------------------------------------
    user_configs_folder = job_config.get("job_subfolder")
    if not user_configs_folder or not os.path.isdir(user_configs_folder):
        logger.error(f"[ERROR] job_subfolder not found or invalid => {user_configs_folder}")
        return

    # -------------------------------------------------------------------------
    # 2) Build an output directory for this job under OUTPUT_DIR
    #    e.g. /usr/src/app/output/<job_id>
    # -------------------------------------------------------------------------
    env_out_dir = os.environ.get("OUTPUT_DIR", "/usr/src/app/output")
    job_output_dir = os.path.join(env_out_dir, job_id)
    os.makedirs(job_output_dir, exist_ok=True)
    logger.info(f"[INFO] Using job-specific output folder: {job_output_dir}")

    # -------------------------------------------------------------------------
    # 3) Load main_config.json from user_configs/<job_id>
    # -------------------------------------------------------------------------
    main_config_path = os.path.join(user_configs_folder, "main_config.json")
    if not os.path.isfile(main_config_path):
        logger.error(f"[ERROR] Cannot find main_config.json at {main_config_path}")
        return

    with open(main_config_path, "r") as f:
        existing_config_raw = json.load(f)
    main_config = existing_config_raw.get("main_config", {})
    logger.info(f"[INFO] Loaded existing main_config from {main_config_path}.")

    # Merge posted_data["main_config"] if present
    posted_data = job_config.get("posted_data", {})
    if "main_config" in posted_data:
        logger.info("[INFO] Deep merging posted_data['main_config'] into main_config.")
        deep_merge_dicts(main_config, posted_data["main_config"])
        # optionally re-save
        with open(main_config_path, "w") as f:
            json.dump({"main_config": main_config}, f, indent=2)

    # -------------------------------------------------------------------------
    # 4) Extract sub-sections from main_config
    # -------------------------------------------------------------------------
    check_canceled()
    paths_dict       = main_config.get("paths", {})
    excel_flags      = main_config.get("excel_overrides", {})
    user_flags       = main_config.get("user_config_overrides", {})
    def_dicts        = main_config.get("default_dicts", {})
    structuring_cfg  = main_config.get("structuring", {})
    modification_cfg = main_config.get("modification", {})
    validation_cfg   = main_config.get("validation", {})
    sens_cfg         = main_config.get("sensitivity", {})
    sur_cfg          = main_config.get("surrogate", {})
    cal_cfg          = main_config.get("calibration", {})

    # IDF creation block
    idf_cfg = main_config.get("idf_creation", {})
    perform_idf_creation = idf_cfg.get("perform_idf_creation", False)
    scenario             = idf_cfg.get("scenario", "scenario1")
    calibration_stage    = idf_cfg.get("calibration_stage", "pre_calibration")
    strategy             = idf_cfg.get("strategy", "B")
    random_seed          = idf_cfg.get("random_seed", 42)
    run_simulations      = idf_cfg.get("run_simulations", True)
    simulate_config      = idf_cfg.get("simulate_config", {})
    post_process         = idf_cfg.get("post_process", True)
    post_process_config  = idf_cfg.get("post_process_config", {})
    output_definitions   = idf_cfg.get("output_definitions", {})
    use_database         = main_config.get("use_database", False)
    db_filter            = main_config.get("db_filter", {})
    filter_by            = main_config.get("filter_by")  # if using DB

    # Summarize which major steps will run
    steps_to_run = []
    if perform_idf_creation:
        steps_to_run.append("IDF creation")
        if run_simulations:
            steps_to_run.append("simulations")
    if structuring_cfg.get("perform_structuring", False):
        steps_to_run.append("structuring")
    if modification_cfg.get("perform_modification", False):
        steps_to_run.append("modification")
    if main_config.get("validation_base", {}).get("perform_validation", False):
        steps_to_run.append("base validation")
    if main_config.get("validation_scenarios", {}).get("perform_validation", False):
        steps_to_run.append("scenario validation")
    if sens_cfg.get("perform_sensitivity", False):
        steps_to_run.append("sensitivity analysis")
    if sur_cfg.get("perform_surrogate", False):
        steps_to_run.append("surrogate modeling")
    if cal_cfg.get("perform_calibration", False):
        steps_to_run.append("calibration")

    if steps_to_run:
        logger.info("[INFO] Steps to execute: " + ", ".join(steps_to_run))
    else:
        logger.info("[INFO] No major steps are enabled in configuration.")

    # -------------------------------------------------------------------------
    # 5) Possibly override idf_creation.idf_config from env, then force IDFs
    #    to go in <job_output_dir>/output_IDFs
    # -------------------------------------------------------------------------
    check_canceled()

    env_idd_path = os.environ.get("IDD_PATH")
    if env_idd_path:
        idf_creation.idf_config["iddfile"] = env_idd_path
    env_base_idf = os.environ.get("BASE_IDF_PATH")
    if env_base_idf:
        idf_creation.idf_config["idf_file_path"] = env_base_idf

    job_idf_dir = os.path.join(job_output_dir, "output_IDFs")
    os.makedirs(job_idf_dir, exist_ok=True)
    idf_creation.idf_config["output_dir"] = job_idf_dir

    # If user explicitly set these in main_config, override again
    if "iddfile" in idf_cfg:
        idf_creation.idf_config["iddfile"] = idf_cfg["iddfile"]
    if "idf_file_path" in idf_cfg:
        idf_creation.idf_config["idf_file_path"] = idf_cfg["idf_file_path"]

    if "output_idf_dir" in idf_cfg:
        subfolder = idf_cfg["output_idf_dir"]  # e.g. "output_IDFs"
        full_dir = os.path.join(job_output_dir, subfolder)
        idf_creation.idf_config["output_dir"] = full_dir
    else:
        idf_creation.idf_config["output_dir"] = os.path.join(job_output_dir, "output_IDFs")

    # -------------------------------------------------------------------------
    # 6) Setup default dictionaries
    # -------------------------------------------------------------------------
    base_res_data    = def_dicts.get("res_data", {})
    base_nonres_data = def_dicts.get("nonres_data", {})
    dhw_lookup       = def_dicts.get("dhw", {})
    epw_lookup       = def_dicts.get("epw", [])
    lighting_lookup  = def_dicts.get("lighting", {})
    hvac_lookup      = def_dicts.get("hvac", {})
    vent_lookup      = def_dicts.get("vent", {})

    # -------------------------------------------------------------------------
    # 7) Apply Excel overrides if flags are set
    # -------------------------------------------------------------------------
    check_canceled()

    updated_res_data, updated_nonres_data = build_fenez_config(
        base_res_data=base_res_data,
        base_nonres_data=base_nonres_data,
        excel_path=paths_dict.get("fenez_excel", ""),
        do_excel_override=excel_flags.get("override_fenez_excel", False),
        user_fenez_overrides=[]
    )

    if excel_flags.get("override_dhw_excel", False):
        dhw_lookup = override_dhw_lookup_from_excel_file(
            dhw_excel_path=paths_dict.get("dhw_excel", ""),
            default_dhw_lookup=dhw_lookup,
            override_dhw_flag=True
        )

    if excel_flags.get("override_epw_excel", False):
        epw_lookup = override_epw_lookup_from_excel_file(
            epw_excel_path=paths_dict.get("epw_excel", ""),
            epw_lookup=epw_lookup,
            override_epw_flag=True
        )

    if excel_flags.get("override_lighting_excel", False):
        lighting_lookup = override_lighting_lookup_from_excel_file(
            lighting_excel_path=paths_dict.get("lighting_excel", ""),
            lighting_lookup=lighting_lookup,
            override_lighting_flag=True
        )

    if excel_flags.get("override_hvac_excel", False):
        hvac_lookup = override_hvac_lookup_from_excel_file(
            hvac_excel_path=paths_dict.get("hvac_excel", ""),
            hvac_lookup=hvac_lookup,
            override_hvac_flag=True
        )

    if excel_flags.get("override_vent_excel", False):
        vent_lookup = override_vent_lookup_from_excel_file(
            vent_excel_path=paths_dict.get("vent_excel", ""),
            vent_lookup=vent_lookup,
            override_vent_flag=True
        )

    # -------------------------------------------------------------------------
    # 8) JSON overrides from user_configs/<job_id> if user_flags are set
    # -------------------------------------------------------------------------
    check_canceled()

    def safe_load_subjson(fname, key):
        """
        Loads user_configs/<job_id>/fname if it exists, returns data.get(key).
        """
        full_path = os.path.join(user_configs_folder, fname)
        if os.path.isfile(full_path):
            try:
                with open(full_path, "r") as ff:
                    data = json.load(ff)
                return data.get(key)
            except Exception as e:
                logger.error(f"[ERROR] loading {fname} => {e}")
        return None

    # Fenestration
    user_fenez_data = []
    if user_flags.get("override_fenez_json", False):
        loaded = safe_load_subjson("fenestration.json", "fenestration")
        if loaded:
            user_fenez_data = loaded

    updated_res_data, updated_nonres_data = build_fenez_config(
        base_res_data=updated_res_data,
        base_nonres_data=updated_nonres_data,
        excel_path="",
        do_excel_override=False,
        user_fenez_overrides=user_fenez_data
    )

    # DHW
    user_config_dhw = None
    if user_flags.get("override_dhw_json", False):
        user_config_dhw = safe_load_subjson("dhw.json", "dhw")

    # EPW
    user_config_epw = []
    if user_flags.get("override_epw_json", False):
        e = safe_load_subjson("epw.json", "epw")
        if e:
            user_config_epw = e

    # Lighting
    user_config_lighting = None
    if user_flags.get("override_lighting_json", False):
        user_config_lighting = safe_load_subjson("lighting.json", "lighting")

    # HVAC
    user_config_hvac = None
    if user_flags.get("override_hvac_json", False):
        user_config_hvac = safe_load_subjson("hvac.json", "hvac")

    # Vent
    user_config_vent = []
    if user_flags.get("override_vent_json", False):
        v = safe_load_subjson("vent.json", "vent")
        if v:
            user_config_vent = v

    # Geometry
    geom_data = {}
    if user_flags.get("override_geometry_json", False):
        g = safe_load_subjson("geometry.json", "geometry")
        if g:
            geom_data["geometry"] = g

    # Shading
    shading_data = {}
    if user_flags.get("override_shading_json", False):
        s = safe_load_subjson("shading.json", "shading")
        if s:
            shading_data["shading"] = s

    # -------------------------------------------------------------------------
    # 9) IDF creation
    # -------------------------------------------------------------------------
    check_canceled()
    df_buildings = pd.DataFrame()

    if perform_idf_creation:
        logger.info("[INFO] IDF creation is ENABLED.")
        with step_timer(logger, "IDF creation and simulations"):
            # a) Load building data
            if use_database:
                logger.info("[INFO] Loading building data from DB.")
                if not filter_by:
                    raise ValueError("[ERROR] 'filter_by' must be specified when 'use_database' is True.")
                df_buildings = load_buildings_from_db(db_filter, filter_by)

                # Optionally save the raw DB buildings
                extracted_csv_path = os.path.join(job_output_dir, "extracted_buildings.csv")
                df_buildings.to_csv(extracted_csv_path, index=False)
                logger.info(f"[INFO] Saved extracted buildings to {extracted_csv_path}")

            else:
                bldg_data_path = paths_dict.get("building_data", "")
                if os.path.isfile(bldg_data_path):
                    df_buildings = pd.read_csv(bldg_data_path)
                else:
                    logger.warning(f"[WARN] building_data CSV not found => {bldg_data_path}")

            logger.info(f"[INFO] Number of buildings to simulate: {len(df_buildings)}")

            # b) Create IDFs & (optionally) run sims in job folder
            df_buildings = create_idfs_for_all_buildings(
                df_buildings=df_buildings,
                scenario=scenario,
                calibration_stage=calibration_stage,
                strategy=strategy,
                random_seed=random_seed,
                user_config_geom=geom_data.get("geometry", []),
                user_config_lighting=user_config_lighting,
                user_config_dhw=user_config_dhw,
                res_data=updated_res_data,
                nonres_data=updated_nonres_data,
                user_config_hvac=user_config_hvac,
                user_config_vent=user_config_vent,
                user_config_epw=user_config_epw,
                output_definitions=output_definitions,
                run_simulations=run_simulations,
                simulate_config=simulate_config,
                post_process=post_process,
                post_process_config=post_process_config,
                logs_base_dir=job_output_dir
            )

            # === Store the mapping (ogc_fid -> idf_name) so we can look it up later ===
            idf_map_csv = os.path.join(job_output_dir, "extracted_idf_buildings.csv")
            df_buildings.to_csv(idf_map_csv, index=False)
            logger.info(f"[INFO] Wrote building -> IDF map to {idf_map_csv}")

    else:
        logger.info("[INFO] Skipping IDF creation.")

    # -------------------------------------------------------------------------
    # 10) Perform structuring if requested
    # -------------------------------------------------------------------------
    check_canceled()
    if structuring_cfg.get("perform_structuring", False):
        with step_timer(logger, "structuring"):
            logger.info("[INFO] Performing structuring ...")

            # --- Fenestration -------------------------------------------------
            from idf_objects.structuring.fenestration_structuring import transform_fenez_log_to_structured_with_ranges
            fenez_conf = structuring_cfg.get("fenestration", {})
            fenez_in = fenez_conf.get("csv_in", "assigned/assigned_fenez_params.csv")
            fenez_out = fenez_conf.get("csv_out", "assigned/structured_fenez_params.csv")
            if not os.path.isabs(fenez_in):
                fenez_in = os.path.join(job_output_dir, fenez_in)
            if not os.path.isabs(fenez_out):
                fenez_out = os.path.join(job_output_dir, fenez_out)
            if os.path.isfile(fenez_in):
                transform_fenez_log_to_structured_with_ranges(csv_input=fenez_in, csv_output=fenez_out)
            else:
                logger.warning(f"[STRUCTURING] Fenestration input CSV not found => {fenez_in}")

            # --- DHW ---------------------------------------------------------
            from idf_objects.structuring.dhw_structuring import transform_dhw_log_to_structured
            dhw_conf = structuring_cfg.get("dhw", {})
            dhw_in = dhw_conf.get("csv_in", "assigned/assigned_dhw_params.csv")
            dhw_out = dhw_conf.get("csv_out", "assigned/structured_dhw_params.csv")
            if not os.path.isabs(dhw_in):
                dhw_in = os.path.join(job_output_dir, dhw_in)
            if not os.path.isabs(dhw_out):
                dhw_out = os.path.join(job_output_dir, dhw_out)
            if os.path.isfile(dhw_in):
                transform_dhw_log_to_structured(dhw_in, dhw_out)
            else:
                logger.warning(f"[STRUCTURING] DHW input CSV not found => {dhw_in}")


            # NEW:--- Shading ----------------------------------------------------
            from idf_objects.structuring.shading_structuring import transform_shading_log_to_structured
            shading_conf = structuring_cfg.get("shading", {})
            user_shading_rules = safe_load_subjson("shading.json", "shading") or []
            
            if shading_conf:
                shading_in = patch_if_relative(shading_conf.get("csv_in"))
                shading_out = patch_if_relative(shading_conf.get("csv_out"))

                transform_shading_log_to_structured(
                    csv_input=shading_in,
                    csv_output=shading_out,
                    user_shading_rules=user_shading_rules
                )
            else:
                logger.warning("[STRUCTURING] 'shading' configuration not found in structuring settings.")


            # --- Equipment --------------------------------------------------
            from idf_objects.structuring.equipment_structuring import transform_equipment_log_to_structured
            equip_conf = structuring_cfg.get("equipment", {})
            user_equip_rules = safe_load_subjson("equipment.json", "equipment") or []
            
            if equip_conf:
                equip_in = patch_if_relative(equip_conf.get("csv_in"))
                equip_out = patch_if_relative(equip_conf.get("csv_out"))

                transform_equipment_log_to_structured(
                    csv_input=equip_in,
                    csv_output=equip_out,
                    user_equipment_rules=user_equip_rules
                )
            else:
                logger.warning("[STRUCTURING] 'equipment' configuration not found in structuring settings.")



            # --- Zone Sizing ------------------------------------------------
            from idf_objects.structuring.zone_sizing_structuring import transform_zone_sizing_log_to_structured
            sizing_conf = structuring_cfg.get("zone_sizing", {})
            user_sizing_rules = safe_load_subjson("zone_sizing.json", "zone_sizing") or []
            
            if sizing_conf:
                sizing_in = patch_if_relative(sizing_conf.get("csv_in"))
                sizing_out = patch_if_relative(sizing_conf.get("csv_out"))

                transform_zone_sizing_log_to_structured(
                    csv_input=sizing_in,
                    csv_output=sizing_out,
                    user_sizing_rules=user_sizing_rules
                )
            else:
                logger.warning("[STRUCTURING] 'zone_sizing' configuration not found.")



            # --- HVAC flatten -----------------------------------------------
            from idf_objects.structuring.flatten_hvac import flatten_hvac_data, parse_assigned_value as parse_hvac
            hvac_conf = structuring_cfg.get("hvac", {})
            hvac_in = hvac_conf.get("csv_in", "assigned/assigned_hvac_params.csv")
            hvac_bld = hvac_conf.get("build_out", "assigned/assigned_hvac_building.csv")
            hvac_zone = hvac_conf.get("zone_out", "assigned/assigned_hvac_zones.csv")
            if not os.path.isabs(hvac_in):
                hvac_in = os.path.join(job_output_dir, hvac_in)
            if not os.path.isabs(hvac_bld):
                hvac_bld = os.path.join(job_output_dir, hvac_bld)
            if not os.path.isabs(hvac_zone):
                hvac_zone = os.path.join(job_output_dir, hvac_zone)
            if os.path.isfile(hvac_in):
                df_hvac = pd.read_csv(hvac_in)
                if "assigned_value" in df_hvac.columns:
                    df_hvac["assigned_value"] = df_hvac["assigned_value"].apply(parse_hvac)
                    flatten_hvac_data(
                        df_input=df_hvac,
                        out_build_csv=hvac_bld,
                        out_zone_csv=hvac_zone,
                    )
                else:
                    logger.warning(
                        f"[STRUCTURING] 'assigned_value' column missing in {hvac_in}. Skipping HVAC flatten."
                    )
            else:
                logger.warning(f"[STRUCTURING] HVAC input CSV not found => {hvac_in}")

            # --- Vent flatten -----------------------------------------------
            from idf_objects.structuring.flatten_assigned_vent import flatten_ventilation_data, parse_assigned_value as parse_vent
            vent_conf = structuring_cfg.get("vent", {})
            vent_in = vent_conf.get("csv_in", "assigned/assigned_ventilation.csv")
            vent_bld = vent_conf.get("build_out", "assigned/assigned_vent_building.csv")
            vent_zone = vent_conf.get("zone_out", "assigned/assigned_vent_zones.csv")
            if not os.path.isabs(vent_in):
                vent_in = os.path.join(job_output_dir, vent_in)
            if not os.path.isabs(vent_bld):
                vent_bld = os.path.join(job_output_dir, vent_bld)
            if not os.path.isabs(vent_zone):
                vent_zone = os.path.join(job_output_dir, vent_zone)
            if os.path.isfile(vent_in):
                df_vent = pd.read_csv(vent_in)
                if "assigned_value" in df_vent.columns:
                    df_vent["assigned_value"] = df_vent["assigned_value"].apply(parse_vent)
                    flatten_ventilation_data(
                        df_input=df_vent,
                        out_build_csv=vent_bld,
                        out_zone_csv=vent_zone,
                    )
                else:
                    logger.warning(
                        f"[STRUCTURING] 'assigned_value' column missing in {vent_in}. Skipping ventilation flatten."
                    )
            else:
                logger.warning(f"[STRUCTURING] Vent input CSV not found => {vent_in}")
    else:
        logger.info("[INFO] Skipping structuring.")

    # -------------------------------------------------------------------------
    # 11) Scenario Modification
    # -------------------------------------------------------------------------
    check_canceled()
    if modification_cfg.get("perform_modification", False):
        with step_timer(logger, "modification"):
            logger.info("[INFO] Scenario modification is ENABLED.")

            mod_cfg = modification_cfg["modify_config"]

            # 1) Ensure scenario IDFs go to <job_output_dir>/scenario_idfs
            scenario_idf_dir = os.path.join(job_output_dir, "scenario_idfs")
            os.makedirs(scenario_idf_dir, exist_ok=True)
            mod_cfg["output_idf_dir"] = scenario_idf_dir

            # 2) Ensure scenario sims => <job_output_dir>/Sim_Results/Scenarios
            if "simulation_config" in mod_cfg:
                sim_out = os.path.join(job_output_dir, "Sim_Results", "Scenarios")
                os.makedirs(sim_out, exist_ok=True)
                mod_cfg["simulation_config"]["output_dir"] = sim_out

            # 3) Post-process => <job_output_dir>/results_scenarioes
            if "post_process_config" in mod_cfg:
                ppcfg = mod_cfg["post_process_config"]
                as_is_csv = os.path.join(job_output_dir, "results_scenarioes", "merged_as_is_scenarios.csv")
                daily_csv = os.path.join(job_output_dir, "results_scenarioes", "merged_daily_mean_scenarios.csv")
                os.makedirs(os.path.dirname(as_is_csv), exist_ok=True)
                os.makedirs(os.path.dirname(daily_csv), exist_ok=True)
                ppcfg["output_csv_as_is"] = as_is_csv
                ppcfg["output_csv_daily_mean"] = daily_csv

            # 4) Fix assigned_csv paths
            assigned_csv_dict = mod_cfg.get("assigned_csv", {})
            for key, rel_path in assigned_csv_dict.items():
                assigned_csv_dict[key] = os.path.join(job_output_dir, rel_path)

            # 5) Fix scenario_csv paths
            scenario_csv_dict = mod_cfg.get("scenario_csv", {})
            for key, rel_path in scenario_csv_dict.items():
                scenario_csv_dict[key] = os.path.join(job_output_dir, rel_path)

            # ----------------------------------------------------------------------
            # NEW LOGIC: pick the base_idf_path from building_id automatically
            # ----------------------------------------------------------------------
            # The user sets "building_id" in the config, e.g. 20233330
            building_id = mod_cfg["building_id"]

            # We need the CSV that was saved right after create_idfs_for_all_buildings(...)
            idf_map_csv = os.path.join(job_output_dir, "extracted_idf_buildings.csv")
            if not os.path.isfile(idf_map_csv):
                raise FileNotFoundError(
                    f"Cannot find building->IDF map CSV at {idf_map_csv}. "
                    f"Did you skip 'perform_idf_creation'?"
                )

        # Read the mapping: each row has "ogc_fid" and "idf_name"
            df_idf_map = pd.read_csv(idf_map_csv)
            row_match = df_idf_map.loc[df_idf_map["ogc_fid"] == building_id]

            if row_match.empty:
                raise ValueError(
                    f"No building found for building_id={building_id} in {idf_map_csv}"
                )

        # e.g. "building_0.idf", "building_16.idf", "building_16_ba62d0.idf", etc.
            idf_filename = row_match.iloc[0]["idf_name"]

        # Build the full path to that IDF in output_IDFs
            base_idf_path = os.path.join(job_output_dir, "output_IDFs", idf_filename)
            mod_cfg["base_idf_path"] = base_idf_path
            logger.info(f"[INFO] Auto-selected base IDF => {base_idf_path}")
        # ----------------------------------------------------------------------

            # Finally, run the scenario workflow
            run_modification_workflow(mod_cfg)
    else:
        logger.info("[INFO] Skipping scenario modification.")


    # -------------------------------------------------------------------------
    # 12) Helper to handle patching CSVs that are "relative" but not "data/".
    # -------------------------------------------------------------------------
    def patch_if_relative(csv_path: str):
        """
        1) If absolute, return as-is.
        2) If starts with 'data/', interpret as /usr/src/app/data/... (no job folder).
        3) Else, join with job_output_dir.
        """
        if not csv_path:
            return csv_path
        if os.path.isabs(csv_path):
            return csv_path
        if csv_path.startswith("data/"):
            return os.path.join("/usr/src/app", csv_path)
        return os.path.join(job_output_dir, csv_path)

    # -------------------------------------------------------------------------
    # 13) Global Validation
    # -------------------------------------------------------------------------
        # (A) Validation - BASE
    # -------------------------------------------------------------------------
    check_canceled()
    base_validation_cfg = main_config.get("validation_base", {})
    if base_validation_cfg.get("perform_validation", False):
        with step_timer(logger, "base validation"):
            logger.info("[INFO] BASE Validation is ENABLED.")
            val_conf = base_validation_cfg["config"]

            # Patch relative paths
            sim_csv = val_conf.get("sim_data_csv")
            if sim_csv:
                val_conf["sim_data_csv"] = patch_if_relative(sim_csv)

            real_csv = val_conf.get("real_data_csv")
            if real_csv:
                val_conf["real_data_csv"] = patch_if_relative(real_csv)

            out_csv = val_conf.get("output_csv")
            if out_csv:
                val_conf["output_csv"] = patch_if_relative(out_csv)

            # Now run the validation
            run_validation_process(val_conf)
    else:
        logger.info("[INFO] Skipping BASE validation or not requested.")

    # (B) Validation - SCENARIOS
    # -------------------------------------------------------------------------
    check_canceled()
    scenario_validation_cfg = main_config.get("validation_scenarios", {})
    if scenario_validation_cfg.get("perform_validation", False):
        with step_timer(logger, "scenario validation"):
            logger.info("[INFO] SCENARIO Validation is ENABLED.")
            val_conf = scenario_validation_cfg["config"]

            # Patch relative paths
            sim_csv = val_conf.get("sim_data_csv")
            if sim_csv:
                val_conf["sim_data_csv"] = patch_if_relative(sim_csv)

            real_csv = val_conf.get("real_data_csv")
            if real_csv:
                val_conf["real_data_csv"] = patch_if_relative(real_csv)

            out_csv = val_conf.get("output_csv")
            if out_csv:
                val_conf["output_csv"] = patch_if_relative(out_csv)

            # Now run the validation
            run_validation_process(val_conf)
    else:
        logger.info("[INFO] Skipping SCENARIO validation or not requested.")


    # -------------------------------------------------------------------------
    # 14) Sensitivity Analysis
    # -------------------------------------------------------------------------
    check_canceled()
    if sens_cfg.get("perform_sensitivity", False):
        with step_timer(logger, "sensitivity analysis"):
            logger.info("[INFO] Sensitivity Analysis is ENABLED.")

            scenario_folder = sens_cfg.get("scenario_folder", "")
            sens_cfg["scenario_folder"] = patch_if_relative(scenario_folder)

            results_csv = sens_cfg.get("results_csv", "")
            sens_cfg["results_csv"] = patch_if_relative(results_csv)

            out_csv = sens_cfg.get("output_csv", "sensitivity_output.csv")
            sens_cfg["output_csv"] = patch_if_relative(out_csv)

            run_sensitivity_analysis(
                scenario_folder=sens_cfg["scenario_folder"],
                method=sens_cfg["method"],
                results_csv=sens_cfg.get("results_csv", ""),
                target_variable=sens_cfg.get("target_variable", []),
                output_csv=sens_cfg.get("output_csv", "sensitivity_output.csv"),
                n_morris_trajectories=sens_cfg.get("n_morris_trajectories", 10),
                num_levels=sens_cfg.get("num_levels", 4),
                n_sobol_samples=sens_cfg.get("n_sobol_samples", 128)
            )
    else:
        logger.info("[INFO] Skipping sensitivity analysis.")

    # -------------------------------------------------------------------------
    # 15) Surrogate Modeling
    # -------------------------------------------------------------------------
    check_canceled()
    if sur_cfg.get("perform_surrogate", False):
        with step_timer(logger, "surrogate modeling"):
            logger.info("[INFO] Surrogate Modeling is ENABLED.")

            scenario_folder = sur_cfg.get("scenario_folder", "")
            sur_cfg["scenario_folder"] = patch_if_relative(scenario_folder)

            results_csv = sur_cfg.get("results_csv", "")
            sur_cfg["results_csv"] = patch_if_relative(results_csv)

            model_out = sur_cfg.get("model_out", "")
            sur_cfg["model_out"] = patch_if_relative(model_out)

            cols_out = sur_cfg.get("cols_out", "")
            sur_cfg["cols_out"] = patch_if_relative(cols_out)

            target_var = sur_cfg["target_variable"]
            test_size  = sur_cfg["test_size"]

            df_scen = sur_load_scenario_params(sur_cfg["scenario_folder"])
            pivot_df = pivot_scenario_params(df_scen)

            df_sim = load_sim_results(sur_cfg["results_csv"])
            df_agg = aggregate_results(df_sim)
            merged_df = merge_params_with_results(pivot_df, df_agg, target_var)

            rf_model, trained_cols = build_and_save_surrogate(
                df_data=merged_df,
                target_col=target_var,
                model_out_path=sur_cfg["model_out"],
                columns_out_path=sur_cfg["cols_out"],
                test_size=test_size,
                random_state=42
            )
            if rf_model:
                logger.info("[INFO] Surrogate model built & saved.")
            else:
                logger.warning("[WARN] Surrogate modeling failed or insufficient data.")
    else:
        logger.info("[INFO] Skipping surrogate modeling.")

    # -------------------------------------------------------------------------
    # 16) Calibration
    # -------------------------------------------------------------------------
    check_canceled()
    if cal_cfg.get("perform_calibration", False):
        with step_timer(logger, "calibration"):
            logger.info("[INFO] Calibration is ENABLED.")

            scen_folder = cal_cfg.get("scenario_folder", "")
            cal_cfg["scenario_folder"] = patch_if_relative(scen_folder)

            real_csv = cal_cfg.get("real_data_csv", "")
            cal_cfg["real_data_csv"] = patch_if_relative(real_csv)

            sur_model_path = cal_cfg.get("surrogate_model_path", "")
            cal_cfg["surrogate_model_path"] = patch_if_relative(sur_model_path)

            sur_cols_path = cal_cfg.get("surrogate_columns_path", "")
            cal_cfg["surrogate_columns_path"] = patch_if_relative(sur_cols_path)

            hist_csv = cal_cfg.get("output_history_csv", "")
            cal_cfg["output_history_csv"] = patch_if_relative(hist_csv)

            best_params_folder = cal_cfg.get("best_params_folder", "")
            cal_cfg["best_params_folder"] = patch_if_relative(best_params_folder)

            run_unified_calibration(cal_cfg)
    else:
        logger.info("[INFO] Skipping calibration.")

    # -------------------------------------------------------------------------
    # 17) Zip & Email final results, if mail_user.json present
    # -------------------------------------------------------------------------
    try:
        with step_timer(logger, "zipping and email"):
            mail_user_path = os.path.join(user_configs_folder, "mail_user.json")
            mail_info = {}
            if os.path.isfile(mail_user_path):
                with open(mail_user_path, "r") as f:
                    mail_info = json.load(f)

                mail_user_list = mail_info.get("mail_user", [])
                if len(mail_user_list) > 0:
                    first_user = mail_user_list[0]
                    recipient_email = first_user.get("email", "")
                    if recipient_email:
                        zip_path = zip_user_output(job_output_dir)
                        send_results_email(zip_path, recipient_email)
                        logger.info(f"[INFO] Emailed zip {zip_path} to {recipient_email}")
                    else:
                        logger.warning("[WARN] mail_user.json => missing 'email'")
                else:
                    logger.warning("[WARN] mail_user.json => 'mail_user' list is empty.")
            else:
                logger.info("[INFO] No mail_user.json found, skipping email.")
    except Exception as e:
        logger.error(f"[ERROR] Zipping/Emailing results failed => {e}")

    # -------------------------------------------------------------------------
    # LAST STEP: (Optional) Call the cleanup function
    # -------------------------------------------------------------------------
    try:
        cleanup_old_results()  # This will remove any job folder older than MAX_AGE_HOURS
    except Exception as e:
        logger.error(f"[CLEANUP ERROR] => {e}")

    total_time = time.perf_counter() - overall_start
    logger.info(f"=== End of orchestrate_workflow (took {total_time:.2f} seconds) ===")

------------------------------------------------------------

File: D:\Documents\E_Plus_2030_py\main_modifi.py
============================================================
"""
main_modifi.py

Handles the generation of scenario-based IDFs for sensitivity, surrogate,
calibration, or any parametric runs, then optionally runs E+ simulation,
post-processing, and validation in a job-specific folder if provided.

Usage:
  - Typically invoked from your orchestrator (or command line) with a config dict:
    {
      "base_idf_path": "output_IDFs/building_0.idf",
      "idd_path": "EnergyPlus/Energy+.idd",
      "assigned_csv": {
        "hvac_building": "output/assigned/assigned_hvac_building.csv",
        "hvac_zones": "output/assigned/assigned_hvac_zones.csv",
        "dhw": "output/assigned/assigned_dhw_params.csv",
        "vent_build": "output/assigned/assigned_vent_building.csv",
        "vent_zones": "output/assigned/assigned_vent_zones.csv",
        "elec": "output/assigned/assigned_lighting.csv",
        "fenez": "output/assigned/structured_fenez_params.csv"
      },
      "scenario_csv": {
        "hvac": "output/scenarios/scenario_params_hvac.csv",
        "dhw": "output/scenarios/scenario_params_dhw.csv",
        "vent": "output/scenarios/scenario_params_vent.csv",
        "elec": "output/scenarios/scenario_params_elec.csv",
        "fenez": "output/scenarios/scenario_params_fenez.csv"
      },
      "output_idf_dir": "output/scenario_idfs",
      "building_id": 4136730,
      "num_scenarios": 5,
      "picking_method": "random_uniform",
      "picking_scale_factor": 0.5,

      "run_simulations": true,
      "simulation_config": {
        "num_workers": 4,
        "output_dir": "output/Sim_Results/Scenarios"
      },
      "perform_post_process": true,
      "post_process_config": {
        "output_csv_as_is": "output/results_scenarioes/merged_as_is_scenarios.csv",
        "output_csv_daily_mean": "output/results_scenarioes/merged_daily_mean_scenarios.csv"
      },
      "perform_validation": true,
      "validation_config": {
        "real_data_csv": "data/mock_merged_daily_mean.csv",
        "sim_data_csv": "output/results_scenarioes/merged_daily_mean_scenarios.csv",
        "bldg_ranges": { "0": [0,1,2] },
        "variables_to_compare": [...],
        "threshold_cv_rmse": 30.0,
        "skip_plots": true,
        "output_csv": "scenario_validation_report.csv"
      },

      "job_output_dir": "/usr/src/app/output/xxxx-uuid"   # (Optional)
    }
"""

import os
import logging
import pandas as pd

# ---------------------------------------------------------------------------
# A) Common Utilities
# ---------------------------------------------------------------------------
from modification.common_utils import (
    load_assigned_csv,
    load_scenario_csv,
    load_idf,
    save_idf,
    generate_multiple_param_sets,
    save_param_scenarios_to_csv
)

# ---------------------------------------------------------------------------
# B) Modules for scenario creation & application
# ---------------------------------------------------------------------------
from modification.hvac_functions import (
    create_hvac_scenarios,
    apply_building_level_hvac,
    apply_zone_level_hvac
)
from modification.dhw_functions import (
    create_dhw_scenarios,
    apply_dhw_params_to_idf
)
from modification.vent_functions import (
    create_vent_scenarios,
    apply_building_level_vent,
    apply_zone_level_vent
)
from modification.elec_functions import (
    create_elec_scenarios,
    apply_building_level_elec,
    apply_object_level_elec
)
from modification.fenez_functions2 import (
    create_fenez_scenarios,
    apply_object_level_fenez
)


# NEW: Import the shading functions
from modification.shading_functions import create_shading_scenarios, apply_shading_params_to_idf

from modification.equipment_functions import create_equipment_scenarios, apply_equipment_params_to_idf

from modification.zone_sizing_functions import create_zone_sizing_scenarios, apply_zone_sizing_params_to_idf




# ---------------------------------------------------------------------------
# C) Simulation + Post-processing + Validation
# ---------------------------------------------------------------------------
from epw.run_epw_sims import simulate_all
from postproc.merge_results import merge_all_results
from validation.main_validation import run_validation_process


def run_all_idfs_in_folder(
    folder_path: str,
    iddfile: str,
    base_output_dir: str,
    default_lat: float = 52.15,
    default_lon: float = 4.40,
    default_year: int = 2020,
    num_workers: int = 4
):
    """
    Utility function to find .idf files in folder_path and run them with simulate_all(...).
    Adjust lat/lon/year or load them from a side CSV if needed.
    """
    logger = logging.getLogger(__name__)
    logger.info(f"[run_all_idfs_in_folder] Searching .idf files in {folder_path}")

    if not os.path.isdir(folder_path):
        logger.warning(f"[run_all_idfs_in_folder] Folder not found => {folder_path}")
        return

    idf_files = [f for f in os.listdir(folder_path) if f.lower().endswith(".idf")]
    if not idf_files:
        logger.warning(f"[run_all_idfs_in_folder] No .idf files in {folder_path} to run.")
        return

    data_rows = []
    for idx, idf_name in enumerate(idf_files):
        data_rows.append({
            "idf_name": idf_name,
            "lat": default_lat,
            "lon": default_lon,
            "desired_climate_year": default_year,
            "ogc_fid": idx  # or parse from filename
        })

    df_scenarios = pd.DataFrame(data_rows)
    logger.info(f"[run_all_idfs_in_folder] Running {len(df_scenarios)} scenario IDFs with simulate_all...")

    simulate_all(
        df_buildings=df_scenarios,
        idf_directory=folder_path,
        iddfile=iddfile,
        base_output_dir=base_output_dir,
        user_config_epw=None,
        assigned_epw_log=None,
        num_workers=num_workers
    )
    logger.info("[run_all_idfs_in_folder] Simulations triggered.")


def run_modification_workflow(config):
    """
    Main function for scenario-based IDF creation + optional E+ simulation,
    post-processing, and validation.

    Steps:
      1) Resolve folder paths (scenario IDFs, results) based on config + optional job_output_dir.
      2) Load assigned CSV data (HVAC, DHW, Vent, Elec, Fenez).
      3) Filter for the chosen building.
      4) Generate scenario param picks (random or otherwise).
      5) For each scenario, load a fresh base IDF, apply picks, save scenario IDF.
      6) (Optional) run E+ sims for these scenario IDFs, then post-process, then validate.

    :param config: dict
    :return: None
    """
    logger = logging.getLogger(__name__)
    logger.info("[MODIFICATION] Starting scenario-based workflow...")

    # -----------------------------------------------------------------------
    # 1) Extract config parts & resolve paths
    # -----------------------------------------------------------------------
    base_idf_path   = config["base_idf_path"]
    idd_path        = config["idd_path"]
    assigned_csvs   = config["assigned_csv"]
    scenario_csvs   = config["scenario_csv"]
    building_id     = config["building_id"]
    num_scenarios   = config["num_scenarios"]
    picking_method  = config["picking_method"]
    scale_factor    = config.get("picking_scale_factor", 1.0)

    # The user might specify something like "output/scenario_idfs" or just "scenario_idfs".
    scenario_idf_dir = config.get("output_idf_dir", "output/scenario_idfs")

    # Also we have simulation, post-processing, and validation flags:
    run_sims        = config.get("run_simulations", False)
    sim_cfg         = config.get("simulation_config", {})
    do_postproc     = config.get("perform_post_process", False)
    postproc_cfg    = config.get("post_process_config", {})
    do_validation   = config.get("perform_validation", False)
    validation_cfg  = config.get("validation_config", {})

    # If "job_output_dir" is provided, make scenario_idf_dir relative to it (if it's not absolute).
    job_output_dir = config.get("job_output_dir")  # optional
    if job_output_dir and not os.path.isabs(scenario_idf_dir):
        scenario_idf_dir = os.path.join(job_output_dir, scenario_idf_dir)
    os.makedirs(scenario_idf_dir, exist_ok=True)

    logger.info(f"[MODIFICATION] Scenario IDFs will be placed in: {scenario_idf_dir}")

    # -----------------------------------------------------------------------
    # 2) Load assigned CSV data
    # -----------------------------------------------------------------------
    # HVAC
    df_hvac_bld = None
    df_hvac_zn  = None
    if "hvac_building" in assigned_csvs and "hvac_zones" in assigned_csvs:
        df_hvac_bld = load_assigned_csv(assigned_csvs["hvac_building"])
        df_hvac_zn  = load_assigned_csv(assigned_csvs["hvac_zones"])
    elif "hvac" in assigned_csvs:
        df_hvac_bld = load_assigned_csv(assigned_csvs["hvac"])

    # DHW
    df_dhw = load_assigned_csv(assigned_csvs["dhw"]) if "dhw" in assigned_csvs else None

    # Vent
    df_vent_bld = None
    df_vent_zn  = None
    if "vent_build" in assigned_csvs and "vent_zones" in assigned_csvs:
        df_vent_bld = load_assigned_csv(assigned_csvs["vent_build"])
        df_vent_zn  = load_assigned_csv(assigned_csvs["vent_zones"])
    elif "vent" in assigned_csvs:
        df_vent_bld = load_assigned_csv(assigned_csvs["vent"])

    # Elec
    df_elec  = load_assigned_csv(assigned_csvs["elec"])  if "elec"  in assigned_csvs else None

    # Fenestration
    df_fenez = load_assigned_csv(assigned_csvs["fenez"]) if "fenez" in assigned_csvs else None

    # NEW: Load assigned shading data
    df_shading = load_assigned_csv(assigned_csvs["shading"]) if "shading" in assigned_csvs else None

    #  Load assigned CSV data
    df_equip = load_assigned_csv(assigned_csvs["equip"]) if "equip" in assigned_csvs else None
    # Zone sizing
    df_zone_sizing = load_assigned_csv(assigned_csvs["zone_sizing"]) if "zone_sizing" in assigned_csvs else None
    



    # -----------------------------------------------------------------------
    # 3) Filter data for this building
    # -----------------------------------------------------------------------
    def filter_for_building(df):
        if df is not None and not df.empty:
            return df[df["ogc_fid"] == building_id].copy()
        return pd.DataFrame()

    df_hvac_bld_sub = filter_for_building(df_hvac_bld)
    df_hvac_zn_sub  = filter_for_building(df_hvac_zn)
    df_dhw_sub      = filter_for_building(df_dhw)
    df_vent_bld_sub = filter_for_building(df_vent_bld)
    df_vent_zn_sub  = filter_for_building(df_vent_zn)
    df_elec_sub     = filter_for_building(df_elec)
    df_fenez_sub    = filter_for_building(df_fenez)



    # NEW: Filter shading data. It uses 'window_id', not 'ogc_fid'.
    # We assume for now that all windows in the base IDF belong to the target building.
    df_shading_sub = df_shading.copy() if df_shading is not None else pd.DataFrame()

    df_equip_sub = filter_for_building(df_equip)
    df_zone_sizing_sub = filter_for_building(df_zone_sizing)



    # -----------------------------------------------------------------------
    # 4) Generate scenario picks (random or otherwise)
    # -----------------------------------------------------------------------
    # HVAC
    if not df_hvac_bld_sub.empty:
        if not df_hvac_zn_sub.empty:
            # multi-step scenario creation for building & zone
            create_hvac_scenarios(
                df_building=df_hvac_bld_sub,
                df_zones=df_hvac_zn_sub,
                building_id=building_id,
                num_scenarios=num_scenarios,
                picking_method=picking_method,
                random_seed=42,
                scenario_csv_out=scenario_csvs["hvac"]
            )
        else:
            hvac_scen = generate_multiple_param_sets(
                df_main_sub=df_hvac_bld_sub,
                num_sets=num_scenarios,
                picking_method=picking_method,
                scale_factor=scale_factor
            )
            save_param_scenarios_to_csv(hvac_scen, building_id, scenario_csvs["hvac"])

    # DHW
    if not df_dhw_sub.empty:
        create_dhw_scenarios(
            df_dhw_input=df_dhw_sub,
            building_id=building_id,
            num_scenarios=num_scenarios,
            picking_method=picking_method,
            random_seed=42,
            scenario_csv_out=scenario_csvs["dhw"]
        )

    # Vent
    if not df_vent_bld_sub.empty or not df_vent_zn_sub.empty:
        create_vent_scenarios(
            df_building=df_vent_bld_sub,
            df_zones=df_vent_zn_sub,
            building_id=building_id,
            num_scenarios=num_scenarios,
            picking_method=picking_method,
            random_seed=42,
            scenario_csv_out=scenario_csvs["vent"]
        )

    # Elec
    if not df_elec_sub.empty:
        create_elec_scenarios(
            df_lighting=df_elec_sub,
            building_id=building_id,
            num_scenarios=num_scenarios,
            picking_method=picking_method,
            random_seed=42,
            scenario_csv_out=scenario_csvs["elec"]
        )

    # Fenestration
    if not df_fenez_sub.empty:
        create_fenez_scenarios(
            df_struct_fenez=df_fenez_sub,
            building_id=building_id,
            num_scenarios=num_scenarios,
            picking_method=picking_method,
            random_seed=42,
            scenario_csv_out=scenario_csvs["fenez"]
        )

    # NEW: Generate shading scenarios
    if not df_shading_sub.empty:
        create_shading_scenarios(
            df_shading_input=df_shading_sub,
            building_id=building_id,
            num_scenarios=num_scenarios,
            picking_method=picking_method,
            random_seed=42,
            scenario_csv_out=scenario_csvs["shading"]
        )


    # eequip
    if not df_equip_sub.empty:
        create_equipment_scenarios(
            df_equipment_input=df_equip_sub,
            building_id=building_id,
            num_scenarios=num_scenarios,
            picking_method=picking_method,
            random_seed=42,
            scenario_csv_out=scenario_csvs["equip"]
        )

    # Sizingg
    if not df_zone_sizing_sub.empty:
        create_zone_sizing_scenarios(
            df_sizing_input=df_zone_sizing_sub,
            building_id=building_id,
            num_scenarios=num_scenarios,
            picking_method='random_uniform', # This function handles its own logic
            random_seed=42,
            scenario_csv_out=scenario_csvs["zone_sizing"]
        )




    # -----------------------------------------------------------------------
    # 5) Load scenario CSV => group by scenario_index
    # -----------------------------------------------------------------------
    def safe_load_scenario(csv_path):
        if os.path.isfile(csv_path):
            return load_scenario_csv(csv_path)
        return pd.DataFrame()

    df_hvac_scen  = safe_load_scenario(scenario_csvs["hvac"])
    df_dhw_scen   = safe_load_scenario(scenario_csvs["dhw"])
    df_vent_scen  = safe_load_scenario(scenario_csvs["vent"])
    df_elec_scen  = safe_load_scenario(scenario_csvs["elec"])
    df_fenez_scen = safe_load_scenario(scenario_csvs["fenez"])
    # NEW: Load shading scenarios
    df_shading_scen = safe_load_scenario(scenario_csvs["shading"])
    df_equip_scen = safe_load_scenario(scenario_csvs["equip"])
    df_sizing_scen = safe_load_scenario(scenario_csvs["zone_sizing"])


    hvac_groups  = df_hvac_scen.groupby("scenario_index")  if not df_hvac_scen.empty  else None
    dhw_groups   = df_dhw_scen.groupby("scenario_index")   if not df_dhw_scen.empty   else None
    vent_groups  = df_vent_scen.groupby("scenario_index")  if not df_vent_scen.empty  else None
    elec_groups  = df_elec_scen.groupby("scenario_index")  if not df_elec_scen.empty  else None
    fenez_groups = df_fenez_scen.groupby("scenario_index") if not df_fenez_scen.empty else None

    # NEW: Group shading scenarios
    shading_groups = df_shading_scen.groupby("scenario_index") if not df_shading_scen.empty else None
    equip_groups = df_equip_scen.groupby("scenario_index") if not df_equip_scen.empty else None
    sizing_groups = df_sizing_scen.groupby("scenario_index") if not df_sizing_scen.empty else None


    # -----------------------------------------------------------------------
    # 6) For each scenario, load base IDF, apply parameters, save new IDF
    # -----------------------------------------------------------------------
    for i in range(num_scenarios):
        logger.info(f"[MODIFICATION] => Creating scenario #{i} for building {building_id}")

        hvac_df   = hvac_groups.get_group(i) if hvac_groups and i in hvac_groups.groups else pd.DataFrame()
        dhw_df    = dhw_groups.get_group(i)  if dhw_groups  and i in dhw_groups.groups  else pd.DataFrame()
        vent_df   = vent_groups.get_group(i) if vent_groups and i in vent_groups.groups else pd.DataFrame()
        elec_df   = elec_groups.get_group(i) if elec_groups and i in elec_groups.groups else pd.DataFrame()
        fenez_df  = fenez_groups.get_group(i)if fenez_groups and i in fenez_groups.groups else pd.DataFrame()
        # NEW: Get the shading data for this specific scenario
        shading_df = shading_groups.get_group(i) if shading_groups and i in shading_groups.groups else pd.DataFrame()
        equip_df = equip_groups.get_group(i) if equip_groups and i in equip_groups.groups else pd.DataFrame()
        sizing_df = sizing_groups.get_group(i) if sizing_groups and i in sizing_groups.groups else pd.DataFrame()


        hvac_bld_df   = hvac_df[hvac_df["zone_name"].isna()]
        hvac_zone_df  = hvac_df[hvac_df["zone_name"].notna()]
        hvac_params   = _make_param_dict(hvac_bld_df)

        dhw_params    = _make_param_dict(dhw_df)

        vent_bld_df   = vent_df[vent_df["zone_name"].isnull()]
        vent_zone_df  = vent_df[vent_df["zone_name"].notnull()]
        vent_params   = _make_param_dict(vent_bld_df)

        elec_params   = _make_param_dict(elec_df)

        # Load base IDF
        idf = load_idf(base_idf_path, idd_path)

        # Apply HVAC
        apply_building_level_hvac(idf, hvac_params)
        apply_zone_level_hvac(idf, hvac_zone_df)

        # Apply DHW
        apply_dhw_params_to_idf(idf, dhw_params, suffix=f"Scenario_{i}")

        # Apply Vent
        if not vent_bld_df.empty or not vent_zone_df.empty:
            apply_building_level_vent(idf, vent_params)
            apply_zone_level_vent(idf, vent_zone_df)

        # Apply Elec => building-level or object-level
        if not elec_df.empty:
            apply_building_level_elec(idf, elec_params, zonelist_name="ALL_ZONES")
            # or use apply_object_level_elec(idf, elec_df) if you prefer

        # Apply Fenestration => object-level
        apply_object_level_fenez(idf, fenez_df)

        # NEW: Apply shading parameters
        apply_shading_params_to_idf(idf, shading_df)
        apply_equipment_params_to_idf(idf, equip_df)
        apply_zone_sizing_params_to_idf(idf, sizing_df)

        # Save scenario IDF
        scenario_idf_name = f"building_{building_id}_scenario_{i}.idf"
        scenario_idf_path = os.path.join(scenario_idf_dir, scenario_idf_name)
        save_idf(idf, scenario_idf_path)
        logger.info(f"[MODIFICATION] Saved scenario IDF => {scenario_idf_path}")

    logger.info("[MODIFICATION] All scenario IDFs generated successfully.")

    # -----------------------------------------------------------------------
    # 7) (Optional) Simulations
    # -----------------------------------------------------------------------
    if run_sims:
        logger.info("[MODIFICATION] Running E+ simulations for all scenario IDFs.")
        base_sim_dir = sim_cfg.get("output_dir", "output/Sim_Results/Scenarios")

        # if job_output_dir is given, we can make the sim results go inside it, too:
        if job_output_dir and not os.path.isabs(base_sim_dir):
            base_sim_dir = os.path.join(job_output_dir, base_sim_dir)
        os.makedirs(base_sim_dir, exist_ok=True)

        num_workers  = sim_cfg.get("num_workers", 4)

        run_all_idfs_in_folder(
            folder_path=scenario_idf_dir,
            iddfile=idd_path,
            base_output_dir=base_sim_dir,
            default_lat=52.15,
            default_lon=4.40,
            default_year=2020,
            num_workers=num_workers
        )

    # -----------------------------------------------------------------------
    # 8) (Optional) Post-processing
    # -----------------------------------------------------------------------
    if do_postproc:
        logger.info("[MODIFICATION] Performing post-processing merges.")

        base_sim_dir = sim_cfg.get("output_dir", "output/Sim_Results/Scenarios")
        if job_output_dir and not os.path.isabs(base_sim_dir):
            base_sim_dir = os.path.join(job_output_dir, base_sim_dir)

        output_csv_as_is = postproc_cfg.get("output_csv_as_is", "")
        output_csv_daily_mean = postproc_cfg.get("output_csv_daily_mean", "")

        # Build full paths inside job_output_dir if they are relative
        if output_csv_as_is:
            if job_output_dir and not os.path.isabs(output_csv_as_is):
                output_csv_as_is = os.path.join(job_output_dir, output_csv_as_is)
            os.makedirs(os.path.dirname(output_csv_as_is), exist_ok=True)

            merge_all_results(
                base_output_dir=base_sim_dir,
                output_csv=output_csv_as_is,
                convert_to_daily=False,
                convert_to_monthly=False
            )

        if output_csv_daily_mean:
            if job_output_dir and not os.path.isabs(output_csv_daily_mean):
                output_csv_daily_mean = os.path.join(job_output_dir, output_csv_daily_mean)
            os.makedirs(os.path.dirname(output_csv_daily_mean), exist_ok=True)

            merge_all_results(
                base_output_dir=base_sim_dir,
                output_csv=output_csv_daily_mean,
                convert_to_daily=True,
                daily_aggregator="mean",
                convert_to_monthly=False
            )

        logger.info("[MODIFICATION] Post-processing step complete.")

    # -----------------------------------------------------------------------
    # 9) (Optional) Validation
    # -----------------------------------------------------------------------
    if do_validation:
        logger.info("[MODIFICATION] Performing scenario validation with config => %s", validation_cfg)

        # If the validation config references CSV paths that might be relative,
        # you could also adjust them to be inside job_output_dir here if desired.
        # e.g.:
        # real_csv = validation_cfg.get("real_data_csv", "")
        # if job_output_dir and not os.path.isabs(real_csv):
        #     real_csv = os.path.join(job_output_dir, real_csv)
        # validation_cfg["real_data_csv"] = real_csv

        run_validation_process(validation_cfg)
        logger.info("[MODIFICATION] Validation step complete.")


def _make_param_dict(df_scenario):
    """
    Builds a dict {param_name: value} from the scenario DataFrame columns,
    checking 'assigned_value' or 'param_value'.
    """
    if df_scenario.empty:
        return {}

    cols = df_scenario.columns.tolist()
    if "assigned_value" in cols:
        val_col = "assigned_value"
    elif "param_value" in cols:
        val_col = "param_value"
    else:
        raise AttributeError(
            "No 'assigned_value' or 'param_value' column found in scenario dataframe! "
            f"Columns are: {cols}"
        )

    result = {}
    for row in df_scenario.itertuples():
        p_name = row.param_name
        raw_val = getattr(row, val_col)
        try:
            result[p_name] = float(raw_val)
        except (ValueError, TypeError):
            result[p_name] = raw_val
    return result

------------------------------------------------------------

File: D:\Documents\E_Plus_2030_py\combined.json
============================================================
{
  "dhw": [
    {
      "building_id": 413673000,
      "param_name": "occupant_density_m2_per_person",
      "min_val": 127.0,
      "max_val": 233.0
    },
    {
      "building_id": 413673000,
      "param_name": "liters_per_person_per_day",
      "fixed_value": 145.0
    },
    {
      "building_function": "residential00",
      "age_range": "1992-2005",
      "param_name": "setpoint_c",
      "min_val": 58.0,
      "max_val": 60.0
    },
    {
      "building_function": "non_residential000",
      "param_name": "occupant_density_m2_per_person",
      "min_val": 12.0,
      "max_val": 18.0
    }
  ],
  "epw": [
    {
      "building_id": 413673000,
      "fixed_epw_path": "data/weather/2050.epw"
    },
    {
      "desired_year": 2050,
      "override_year_to": 2020
    }
  ],
  "zone_sizing": [
    {
      "param_name": "cooling_supply_air_temp",
      "min_val": 12.0,
      "max_val": 15.0
    },
    {
      "param_name": "heating_supply_air_temp",
      "min_val": 40.0,
      "max_val": 50.0
    },
    {
      "param_name": "cooling_supply_air_hr",
      "min_val": 0.008,
      "max_val": 0.010
    },
    {
      "param_name": "heating_supply_air_hr",
      "min_val": 0.003,
      "max_val": 0.005
    },
    {
      "param_name": "cooling_design_air_flow_method",
      "choices": ["Flow/Zone", "DesignDay", "DesignDayWithLimit"]
    }
  ],
  "equipment": [
    {
      "param_name": "equip_wm2",
      "min_val": 3.0,
      "max_val": 7.0
    },
    {
      "param_name": "tD",
      "description": "Schedule value for day occupancy (W)",
      "min_val": 400.0,
      "max_val": 800.0
    },
    {
      "param_name": "tN",
      "description": "Schedule value for night occupancy (W)",
      "min_val": 150.0,
      "max_val": 350.0
    }
  ],
  "shading": [
    {
      "param_name": "slat_angle_deg",
      "min_val": 30.0,
      "max_val": 60.0
    },
    {
      "param_name": "slat_beam_solar_reflectance",
      "min_val": 0.6,
      "max_val": 0.8
    },
    {
      "param_name": "blind_to_glass_distance",
      "fixed_value": 0.075
    },
    {
      "param_name": "slat_width",
      "min_val": 0.020,
      "max_val": 0.030
    }
  ],
  "mail_user": [
    {
      "email": "amin.jalilzade1@gmail.com"
    }
  ],
  "fenestration": [
    {
      "building_id": 413673000,
      "building_function": "residential",
      "building_type": "Two-and-a-half-story House",
      "age_range": "1992-2005",
      "scenario": "scenario1",
      "param_name": "wwr",
      "min_val": 0.025,
      "max_val": 0.03
    },
    {
      "building_function": "non_residential0",
      "age_range": "2015 and later",
      "scenario": "scenario1",
      "param_name": "roof_R_value",
      "fixed_value": 3.0
    }
  ],
  "geometry": [
    {
      "building_id": 413673,
      "param_name": "perimeter_depth",
      "min_val": 3.5,
      "max_val": 4.0,
      "fixed_value": true
    },
    {
      "building_id": 4136733,
      "param_name": "has_core",
      "fixed_value": true
    },
    {
      "building_id": 4136737,
      "param_name": "has_core",
      "fixed_value": false
    },
    {
      "building_type": "Meeting Function",
      "param_name": "has_core",
      "fixed_value": true
    },
    {
      "building_id": 4136738,
      "param_name": "has_core",
      "fixed_value": false
    }

    
  ],
  "hvac": [
    {
      "building_id": 413673000,
      "param_name": "heating_day_setpoint",
      "min_val": 10.0,
      "max_val": 11.0
    },
    {
      "building_function": "residential0",
      "param_name": "cooling_day_setpoint",
      "fixed_value": 25.6
    },
    {
      "scenario": "scenario10",
      "age_range": "2015 and later",
      "param_name": "max_heating_supply_air_temp",
      "min_val": 48.6,
      "max_val": 50.0
    }
  ],
  "lighting": [
    {
      "building_id": 413673000,
      "param_name": "lights_wm2",
      "min_val": 18.0,
      "max_val": 20.0
    },
    {
      "building_type": "Residential0",
      "param_name": "tN",
      "min_val": 10999,
      "max_val": 11999
    },
    {
      "building_id": 4136731,
      "building_type": "non_residential",
      "param_name": "parasitic_wm2",
      "min_val": 0.28,
      "max_val": 0.3
    }
  ],
  "main_config": {
    "paths": {
      "building_data": "data/df_buildings.csv",
      "fenez_excel": "excel_data/envelop.xlsx",
      "dhw_excel": "excel_data/dhw_overrides.xlsx",
      "epw_excel": "excel_data/epw_overrides.xlsx",
      "lighting_excel": "excel_data/lighting_overrides.xlsx",
      "hvac_excel": "excel_data/hvac_overrides.xlsx",
      "vent_excel": "excel_data/vent_overrides.xlsx"
    },


    "use_database": false,
      "filter_by": "meestvoorkomendepostcode", 
      "db_filter": {
        "meestvoorkomendepostcode": ["3066HG"], 
        "pand_id": ["0383100000001369", "0383100000001370", "5473VX"],
        "pand_ids": ["XYZ123", "XYZ999"],
        "bbox_xy": [120000.0, 487000.0, 121000.0, 488000.0],
        "bbox_latlon": [51.873, 5.589, 51.878, 5.60]


    },
    "excel_overrides": {
      "override_fenez_excel": false,
      "override_dhw_excel": false,
      "override_epw_excel": false,
      "override_lighting_excel": false,
      "override_hvac_excel": false,
      "override_vent_excel": false
    },
    "user_config_overrides": {
      "override_fenez_json": true,
      "override_dhw_json": true,
      "override_epw_json": true,
      "override_lighting_json": true,
      "override_hvac_json": true,
      "override_vent_json": true,
      "override_geometry_json": true,
      "override_shading_json": false
    },
    "idf_creation": {
      "perform_idf_creation": true,
      "scenario": "scenario1",
      "calibration_stage": "pre_calibration",
      "strategy": "B",
      "random_seed": 42,
      "iddfile": "EnergyPlus/Energy+.idd",
      "idf_file_path": "EnergyPlus/Minimal.idf",
      "output_idf_dir": "output_IDFs",
      "run_simulations": true,
      "simulate_config": {
        "num_workers": 4,
        "ep_force_overwrite": true
      },
      "post_process": true,
      "post_process_config": {
        "base_output_dir": "Sim_Results",
        "outputs": [
          {
            "convert_to_daily": true,
            "convert_to_monthly": false,
            "aggregator": "none",
            "output_csv": "output/results/merged_as_is.csv"
          },
          {
            "convert_to_daily": true,
            "convert_to_monthly": false,
            "aggregator": "mean",
            "output_csv": "output/results/merged_daily_mean.csv"
          },
          {
            "convert_to_daily": true,
            "convert_to_monthly": true,
            "aggregator": "mean",
            "output_csv": "output/results/merged_monthly_mean.csv"
          }
        ]
      },
      "output_definitions": {
        "desired_variables": [
          "Facility Total Electric Demand Power",
          "Zone Air Temperature",
          "Boiler Heating Energy",
          "Water Heater Heating Energy"
        ],
        "desired_meters": [
          "Electricity:Facility",
          "Heating:EnergyTransfer",
          "Cooling:EnergyTransfer"
        ],
        "override_variable_frequency": "Daily",
        "override_meter_frequency": "Daily",
        "include_tables": true,
        "include_summary": true
      }

    },
    "structuring": {
      "perform_structuring": true,
      "dhw": {
        "csv_in": "assigned/assigned_dhw_params.csv",
        "csv_out": "assigned/structured_dhw_params.csv"
      },
      "fenestration": {
        "csv_in": "assigned/assigned_fenez_params.csv",
        "csv_out": "assigned/structured_fenez_params.csv"
      },
      "hvac": {
        "csv_in": "assigned/assigned_hvac_params.csv",
        "build_out": "assigned/assigned_hvac_building.csv",
        "zone_out": "assigned/assigned_hvac_zones.csv"
      },
      "vent": {
        "csv_in": "assigned/assigned_ventilation.csv",
        "build_out": "assigned/assigned_vent_building.csv",
        "zone_out": "assigned/assigned_vent_zones.csv"
      },
      "shading": {
        "csv_in": "assigned/assigned_shading_params.csv",
        "csv_out": "assigned/structured_shading_params.csv"        
      },
      "equipment": {
        "csv_in": "assigned/assigned_equipment.csv",
        "csv_out": "assigned/structured_equipment.csv"
    },
      "zone_sizing": {
        "csv_in": "assigned/assigned_zone_sizing_outdoor_air.csv",
        "csv_out": "assigned/structured_zone_sizing.csv"
    },
    "modification": {
      "perform_modification": true,
      "modify_config": {


        "idd_path": "EnergyPlus/Energy+.idd",
        "assigned_csv": {
          "hvac_building": "assigned/assigned_hvac_building.csv",
          "hvac_zones": "assigned/assigned_hvac_zones.csv",
          "dhw": "assigned/assigned_dhw_params.csv",
          "vent_build": "assigned/assigned_vent_building.csv",
          "vent_zones": "assigned/assigned_vent_zones.csv",
          "elec": "assigned/assigned_lighting.csv",
          "fenez": "assigned/structured_fenez_params.csv",
          "shading": "assigned/structured_shading_params.csv",
          "equip": "assigned/structured_equipment.csv",
          "zone_sizing": "assigned/structured_zone_sizing.csv"

        },
        "scenario_csv": {
          "hvac": "scenarios/scenario_params_hvac.csv",
          "dhw": "scenarios/scenario_params_dhw.csv",
          "vent": "scenarios/scenario_params_vent.csv",
          "elec": "scenarios/scenario_params_elec.csv",
          "fenez": "scenarios/scenario_params_fenez.csv",
          "shading": "scenarios/scenario_params_shading.csv",
          "equip": "scenarios/scenario_params_equipment.csv",
          "zone_sizing": "scenarios/scenario_params_zone_sizing.csv"

        },
        "output_idf_dir": "scenario_idfs",
        "building_id": 4136733,
        "num_scenarios": 5,
        "picking_method": "random_uniform",
        "picking_scale_factor": 0.5,
        "run_simulations": true,
        "simulation_config": {
          "num_workers": 4,
          "output_dir": "Sim_Results/Scenarios"
        },
        "perform_post_process": true,
        "post_process_config": {
          "output_csv_as_is": "results_scenarioes/merged_as_is_scenarios.csv",
          "output_csv_daily_mean": "results_scenarioes/merged_daily_mean_scenarios.csv"
        },
        "perform_validation": false,
        "validation_config": {
          "real_data_csv": "data/mock_merged_daily_mean.csv",
          "sim_data_csv":  "results/merged_daily_mean.csv",
          "bldg_ranges": {
            "0": [0, 1, 2]
          },
          "variables_to_compare": [
            "Electricity:Facility [J](Hourly)",
            "Heating:EnergyTransfer [J](Hourly)",
            "Cooling:EnergyTransfer [J](Hourly)"
          ],
          "threshold_cv_rmse": 30.0,
          "skip_plots": true,
          "output_csv": "scenario_validation_report.csv"
        }
      }
    },

    


  "validation_base": {
    "perform_validation": false,
    "config": {
      "real_data_csv": "data/mock_merged_daily_mean.csv",
      "sim_data_csv":  "results/merged_daily_mean.csv",
      "bldg_ranges": {
        "0": [0,1,2,3]
      },
      "variables_to_compare": [
        "Electricity:Facility [J](Hourly)",
        "Heating:EnergyTransfer [J](Hourly)",
        "Cooling:EnergyTransfer [J](Hourly)"
      ],
      "threshold_cv_rmse": 30.0,
      "skip_plots": true,
      "output_csv": "validation_report_base.csv"
    }
  },

  "validation_scenarios": {
    "perform_validation": false,
    "config": {
      "real_data_csv": "data/mock_merged_daily_mean.csv",
      "sim_data_csv":  "results_scenarioes/merged_daily_mean_scenarios.csv",
      "bldg_ranges": {
        "0": [0,1,2]
      },
      "variables_to_compare": [
        "Electricity:Facility [J](Hourly)",
        "Heating:EnergyTransfer [J](Hourly)",
        "Cooling:EnergyTransfer [J](Hourly)"
      ],
      "threshold_cv_rmse": 30.0,
      "skip_plots": true,
      "output_csv": "validation_report_scenarios.csv"
    }
  },








    "sensitivity": {
      "perform_sensitivity": true,
      "scenario_folder": "scenarios",
      "method": "morris",
      "results_csv": "results_scenarioes/merged_daily_mean_scenarios.csv",
      "target_variable": [
        "Heating:EnergyTransfer [J](Hourly)",
        "Cooling:EnergyTransfer [J](Hourly)",
        "Electricity:Facility [J](Hourly)"
      ],
      "output_csv": "multi_corr_sensitivity.csv",
      "n_morris_trajectories": 10,
      "num_levels": 4
    },
    "surrogate": {
      "perform_surrogate": true,
      "scenario_folder": "scenarios",
      "results_csv": "results_scenarioes/merged_daily_mean_scenarios.csv",
      "target_variable": "Heating:EnergyTransfer [J](Hourly)",
      "model_out": "heating_surrogate_model.joblib",
      "cols_out": "heating_surrogate_columns.joblib",
      "test_size": 0.3
    },
    "calibration": {
      "perform_calibration": true,
      "scenario_folder": "scenarios",
      "scenario_files": [
        "scenario_params_dhw.csv",
        "scenario_params_elec.csv"
      ],
      "subset_sensitivity_csv": "multi_corr_sensitivity.csv",
      "top_n_params": 10,
      "method": "ga",
      "use_surrogate": true,
      "real_data_csv": "data/mock_merged_daily_mean.csv",
      "surrogate_model_path": "heating_surrogate_model.joblib",
      "surrogate_columns_path": "heating_surrogate_columns.joblib",
      "calibrate_min_max": true,
      "ga_pop_size": 10,
      "ga_generations": 5,
      "ga_crossover_prob": 0.7,
      "ga_mutation_prob": 0.2,
      "bayes_n_calls": 15,
      "random_n_iter": 20,
      "output_history_csv": "calibration_history.csv",
      "best_params_folder": "calibrated",
      "history_folder": "calibrated"
    }
  }
  },
  "shading_tree": [
    {
      "building_id": 413673000,
      "param_name": "top_n_buildings",
      "fixed_value": 5
    },
    {
      "building_function": "residential",
      "param_name": "summer_value",
      "min_val": 0.4,
      "max_val": 0.6
    },
    {
      "param_name": "top_n_trees",
      "fixed_value": 3
    }
  ],
  "vent": [
    {
      "building_id": 413673000,
      "param_name": "infiltration_base",
      "min_val": 100.3,
      "max_val": 100.4
    },
    {
      "building_function": "residential",
      "age_range": "1992-2005",
      "param_name": "year_factor",
      "min_val": 21.4,
      "max_val": 21.5
    },
    {
      "building_id": 4136731,
      "param_name": "system_type",
      "fixed_value": "D"
    },
    {
      "building_id": 4136731,
      "param_name": "fan_pressure",
      "min_val": 100,
      "max_val": 120
    }
  ]
}

------------------------------------------------------------

