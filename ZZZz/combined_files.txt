File: D:\Documents\E_Plus_2030_py\app.py
============================================================
"""
app.py

A multi-endpoint, job-based Flask application to handle EnergyPlus workflows.

Endpoints:
  1) POST /jobs
     - Receives a combined JSON payload (like 'combined.json'),
       creates a new job with a unique job_id,
       splits the JSON into separate files (e.g., main_config.json, fenestration.json, etc.)
       in user_configs/<job_id>.
       Also creates an output subfolder output/<job_id>.
       Stores these paths in the job config.

  2) POST /jobs/<job_id>/start
     - Enqueues the job for execution (runs orchestrate_workflow in a background thread).

  3) GET /jobs/<job_id>/logs
     - Streams the logs in real time from the job_manager's log queue.

  4) POST /jobs/<job_id>/cancel
     - Attempts to cancel the job (if queued or running).

  5) GET /jobs/<job_id>/status
     - Returns the current status: CREATED, QUEUED, RUNNING, FINISHED, ERROR, or CANCELED.

  6) GET /jobs/<job_id>/results
     - Example endpoint to list or fetch final simulation results in output/<job_id>,
       or you can adapt to zip & return them. 
"""

import logging
import os

from flask import Flask, request, Response, jsonify

# ------------------------------------------------------------------------
# Import the job manager for concurrency, queue-based logs, etc.
# ------------------------------------------------------------------------
from job_manager import (
    create_job,
    enqueue_job,
    get_job,
    get_job_logs_queue,
    get_job_status,
    cancel_job
)

# ------------------------------------------------------------------------
# Import the splitter to break the combined JSON into sub-files
# ------------------------------------------------------------------------
from splitter import split_combined_json

###############################################################################
# Flask Application
###############################################################################
app = Flask(__name__)

###############################################################################
# 1) CREATE JOB - POST /jobs
###############################################################################
@app.route("/jobs", methods=["POST"])
def create_job_endpoint():
    """
    Accepts a JSON payload describing the entire workflow config (similar to 'combined.json').
    
    Steps performed:
      - Generate a new job_id (status=CREATED).
      - Create a subfolder for user_configs/<job_id> for the input config files.
      - Create a subfolder for output/<job_id> for the simulation results.
      - Split the posted JSON into multiple config files (e.g., main_config.json).
      - Store these paths in the job's config.
    
    Returns:
      JSON response: { "job_id": "<unique-uuid>" }
    """
    if not request.is_json:
        return jsonify({"error": "Expected JSON payload"}), 400

    # 1) Load the combined JSON from request
    posted_data = request.get_json()

    # 2) Create a job with an initially empty config
    job_id = create_job(config={})  
    # Note: create_job() sets job["config"]["job_id"] = job_id internally.

    # 3) Create a subfolder for user_configs/<job_id>
    #    This ensures concurrency-friendly isolation (no overwriting each other's files).
    base_user_configs = os.path.join(os.getcwd(), "user_configs")
    job_subfolder = os.path.join(base_user_configs, job_id)
    os.makedirs(job_subfolder, exist_ok=True)

    # 4) Create a corresponding output folder for this job's results: output/<job_id>
    base_output = os.path.join(os.getcwd(), "output")
    job_output_folder = os.path.join(base_output, job_id)
    os.makedirs(job_output_folder, exist_ok=True)

    # 5) Split the combined JSON into multiple files, each named <top_key>.json
    split_combined_json(posted_data, job_subfolder)

    # 6) Update the job config with the subfolder paths
    new_config = {
        # job_id is already stored, but you can re-store it if you want:
        "job_id": job_id,
        "job_subfolder": job_subfolder,
        "output_subfolder": job_output_folder,
        "posted_data": posted_data
    }

    job = get_job(job_id)
    if job:
        job["config"] = new_config

    # 7) Return the job_id so the user can call /jobs/<job_id>/start
    return jsonify({"job_id": job_id}), 200


###############################################################################
# 2) START JOB - POST /jobs/<job_id>/start
###############################################################################
@app.route("/jobs/<job_id>/start", methods=["POST"])
def start_job(job_id):
    """
    Moves the job from CREATED => RUNNING (or QUEUED if concurrency is max).
    The job_manager enqueue_job() will handle concurrency checks.

    Response:
      {
        "message": "Job enqueued or running",
        "job_id": "<the job_id>"
      }
    """
    job = get_job(job_id)
    if not job:
        return jsonify({"error": "No such job_id"}), 404

    # Enqueue or start the job
    enqueue_job(job_id)

    return jsonify({"message": "Job enqueued or running", "job_id": job_id}), 200


###############################################################################
# 3) STREAM LOGS - GET /jobs/<job_id>/logs
###############################################################################
@app.route("/jobs/<job_id>/logs", methods=["GET"])
def get_job_logs(job_id):
    """
    Streams the logs in real time from the job's log queue.
    The job_manager signals end of logs by sending a 'None' sentinel.

    Usage Example (PowerShell):
      curl.exe http://localhost:8000/jobs/<job_id>/logs
    """
    log_queue = get_job_logs_queue(job_id)
    if not log_queue:
        return jsonify({"error": "No such job or no logs queue"}), 404

    def log_stream():
        while True:
            line = log_queue.get()
            if line is None:
                break
            yield line + "\n"

    return Response(log_stream(), mimetype="text/plain")


###############################################################################
# 4) CANCEL JOB - POST /jobs/<job_id>/cancel
###############################################################################
@app.route("/jobs/<job_id>/cancel", methods=["POST"])
def cancel_job_endpoint(job_id):
    """
    Attempts to cancel the job:
      - If QUEUED, remove from queue -> status=CANCELED
      - If RUNNING, set cancel_event -> orchestrator can gracefully stop if it checks for cancellations
      - If FINISHED/ERROR/CANCELED, no effect

    Response:
      { "message": "Job <job_id> canceled" } or { "error": ... }
    """
    success = cancel_job(job_id)
    if success:
        return jsonify({"message": f"Job {job_id} canceled"}), 200
    else:
        return jsonify({"error": "Could not cancel (job not found or already finished/canceled)"}), 400


###############################################################################
# 5) JOB STATUS - GET /jobs/<job_id>/status
###############################################################################
@app.route("/jobs/<job_id>/status", methods=["GET"])
def get_job_status_endpoint(job_id):
    """
    Returns the current status of the job:
      - CREATED
      - QUEUED
      - RUNNING
      - FINISHED
      - ERROR
      - CANCELED

    Response:
      {
        "job_id": "<job_id>",
        "status": "<STATUS>"
      }
    """
    status = get_job_status(job_id)
    if status is None:
        return jsonify({"error": "No such job_id"}), 404

    return jsonify({"job_id": job_id, "status": status}), 200


###############################################################################
# 6) GET /jobs/<job_id>/results
###############################################################################
@app.route("/jobs/<job_id>/results", methods=["GET"])
def get_job_results(job_id):
    """
    (Placeholder endpoint)
    If your orchestrator or IDF creation writes final outputs to 
    output/<job_id> (or some subfolder), you can retrieve or list them here.
    
    Example usage:
      - Check existence of job config to get "output_subfolder".
      - Possibly zip the folder or return direct file.
    """
    job = get_job(job_id)
    if not job:
        return jsonify({"error": "No such job"}), 404

    cfg = job["config"]
    out_dir = cfg.get("output_subfolder")
    if not out_dir or not os.path.exists(out_dir):
        return jsonify({"error": f"No results found for job {job_id}"}), 404

    # For now, just respond with a simple message:
    return jsonify({
        "message": f"Results directory is: {out_dir} (Implement your own listing or zip download)."
    }), 200


###############################################################################
# MAIN - For local dev
###############################################################################
if __name__ == "__main__":
    """
    In local dev, run Flask directly:
      python app.py

    For production usage, it's recommended to run:
      gunicorn -b 0.0.0.0:8000 app:app --workers=4
    from your Dockerfile or docker-compose.
    """
    app.run(host="0.0.0.0", port=8000, debug=True)

------------------------------------------------------------

File: D:\Documents\E_Plus_2030_py\orchestrator.py
============================================================
"""
orchestrator.py

Orchestrates the entire EnergyPlus workflow using a job-specific subfolder
for config files and a job-specific folder in /output for results.

Steps:
  1. Retrieve 'job_id' from job_config (set by job_manager or app).
  2. Form an output directory: <OUTPUT_DIR>/<job_id>.
  3. Load main_config.json from user_configs/<job_id>.
  4. Merge with posted_data["main_config"] if present.
  5. Apply Excel overrides, JSON overrides, create IDFs, run sims, etc.
  6. If scenario modification is enabled, override paths so scenario IDFs/results
     stay in the same job folder, then run scenario-based modifications.
  7. Perform structuring (e.g., flatten assigned CSVs) if requested.
  8. Perform global validation, sensitivity, surrogate, calibration if requested;
     patch any relative CSV paths to be inside the job folder (unless "data/").
  9. Zip & email final results if mail_user.json is present.
  10. Respect any cancel_event from job_manager.
"""

import os
import json
import logging
import threading
import pandas as pd

# Splitting / deep-merge
from splitter import deep_merge_dicts

# DB loading if needed
from database_handler import load_buildings_from_db

# Excel overrides
from excel_overrides import (
    override_dhw_lookup_from_excel_file,
    override_epw_lookup_from_excel_file,
    override_lighting_lookup_from_excel_file,
    override_hvac_lookup_from_excel_file,
    override_vent_lookup_from_excel_file
)

# Fenestration config
from idf_objects.fenez.fenez_config_manager import build_fenez_config

# IDF creation
import idf_creation
from idf_creation import create_idfs_for_all_buildings

# Scenario modification
from main_modifi import run_modification_workflow

# Validation
from validation.main_validation import run_validation_process

# Sensitivity / Surrogate / Calibration
from cal.unified_sensitivity import run_sensitivity_analysis
from cal.unified_surrogate import (
    load_scenario_params as sur_load_scenario_params,
    pivot_scenario_params,
    load_sim_results,
    aggregate_results,
    merge_params_with_results,
    build_and_save_surrogate
)
from cal.unified_calibration import run_unified_calibration

# Zip & email
from zip_and_mail import zip_user_output, send_results_email

from cleanup_old_jobs import cleanup_old_results


class WorkflowCanceled(Exception):
    """Custom exception used to stop the workflow if a cancel_event is set."""
    pass


def orchestrate_workflow(job_config: dict, cancel_event: threading.Event = None):
    """
    Orchestrates the entire E+ workflow using a job-specific subfolder for config JSON,
    plus a job-specific output folder for results.

    Args:
        job_config (dict): includes:
            {
              "job_id": "<unique_id_for_this_job>",
              "job_subfolder": "user_configs/<job_id>",
              "posted_data": {...} (optional),
              ...
            }
        cancel_event (threading.Event): If set, we gracefully exit early.

    Raises:
        WorkflowCanceled if cancel_event is set mid-way.

    Returns:
        None (logs extensively, optionally zips/emails final results).
    """
    logger = logging.getLogger(__name__)
    logger.info("=== Starting orchestrate_workflow ===")

    # -------------------------------------------------------------------------
    # 0) Identify job_id, define check_canceled
    # -------------------------------------------------------------------------
    job_id = job_config.get("job_id", "unknown_job_id")
    logger.info(f"[INFO] Orchestrator for job_id={job_id}")

    def check_canceled():
        """Raise WorkflowCanceled if cancel_event is set."""
        if cancel_event and cancel_event.is_set():
            logger.warning("=== CANCEL event detected. Stopping workflow. ===")
            raise WorkflowCanceled("Workflow was canceled by user request.")

    # -------------------------------------------------------------------------
    # 1) Identify the user_configs folder (where main_config.json resides)
    # -------------------------------------------------------------------------
    user_configs_folder = job_config.get("job_subfolder")
    if not user_configs_folder or not os.path.isdir(user_configs_folder):
        logger.error(f"[ERROR] job_subfolder not found or invalid => {user_configs_folder}")
        return

    # -------------------------------------------------------------------------
    # 2) Build an output directory for this job under OUTPUT_DIR
    #    e.g. /usr/src/app/output/<job_id>
    # -------------------------------------------------------------------------
    env_out_dir = os.environ.get("OUTPUT_DIR", "/usr/src/app/output")
    job_output_dir = os.path.join(env_out_dir, job_id)
    os.makedirs(job_output_dir, exist_ok=True)
    logger.info(f"[INFO] Using job-specific output folder: {job_output_dir}")

    # -------------------------------------------------------------------------
    # 3) Load main_config.json from user_configs/<job_id>
    # -------------------------------------------------------------------------
    main_config_path = os.path.join(user_configs_folder, "main_config.json")
    if not os.path.isfile(main_config_path):
        logger.error(f"[ERROR] Cannot find main_config.json at {main_config_path}")
        return

    with open(main_config_path, "r") as f:
        existing_config_raw = json.load(f)
    main_config = existing_config_raw.get("main_config", {})
    logger.info(f"[INFO] Loaded existing main_config from {main_config_path}.")

    # Merge posted_data["main_config"] if present
    posted_data = job_config.get("posted_data", {})
    if "main_config" in posted_data:
        logger.info("[INFO] Deep merging posted_data['main_config'] into main_config.")
        deep_merge_dicts(main_config, posted_data["main_config"])
        # optionally re-save
        with open(main_config_path, "w") as f:
            json.dump({"main_config": main_config}, f, indent=2)

    # -------------------------------------------------------------------------
    # 4) Extract sub-sections from main_config
    # -------------------------------------------------------------------------
    check_canceled()
    paths_dict       = main_config.get("paths", {})
    excel_flags      = main_config.get("excel_overrides", {})
    user_flags       = main_config.get("user_config_overrides", {})
    def_dicts        = main_config.get("default_dicts", {})
    structuring_cfg  = main_config.get("structuring", {})
    modification_cfg = main_config.get("modification", {})
    validation_cfg   = main_config.get("validation", {})
    sens_cfg         = main_config.get("sensitivity", {})
    sur_cfg          = main_config.get("surrogate", {})
    cal_cfg          = main_config.get("calibration", {})

    # IDF creation block
    idf_cfg = main_config.get("idf_creation", {})
    perform_idf_creation = idf_cfg.get("perform_idf_creation", False)
    scenario             = idf_cfg.get("scenario", "scenario1")
    calibration_stage    = idf_cfg.get("calibration_stage", "pre_calibration")
    strategy             = idf_cfg.get("strategy", "B")
    random_seed          = idf_cfg.get("random_seed", 42)
    run_simulations      = idf_cfg.get("run_simulations", True)
    simulate_config      = idf_cfg.get("simulate_config", {})
    post_process         = idf_cfg.get("post_process", True)
    post_process_config  = idf_cfg.get("post_process_config", {})
    output_definitions   = idf_cfg.get("output_definitions", {})
    use_database         = main_config.get("use_database", False)
    db_filter            = main_config.get("db_filter", {})
    filter_by            = main_config.get("filter_by")  # if using DB

    # -------------------------------------------------------------------------
    # 5) Possibly override idf_creation.idf_config from env, then force IDFs
    #    to go in <job_output_dir>/output_IDFs
    # -------------------------------------------------------------------------
    check_canceled()

    env_idd_path = os.environ.get("IDD_PATH")
    if env_idd_path:
        idf_creation.idf_config["iddfile"] = env_idd_path
    env_base_idf = os.environ.get("BASE_IDF_PATH")
    if env_base_idf:
        idf_creation.idf_config["idf_file_path"] = env_base_idf

    job_idf_dir = os.path.join(job_output_dir, "output_IDFs")
    os.makedirs(job_idf_dir, exist_ok=True)
    idf_creation.idf_config["output_dir"] = job_idf_dir

    # If user explicitly set these in main_config, override again
    if "iddfile" in idf_cfg:
        idf_creation.idf_config["iddfile"] = idf_cfg["iddfile"]
    if "idf_file_path" in idf_cfg:
        idf_creation.idf_config["idf_file_path"] = idf_cfg["idf_file_path"]

    if "output_idf_dir" in idf_cfg:
        subfolder = idf_cfg["output_idf_dir"]  # e.g. "output_IDFs"
        full_dir = os.path.join(job_output_dir, subfolder)
        idf_creation.idf_config["output_dir"] = full_dir
    else:
        idf_creation.idf_config["output_dir"] = os.path.join(job_output_dir, "output_IDFs")

    # -------------------------------------------------------------------------
    # 6) Setup default dictionaries
    # -------------------------------------------------------------------------
    base_res_data    = def_dicts.get("res_data", {})
    base_nonres_data = def_dicts.get("nonres_data", {})
    dhw_lookup       = def_dicts.get("dhw", {})
    epw_lookup       = def_dicts.get("epw", [])
    lighting_lookup  = def_dicts.get("lighting", {})
    hvac_lookup      = def_dicts.get("hvac", {})
    vent_lookup      = def_dicts.get("vent", {})

    # -------------------------------------------------------------------------
    # 7) Apply Excel overrides if flags are set
    # -------------------------------------------------------------------------
    check_canceled()

    updated_res_data, updated_nonres_data = build_fenez_config(
        base_res_data=base_res_data,
        base_nonres_data=base_nonres_data,
        excel_path=paths_dict.get("fenez_excel", ""),
        do_excel_override=excel_flags.get("override_fenez_excel", False),
        user_fenez_overrides=[]
    )

    if excel_flags.get("override_dhw_excel", False):
        dhw_lookup = override_dhw_lookup_from_excel_file(
            dhw_excel_path=paths_dict.get("dhw_excel", ""),
            default_dhw_lookup=dhw_lookup,
            override_dhw_flag=True
        )

    if excel_flags.get("override_epw_excel", False):
        epw_lookup = override_epw_lookup_from_excel_file(
            epw_excel_path=paths_dict.get("epw_excel", ""),
            epw_lookup=epw_lookup,
            override_epw_flag=True
        )

    if excel_flags.get("override_lighting_excel", False):
        lighting_lookup = override_lighting_lookup_from_excel_file(
            lighting_excel_path=paths_dict.get("lighting_excel", ""),
            lighting_lookup=lighting_lookup,
            override_lighting_flag=True
        )

    if excel_flags.get("override_hvac_excel", False):
        hvac_lookup = override_hvac_lookup_from_excel_file(
            hvac_excel_path=paths_dict.get("hvac_excel", ""),
            hvac_lookup=hvac_lookup,
            override_hvac_flag=True
        )

    if excel_flags.get("override_vent_excel", False):
        vent_lookup = override_vent_lookup_from_excel_file(
            vent_excel_path=paths_dict.get("vent_excel", ""),
            vent_lookup=vent_lookup,
            override_vent_flag=True
        )

    # -------------------------------------------------------------------------
    # 8) JSON overrides from user_configs/<job_id> if user_flags are set
    # -------------------------------------------------------------------------
    check_canceled()

    def safe_load_subjson(fname, key):
        """
        Loads user_configs/<job_id>/fname if it exists, returns data.get(key).
        """
        full_path = os.path.join(user_configs_folder, fname)
        if os.path.isfile(full_path):
            try:
                with open(full_path, "r") as ff:
                    data = json.load(ff)
                return data.get(key)
            except Exception as e:
                logger.error(f"[ERROR] loading {fname} => {e}")
        return None

    # Fenestration
    user_fenez_data = []
    if user_flags.get("override_fenez_json", False):
        loaded = safe_load_subjson("fenestration.json", "fenestration")
        if loaded:
            user_fenez_data = loaded

    updated_res_data, updated_nonres_data = build_fenez_config(
        base_res_data=updated_res_data,
        base_nonres_data=updated_nonres_data,
        excel_path="",
        do_excel_override=False,
        user_fenez_overrides=user_fenez_data
    )

    # DHW
    user_config_dhw = None
    if user_flags.get("override_dhw_json", False):
        user_config_dhw = safe_load_subjson("dhw.json", "dhw")

    # EPW
    user_config_epw = []
    if user_flags.get("override_epw_json", False):
        e = safe_load_subjson("epw.json", "epw")
        if e:
            user_config_epw = e

    # Lighting
    user_config_lighting = None
    if user_flags.get("override_lighting_json", False):
        user_config_lighting = safe_load_subjson("lighting.json", "lighting")

    # HVAC
    user_config_hvac = None
    if user_flags.get("override_hvac_json", False):
        user_config_hvac = safe_load_subjson("hvac.json", "hvac")

    # Vent
    user_config_vent = []
    if user_flags.get("override_vent_json", False):
        v = safe_load_subjson("vent.json", "vent")
        if v:
            user_config_vent = v

    # Geometry
    geom_data = {}
    if user_flags.get("override_geometry_json", False):
        g = safe_load_subjson("geometry.json", "geometry")
        if g:
            geom_data["geometry"] = g

    # Shading
    shading_data = {}
    if user_flags.get("override_shading_json", False):
        s = safe_load_subjson("shading.json", "shading")
        if s:
            shading_data["shading"] = s

    # -------------------------------------------------------------------------
    # 9) IDF creation
    # -------------------------------------------------------------------------
    check_canceled()
    df_buildings = pd.DataFrame()

    if perform_idf_creation:
        logger.info("[INFO] IDF creation is ENABLED.")

        # a) Load building data
        if use_database:
            logger.info("[INFO] Loading building data from DB.")
            if not filter_by:
                raise ValueError("[ERROR] 'filter_by' must be specified when 'use_database' is True.")
            df_buildings = load_buildings_from_db(db_filter, filter_by)

            # Optionally save the raw DB buildings
            extracted_csv_path = os.path.join(job_output_dir, "extracted_buildings.csv")
            df_buildings.to_csv(extracted_csv_path, index=False)
            logger.info(f"[INFO] Saved extracted buildings to {extracted_csv_path}")

        else:
            bldg_data_path = paths_dict.get("building_data", "")
            if os.path.isfile(bldg_data_path):
                df_buildings = pd.read_csv(bldg_data_path)
            else:
                logger.warning(f"[WARN] building_data CSV not found => {bldg_data_path}")

        # b) Create IDFs & (optionally) run sims in job folder
        df_buildings = create_idfs_for_all_buildings(
            df_buildings=df_buildings,
            scenario=scenario,
            calibration_stage=calibration_stage,
            strategy=strategy,
            random_seed=random_seed,
            user_config_geom=geom_data.get("geometry", []),
            user_config_lighting=user_config_lighting,
            user_config_dhw=user_config_dhw,
            res_data=updated_res_data,
            nonres_data=updated_nonres_data,
            user_config_hvac=user_config_hvac,
            user_config_vent=user_config_vent,
            user_config_epw=user_config_epw,
            output_definitions=output_definitions,
            run_simulations=run_simulations,
            simulate_config=simulate_config,
            post_process=post_process,
            post_process_config=post_process_config,
            logs_base_dir=job_output_dir
        )

        # === Store the mapping (ogc_fid -> idf_name) so we can look it up later ===
        idf_map_csv = os.path.join(job_output_dir, "extracted_idf_buildings.csv")
        df_buildings.to_csv(idf_map_csv, index=False)
        logger.info(f"[INFO] Wrote building -> IDF map to {idf_map_csv}")

    else:
        logger.info("[INFO] Skipping IDF creation.")

    # -------------------------------------------------------------------------
    # 10) Perform structuring if requested
    # -------------------------------------------------------------------------
    check_canceled()
    if structuring_cfg.get("perform_structuring", False):
        logger.info("[INFO] Performing structuring ...")

        # Example: Fenestration
        from idf_objects.structuring.fenestration_structuring import transform_fenez_log_to_structured_with_ranges
        fenez_conf = structuring_cfg.get("fenestration", {})
        fenez_in   = fenez_conf.get("csv_in",  "assigned/assigned_fenez_params.csv")
        fenez_out  = fenez_conf.get("csv_out", "assigned/structured_fenez_params.csv")

        if not os.path.isabs(fenez_in):
            fenez_in = os.path.join(job_output_dir, fenez_in)
        if not os.path.isabs(fenez_out):
            fenez_out = os.path.join(job_output_dir, fenez_out)

        if os.path.isfile(fenez_in):
            transform_fenez_log_to_structured_with_ranges(csv_input=fenez_in, csv_output=fenez_out)
        else:
            logger.warning(f"[STRUCTURING] Fenestration input CSV not found => {fenez_in}")

        # Example: DHW
        from idf_objects.structuring.dhw_structuring import transform_dhw_log_to_structured
        dhw_conf = structuring_cfg.get("dhw", {})
        dhw_in   = dhw_conf.get("csv_in",  "assigned/assigned_dhw_params.csv")
        dhw_out  = dhw_conf.get("csv_out", "assigned/structured_dhw_params.csv")

        if not os.path.isabs(dhw_in):
            dhw_in = os.path.join(job_output_dir, dhw_in)
        if not os.path.isabs(dhw_out):
            dhw_out = os.path.join(job_output_dir, dhw_out)

        if os.path.isfile(dhw_in):
            transform_dhw_log_to_structured(dhw_in, dhw_out)
        else:
            logger.warning(f"[STRUCTURING] DHW input CSV not found => {dhw_in}")

        # Example: HVAC flatten
        from idf_objects.structuring.flatten_hvac import flatten_hvac_data, parse_assigned_value as parse_hvac
        hvac_conf = structuring_cfg.get("hvac", {})
        hvac_in   = hvac_conf.get("csv_in",  "assigned/assigned_hvac_params.csv")
        hvac_bld  = hvac_conf.get("build_out", "assigned/assigned_hvac_building.csv")
        hvac_zone = hvac_conf.get("zone_out",  "assigned/assigned_hvac_zones.csv")

        if not os.path.isabs(hvac_in):
            hvac_in = os.path.join(job_output_dir, hvac_in)
        if not os.path.isabs(hvac_bld):
            hvac_bld = os.path.join(job_output_dir, hvac_bld)
        if not os.path.isabs(hvac_zone):
            hvac_zone = os.path.join(job_output_dir, hvac_zone)

        if os.path.isfile(hvac_in):
            df_hvac = pd.read_csv(hvac_in)
            df_hvac["assigned_value"] = df_hvac["assigned_value"].apply(parse_hvac)
            flatten_hvac_data(
                df_input=df_hvac,
                out_build_csv=hvac_bld,
                out_zone_csv=hvac_zone
            )
        else:
            logger.warning(f"[STRUCTURING] HVAC input CSV not found => {hvac_in}")

        # Example: Vent flatten
        from idf_objects.structuring.flatten_assigned_vent import flatten_ventilation_data, parse_assigned_value as parse_vent
        vent_conf = structuring_cfg.get("vent", {})
        vent_in   = vent_conf.get("csv_in", "assigned/assigned_ventilation.csv")
        vent_bld  = vent_conf.get("build_out", "assigned/assigned_vent_building.csv")
        vent_zone = vent_conf.get("zone_out", "assigned/assigned_vent_zones.csv")

        if not os.path.isabs(vent_in):
            vent_in = os.path.join(job_output_dir, vent_in)
        if not os.path.isabs(vent_bld):
            vent_bld = os.path.join(job_output_dir, vent_bld)
        if not os.path.isabs(vent_zone):
            vent_zone = os.path.join(job_output_dir, vent_zone)

        if os.path.isfile(vent_in):
            df_vent = pd.read_csv(vent_in)
            df_vent["assigned_value"] = df_vent["assigned_value"].apply(parse_vent)
            flatten_ventilation_data(
                df_input=df_vent,
                out_build_csv=vent_bld,
                out_zone_csv=vent_zone
            )
        else:
            logger.warning(f"[STRUCTURING] Vent input CSV not found => {vent_in}")

    else:
        logger.info("[INFO] Skipping structuring.")

    # -------------------------------------------------------------------------
    # 11) Scenario Modification
    # -------------------------------------------------------------------------
    check_canceled()
    if modification_cfg.get("perform_modification", False):
        logger.info("[INFO] Scenario modification is ENABLED.")

        mod_cfg = modification_cfg["modify_config"]

        # 1) Ensure scenario IDFs go to <job_output_dir>/scenario_idfs
        scenario_idf_dir = os.path.join(job_output_dir, "scenario_idfs")
        os.makedirs(scenario_idf_dir, exist_ok=True)
        mod_cfg["output_idf_dir"] = scenario_idf_dir

        # 2) Ensure scenario sims => <job_output_dir>/Sim_Results/Scenarios
        if "simulation_config" in mod_cfg:
            sim_out = os.path.join(job_output_dir, "Sim_Results", "Scenarios")
            os.makedirs(sim_out, exist_ok=True)
            mod_cfg["simulation_config"]["output_dir"] = sim_out

        # 3) Post-process => <job_output_dir>/results_scenarioes
        if "post_process_config" in mod_cfg:
            ppcfg = mod_cfg["post_process_config"]
            as_is_csv = os.path.join(job_output_dir, "results_scenarioes", "merged_as_is_scenarios.csv")
            daily_csv = os.path.join(job_output_dir, "results_scenarioes", "merged_daily_mean_scenarios.csv")
            os.makedirs(os.path.dirname(as_is_csv), exist_ok=True)
            os.makedirs(os.path.dirname(daily_csv), exist_ok=True)
            ppcfg["output_csv_as_is"] = as_is_csv
            ppcfg["output_csv_daily_mean"] = daily_csv

        # 4) Fix assigned_csv paths
        assigned_csv_dict = mod_cfg.get("assigned_csv", {})
        for key, rel_path in assigned_csv_dict.items():
            assigned_csv_dict[key] = os.path.join(job_output_dir, rel_path)

        # 5) Fix scenario_csv paths
        scenario_csv_dict = mod_cfg.get("scenario_csv", {})
        for key, rel_path in scenario_csv_dict.items():
            scenario_csv_dict[key] = os.path.join(job_output_dir, rel_path)

        # ----------------------------------------------------------------------
        # NEW LOGIC: pick the base_idf_path from building_id automatically
        # ----------------------------------------------------------------------
        # The user sets "building_id" in the config, e.g. 20233330
        building_id = mod_cfg["building_id"]

        # We need the CSV that was saved right after create_idfs_for_all_buildings(...)
        idf_map_csv = os.path.join(job_output_dir, "extracted_idf_buildings.csv")
        if not os.path.isfile(idf_map_csv):
            raise FileNotFoundError(
                f"Cannot find building->IDF map CSV at {idf_map_csv}. "
                f"Did you skip 'perform_idf_creation'?"
            )

        # Read the mapping: each row has "ogc_fid" and "idf_name"
        df_idf_map = pd.read_csv(idf_map_csv)
        row_match = df_idf_map.loc[df_idf_map["ogc_fid"] == building_id]

        if row_match.empty:
            raise ValueError(
                f"No building found for building_id={building_id} in {idf_map_csv}"
            )

        # e.g. "building_0.idf", "building_16.idf", "building_16_ba62d0.idf", etc.
        idf_filename = row_match.iloc[0]["idf_name"]

        # Build the full path to that IDF in output_IDFs
        base_idf_path = os.path.join(job_output_dir, "output_IDFs", idf_filename)
        mod_cfg["base_idf_path"] = base_idf_path
        logger.info(f"[INFO] Auto-selected base IDF => {base_idf_path}")
        # ----------------------------------------------------------------------

        # Finally, run the scenario workflow
        run_modification_workflow(mod_cfg)
    else:
        logger.info("[INFO] Skipping scenario modification.")


    # -------------------------------------------------------------------------
    # 12) Helper to handle patching CSVs that are "relative" but not "data/".
    # -------------------------------------------------------------------------
    def patch_if_relative(csv_path: str):
        """
        1) If absolute, return as-is.
        2) If starts with 'data/', interpret as /usr/src/app/data/... (no job folder).
        3) Else, join with job_output_dir.
        """
        if not csv_path:
            return csv_path
        if os.path.isabs(csv_path):
            return csv_path
        if csv_path.startswith("data/"):
            return os.path.join("/usr/src/app", csv_path)
        return os.path.join(job_output_dir, csv_path)

    # -------------------------------------------------------------------------
    # 13) Global Validation
    # -------------------------------------------------------------------------
        # (A) Validation - BASE
    # -------------------------------------------------------------------------
    check_canceled()
    base_validation_cfg = main_config.get("validation_base", {})
    if base_validation_cfg.get("perform_validation", False):
        logger.info("[INFO] BASE Validation is ENABLED.")
        val_conf = base_validation_cfg["config"]

        # Patch relative paths
        sim_csv = val_conf.get("sim_data_csv")
        if sim_csv:
            val_conf["sim_data_csv"] = patch_if_relative(sim_csv)

        real_csv = val_conf.get("real_data_csv")
        if real_csv:
            val_conf["real_data_csv"] = patch_if_relative(real_csv)

        out_csv = val_conf.get("output_csv")
        if out_csv:
            val_conf["output_csv"] = patch_if_relative(out_csv)

        # Now run the validation
        run_validation_process(val_conf)
    else:
        logger.info("[INFO] Skipping BASE validation or not requested.")

    # (B) Validation - SCENARIOS
    # -------------------------------------------------------------------------
    check_canceled()
    scenario_validation_cfg = main_config.get("validation_scenarios", {})
    if scenario_validation_cfg.get("perform_validation", False):
        logger.info("[INFO] SCENARIO Validation is ENABLED.")
        val_conf = scenario_validation_cfg["config"]

        # Patch relative paths
        sim_csv = val_conf.get("sim_data_csv")
        if sim_csv:
            val_conf["sim_data_csv"] = patch_if_relative(sim_csv)

        real_csv = val_conf.get("real_data_csv")
        if real_csv:
            val_conf["real_data_csv"] = patch_if_relative(real_csv)

        out_csv = val_conf.get("output_csv")
        if out_csv:
            val_conf["output_csv"] = patch_if_relative(out_csv)

        # Now run the validation
        run_validation_process(val_conf)
    else:
        logger.info("[INFO] Skipping SCENARIO validation or not requested.")


    # -------------------------------------------------------------------------
    # 14) Sensitivity Analysis
    # -------------------------------------------------------------------------
    check_canceled()
    if sens_cfg.get("perform_sensitivity", False):
        logger.info("[INFO] Sensitivity Analysis is ENABLED.")

        scenario_folder = sens_cfg.get("scenario_folder", "")
        sens_cfg["scenario_folder"] = patch_if_relative(scenario_folder)

        results_csv = sens_cfg.get("results_csv", "")
        sens_cfg["results_csv"] = patch_if_relative(results_csv)

        out_csv = sens_cfg.get("output_csv", "sensitivity_output.csv")
        sens_cfg["output_csv"] = patch_if_relative(out_csv)

        run_sensitivity_analysis(
            scenario_folder=sens_cfg["scenario_folder"],
            method=sens_cfg["method"],
            results_csv=sens_cfg.get("results_csv", ""),
            target_variable=sens_cfg.get("target_variable", []),
            output_csv=sens_cfg.get("output_csv", "sensitivity_output.csv"),
            n_morris_trajectories=sens_cfg.get("n_morris_trajectories", 10),
            num_levels=sens_cfg.get("num_levels", 4),
            n_sobol_samples=sens_cfg.get("n_sobol_samples", 128)
        )
    else:
        logger.info("[INFO] Skipping sensitivity analysis.")

    # -------------------------------------------------------------------------
    # 15) Surrogate Modeling
    # -------------------------------------------------------------------------
    check_canceled()
    if sur_cfg.get("perform_surrogate", False):
        logger.info("[INFO] Surrogate Modeling is ENABLED.")

        scenario_folder = sur_cfg.get("scenario_folder", "")
        sur_cfg["scenario_folder"] = patch_if_relative(scenario_folder)

        results_csv = sur_cfg.get("results_csv", "")
        sur_cfg["results_csv"] = patch_if_relative(results_csv)

        model_out = sur_cfg.get("model_out", "")
        sur_cfg["model_out"] = patch_if_relative(model_out)

        cols_out = sur_cfg.get("cols_out", "")
        sur_cfg["cols_out"] = patch_if_relative(cols_out)

        target_var = sur_cfg["target_variable"]
        test_size  = sur_cfg["test_size"]

        df_scen = sur_load_scenario_params(sur_cfg["scenario_folder"])
        pivot_df = pivot_scenario_params(df_scen)

        df_sim = load_sim_results(sur_cfg["results_csv"])
        df_agg = aggregate_results(df_sim)
        merged_df = merge_params_with_results(pivot_df, df_agg, target_var)

        rf_model, trained_cols = build_and_save_surrogate(
            df_data=merged_df,
            target_col=target_var,
            model_out_path=sur_cfg["model_out"],
            columns_out_path=sur_cfg["cols_out"],
            test_size=test_size,
            random_state=42
        )
        if rf_model:
            logger.info("[INFO] Surrogate model built & saved.")
        else:
            logger.warning("[WARN] Surrogate modeling failed or insufficient data.")
    else:
        logger.info("[INFO] Skipping surrogate modeling.")

    # -------------------------------------------------------------------------
    # 16) Calibration
    # -------------------------------------------------------------------------
    check_canceled()
    if cal_cfg.get("perform_calibration", False):
        logger.info("[INFO] Calibration is ENABLED.")

        scen_folder = cal_cfg.get("scenario_folder", "")
        cal_cfg["scenario_folder"] = patch_if_relative(scen_folder)

        real_csv = cal_cfg.get("real_data_csv", "")
        cal_cfg["real_data_csv"] = patch_if_relative(real_csv)

        sur_model_path = cal_cfg.get("surrogate_model_path", "")
        cal_cfg["surrogate_model_path"] = patch_if_relative(sur_model_path)

        sur_cols_path = cal_cfg.get("surrogate_columns_path", "")
        cal_cfg["surrogate_columns_path"] = patch_if_relative(sur_cols_path)

        hist_csv = cal_cfg.get("output_history_csv", "")
        cal_cfg["output_history_csv"] = patch_if_relative(hist_csv)

        best_params_folder = cal_cfg.get("best_params_folder", "")
        cal_cfg["best_params_folder"] = patch_if_relative(best_params_folder)

        run_unified_calibration(cal_cfg)
    else:
        logger.info("[INFO] Skipping calibration.")

    # -------------------------------------------------------------------------
    # 17) Zip & Email final results, if mail_user.json present
    # -------------------------------------------------------------------------
    try:
        mail_user_path = os.path.join(user_configs_folder, "mail_user.json")
        if os.path.isfile(mail_user_path):
            with open(mail_user_path, "r") as f:
                mail_info = json.load(f)

            mail_user_list = mail_info.get("mail_user", [])
            if len(mail_user_list) > 0:
                first_user = mail_user_list[0]
                recipient_email = first_user.get("email", "")
                if recipient_email:
                    zip_path = zip_user_output(job_output_dir)
                    send_results_email(zip_path, recipient_email)
                    logger.info(f"[INFO] Emailed zip {zip_path} to {recipient_email}")
                else:
                    logger.warning("[WARN] mail_user.json => missing 'email'")
            else:
                logger.warning("[WARN] mail_user.json => 'mail_user' list is empty.")
        else:
            logger.info("[INFO] No mail_user.json found, skipping email.")
    except Exception as e:
        logger.error(f"[ERROR] Zipping/Emailing results failed => {e}")

    # -------------------------------------------------------------------------
    # LAST STEP: (Optional) Call the cleanup function
    # -------------------------------------------------------------------------
    try:
        cleanup_old_results()  # This will remove any job folder older than MAX_AGE_HOURS
    except Exception as e:
        logger.error(f"[CLEANUP ERROR] => {e}")

    logger.info("=== End of orchestrate_workflow ===")

------------------------------------------------------------

File: D:\Documents\E_Plus_2030_py\job_manager.py
============================================================
"""
job_manager.py

A more comprehensive in-memory job manager to handle multiple
workflow runs (jobs). Each job can be in states like CREATED,
QUEUED, RUNNING, FINISHED, ERROR, or CANCELED.

Features:
  - Unique job_id generation
  - Concurrency limit (max simultaneous running jobs)
  - Job queue (if concurrency limit is reached)
  - In-memory log queue for real-time streaming
  - Support for cancellation
  - Placeholders for persistence (if needed in future)

CAVEATS:
  - If the Python process exits, in-memory jobs are lost.
  - For production usage, consider a persistent store (Redis, DB).
"""

import uuid
import queue
import threading
import time
import enum
from typing import Any, Dict, Optional, Union

from orchestrator import WorkflowCanceled


###############################################################################
# Global Job Store and Concurrency
###############################################################################

# You can store up to X jobs as RUNNING at once. Others will wait in a queue.
MAX_RUNNING_JOBS = 2

# The global store for jobs and a global queue for waiting jobs.
_jobs: Dict[str, Dict] = {}
_waiting_jobs: queue.Queue = queue.Queue()  # For jobs awaiting execution if concurrency is at max.

# A global lock to protect shared state, e.g., reading/writing _jobs, counting running jobs, etc.
_jobs_lock = threading.Lock()

###############################################################################
# Enum: JobStatus
###############################################################################
class JobStatus(str, enum.Enum):
    CREATED = "CREATED"
    QUEUED = "QUEUED"
    RUNNING = "RUNNING"
    FINISHED = "FINISHED"
    ERROR = "ERROR"
    CANCELED = "CANCELED"


###############################################################################
# Data Structure for Each Job
###############################################################################
# Each job dictionary in _jobs has keys:
#   {
#       "status": JobStatus,
#       "config": dict,             # includes "job_id" and other user config
#       "logs": queue.Queue,        # For log streaming
#       "thread": threading.Thread or None,
#       "result": Any,              # Store final result or summary
#       "cancel_event": threading.Event # Signals that we want to cancel
#   }


###############################################################################
# Main API
###############################################################################

def create_job(config: dict) -> str:
    """
    Create a new job, store it in _jobs, return its job_id.
    
    :param config: Dictionary of user-supplied config for the job
    :return: job_id (string)
    """
    job_id = str(uuid.uuid4())
    with _jobs_lock:
        # We also store job_id in the config so orchestrator can see it.
        config["job_id"] = job_id

        _jobs[job_id] = {
            "status": JobStatus.CREATED,
            "config": config,
            "logs": queue.Queue(),
            "thread": None,
            "result": None,
            "cancel_event": threading.Event(),
        }
    return job_id


def enqueue_job(job_id: str) -> None:
    """
    Place a job into the waiting queue if concurrency is at max, or run it immediately if not.
    """
    with _jobs_lock:
        job = _jobs.get(job_id)
        if not job:
            return
        # If job is not in CREATED state, do nothing
        if job["status"] != JobStatus.CREATED:
            return

        # Count how many are running
        running_count = sum(1 for j in _jobs.values() if j["status"] == JobStatus.RUNNING)
        if running_count < MAX_RUNNING_JOBS:
            # We can start this job immediately
            _start_job(job_id)
        else:
            # We have to queue it
            job["status"] = JobStatus.QUEUED
            _waiting_jobs.put(job_id)


def get_job(job_id: str) -> Optional[Dict]:
    """Retrieve the entire job dictionary by job_id."""
    with _jobs_lock:
        return _jobs.get(job_id)


def get_job_status(job_id: str) -> Optional[str]:
    """Return the status of the job as a string (or None if not found)."""
    with _jobs_lock:
        job = _jobs.get(job_id)
        if job:
            return job["status"]
    return None


def get_job_logs_queue(job_id: str) -> Optional[queue.Queue]:
    """Return the logs queue for a job, if it exists."""
    with _jobs_lock:
        job = _jobs.get(job_id)
        if job:
            return job["logs"]
    return None


def cancel_job(job_id: str) -> bool:
    """
    Signal that a job should be canceled (if RUNNING or QUEUED).
    This sets the 'cancel_event' so the job's thread can check and stop if possible.
    
    :return: True if job was canceled or is in the process of being canceled, False if not found.
    """
    with _jobs_lock:
        job = _jobs.get(job_id)
        if not job:
            return False

        status = job["status"]
        if status in [JobStatus.FINISHED, JobStatus.ERROR, JobStatus.CANCELED]:
            return False  # It's already done or canceled

        # Signal the thread to stop
        job["cancel_event"].set()

        if status == JobStatus.QUEUED:
            # If queued, remove it from the queue
            # We'll need to rebuild the queue without that job_id
            _remove_queued_job(job_id)
            job["status"] = JobStatus.CANCELED
        elif status == JobStatus.RUNNING:
            # The thread should eventually notice the cancel_event
            pass
        else:
            # If CREATED and not yet enqueued, we'll just mark it canceled
            if status == JobStatus.CREATED:
                job["status"] = JobStatus.CANCELED

        return True


def set_job_result(job_id: str, result_data: Any) -> None:
    """Store final results or summary data for the given job."""
    with _jobs_lock:
        if job_id in _jobs:
            _jobs[job_id]["result"] = result_data


def get_job_result(job_id: str) -> Any:
    """Retrieve whatever was set as the result data."""
    with _jobs_lock:
        job = _jobs.get(job_id)
        if job:
            return job["result"]
    return None


###############################################################################
# Internal Worker Helpers
###############################################################################

def _start_job(job_id: str) -> None:
    """
    Transition job to RUNNING and spawn a thread to execute it.
    """
    job = _jobs[job_id]
    job["status"] = JobStatus.RUNNING

    # We'll create a new thread that calls _job_thread_runner(job_id)
    t = threading.Thread(target=_job_thread_runner, args=(job_id,), daemon=True)
    job["thread"] = t
    t.start()


def _job_thread_runner(job_id: str) -> None:
    """Worker function that runs in a separate thread for each job."""
    try:
        job = _jobs[job_id]
        cancel_event = job["cancel_event"]

        # 1) Import orchestrate_workflow
        from orchestrator import orchestrate_workflow

        # 2) Run the workflow, passing the entire job["config"] which has "job_id"
        orchestrate_workflow(job["config"], cancel_event=cancel_event)

        # If orchestrate_workflow finishes with no exception => FINISHED
        job["status"] = JobStatus.FINISHED

    except WorkflowCanceled:
        job["status"] = JobStatus.CANCELED
    except Exception as e:
        job["status"] = JobStatus.ERROR
        err_msg = f"[Job {job_id}] crashed => {e}"
        job["logs"].put(err_msg)
    finally:
        _signal_end_of_logs(job_id)
        _try_start_next_queued_job()


def _try_start_next_queued_job():
    """
    Check if there's a queued job. If concurrency allows, start it.
    """
    with _jobs_lock:
        running_count = sum(1 for j in _jobs.values() if j["status"] == JobStatus.RUNNING)
        if running_count >= MAX_RUNNING_JOBS:
            return  # No slot available

        # Otherwise, pop from waiting queue if any
        while not _waiting_jobs.empty():
            next_job_id = _waiting_jobs.get()
            job = _jobs.get(next_job_id)
            if not job:
                continue  # might have been canceled or removed

            if job["status"] == JobStatus.QUEUED:
                # Start it
                _start_job(next_job_id)
                break


def _remove_queued_job(job_id: str):
    """
    Remove a job from the waiting queue if it's in there by rebuilding the queue 
    without that job_id.
    """
    temp_list = []
    while not _waiting_jobs.empty():
        j_id = _waiting_jobs.get()
        if j_id != job_id:
            temp_list.append(j_id)
    # re-queue the others
    for j_id in temp_list:
        _waiting_jobs.put(j_id)


def _signal_end_of_logs(job_id: str):
    """
    Put a `None` sentinel in the job's log queue to indicate no more logs.
    The client log streamer can break on encountering None.
    """
    with _jobs_lock:
        job = _jobs.get(job_id)
        if job:
            job["logs"].put(None)

------------------------------------------------------------

File: D:\Documents\E_Plus_2030_py\main_modifi.py
============================================================
"""
main_modifi.py

Handles the generation of scenario-based IDFs for sensitivity, surrogate,
calibration, or any parametric runs, then optionally runs E+ simulation,
post-processing, and validation in a job-specific folder if provided.

Usage:
  - Typically invoked from your orchestrator (or command line) with a config dict:
    {
      "base_idf_path": "output_IDFs/building_0.idf",
      "idd_path": "EnergyPlus/Energy+.idd",
      "assigned_csv": {
        "hvac_building": "output/assigned/assigned_hvac_building.csv",
        "hvac_zones": "output/assigned/assigned_hvac_zones.csv",
        "dhw": "output/assigned/assigned_dhw_params.csv",
        "vent_build": "output/assigned/assigned_vent_building.csv",
        "vent_zones": "output/assigned/assigned_vent_zones.csv",
        "elec": "output/assigned/assigned_lighting.csv",
        "fenez": "output/assigned/structured_fenez_params.csv"
      },
      "scenario_csv": {
        "hvac": "output/scenarios/scenario_params_hvac.csv",
        "dhw": "output/scenarios/scenario_params_dhw.csv",
        "vent": "output/scenarios/scenario_params_vent.csv",
        "elec": "output/scenarios/scenario_params_elec.csv",
        "fenez": "output/scenarios/scenario_params_fenez.csv"
      },
      "output_idf_dir": "output/scenario_idfs",
      "building_id": 4136730,
      "num_scenarios": 5,
      "picking_method": "random_uniform",
      "picking_scale_factor": 0.5,

      "run_simulations": true,
      "simulation_config": {
        "num_workers": 4,
        "output_dir": "output/Sim_Results/Scenarios"
      },
      "perform_post_process": true,
      "post_process_config": {
        "output_csv_as_is": "output/results_scenarioes/merged_as_is_scenarios.csv",
        "output_csv_daily_mean": "output/results_scenarioes/merged_daily_mean_scenarios.csv"
      },
      "perform_validation": true,
      "validation_config": {
        "real_data_csv": "data/mock_merged_daily_mean.csv",
        "sim_data_csv": "output/results_scenarioes/merged_daily_mean_scenarios.csv",
        "bldg_ranges": { "0": [0,1,2] },
        "variables_to_compare": [...],
        "threshold_cv_rmse": 30.0,
        "skip_plots": true,
        "output_csv": "scenario_validation_report.csv"
      },

      "job_output_dir": "/usr/src/app/output/xxxx-uuid"   # (Optional)
    }
"""

import os
import logging
import pandas as pd

# ---------------------------------------------------------------------------
# A) Common Utilities
# ---------------------------------------------------------------------------
from modification.common_utils import (
    load_assigned_csv,
    load_scenario_csv,
    load_idf,
    save_idf,
    generate_multiple_param_sets,
    save_param_scenarios_to_csv
)

# ---------------------------------------------------------------------------
# B) Modules for scenario creation & application
# ---------------------------------------------------------------------------
from modification.hvac_functions import (
    create_hvac_scenarios,
    apply_building_level_hvac,
    apply_zone_level_hvac
)
from modification.dhw_functions import (
    create_dhw_scenarios,
    apply_dhw_params_to_idf
)
from modification.vent_functions import (
    create_vent_scenarios,
    apply_building_level_vent,
    apply_zone_level_vent
)
from modification.elec_functions import (
    create_elec_scenarios,
    apply_building_level_elec,
    apply_object_level_elec
)
from modification.fenez_functions2 import (
    create_fenez_scenarios,
    apply_object_level_fenez
)

# ---------------------------------------------------------------------------
# C) Simulation + Post-processing + Validation
# ---------------------------------------------------------------------------
from epw.run_epw_sims import simulate_all
from postproc.merge_results import merge_all_results
from validation.main_validation import run_validation_process


def run_all_idfs_in_folder(
    folder_path: str,
    iddfile: str,
    base_output_dir: str,
    default_lat: float = 52.15,
    default_lon: float = 4.40,
    default_year: int = 2020,
    num_workers: int = 4
):
    """
    Utility function to find .idf files in folder_path and run them with simulate_all(...).
    Adjust lat/lon/year or load them from a side CSV if needed.
    """
    logger = logging.getLogger(__name__)
    logger.info(f"[run_all_idfs_in_folder] Searching .idf files in {folder_path}")

    if not os.path.isdir(folder_path):
        logger.warning(f"[run_all_idfs_in_folder] Folder not found => {folder_path}")
        return

    idf_files = [f for f in os.listdir(folder_path) if f.lower().endswith(".idf")]
    if not idf_files:
        logger.warning(f"[run_all_idfs_in_folder] No .idf files in {folder_path} to run.")
        return

    data_rows = []
    for idx, idf_name in enumerate(idf_files):
        data_rows.append({
            "idf_name": idf_name,
            "lat": default_lat,
            "lon": default_lon,
            "desired_climate_year": default_year,
            "ogc_fid": idx  # or parse from filename
        })

    df_scenarios = pd.DataFrame(data_rows)
    logger.info(f"[run_all_idfs_in_folder] Running {len(df_scenarios)} scenario IDFs with simulate_all...")

    simulate_all(
        df_buildings=df_scenarios,
        idf_directory=folder_path,
        iddfile=iddfile,
        base_output_dir=base_output_dir,
        user_config_epw=None,
        assigned_epw_log=None,
        num_workers=num_workers
    )
    logger.info("[run_all_idfs_in_folder] Simulations triggered.")


def run_modification_workflow(config):
    """
    Main function for scenario-based IDF creation + optional E+ simulation,
    post-processing, and validation.

    Steps:
      1) Resolve folder paths (scenario IDFs, results) based on config + optional job_output_dir.
      2) Load assigned CSV data (HVAC, DHW, Vent, Elec, Fenez).
      3) Filter for the chosen building.
      4) Generate scenario param picks (random or otherwise).
      5) For each scenario, load a fresh base IDF, apply picks, save scenario IDF.
      6) (Optional) run E+ sims for these scenario IDFs, then post-process, then validate.

    :param config: dict
    :return: None
    """
    logger = logging.getLogger(__name__)
    logger.info("[MODIFICATION] Starting scenario-based workflow...")

    # -----------------------------------------------------------------------
    # 1) Extract config parts & resolve paths
    # -----------------------------------------------------------------------
    base_idf_path   = config["base_idf_path"]
    idd_path        = config["idd_path"]
    assigned_csvs   = config["assigned_csv"]
    scenario_csvs   = config["scenario_csv"]
    building_id     = config["building_id"]
    num_scenarios   = config["num_scenarios"]
    picking_method  = config["picking_method"]
    scale_factor    = config.get("picking_scale_factor", 1.0)

    # The user might specify something like "output/scenario_idfs" or just "scenario_idfs".
    scenario_idf_dir = config.get("output_idf_dir", "output/scenario_idfs")

    # Also we have simulation, post-processing, and validation flags:
    run_sims        = config.get("run_simulations", False)
    sim_cfg         = config.get("simulation_config", {})
    do_postproc     = config.get("perform_post_process", False)
    postproc_cfg    = config.get("post_process_config", {})
    do_validation   = config.get("perform_validation", False)
    validation_cfg  = config.get("validation_config", {})

    # If "job_output_dir" is provided, make scenario_idf_dir relative to it (if it's not absolute).
    job_output_dir = config.get("job_output_dir")  # optional
    if job_output_dir and not os.path.isabs(scenario_idf_dir):
        scenario_idf_dir = os.path.join(job_output_dir, scenario_idf_dir)
    os.makedirs(scenario_idf_dir, exist_ok=True)

    logger.info(f"[MODIFICATION] Scenario IDFs will be placed in: {scenario_idf_dir}")

    # -----------------------------------------------------------------------
    # 2) Load assigned CSV data
    # -----------------------------------------------------------------------
    # HVAC
    df_hvac_bld = None
    df_hvac_zn  = None
    if "hvac_building" in assigned_csvs and "hvac_zones" in assigned_csvs:
        df_hvac_bld = load_assigned_csv(assigned_csvs["hvac_building"])
        df_hvac_zn  = load_assigned_csv(assigned_csvs["hvac_zones"])
    elif "hvac" in assigned_csvs:
        df_hvac_bld = load_assigned_csv(assigned_csvs["hvac"])

    # DHW
    df_dhw = load_assigned_csv(assigned_csvs["dhw"]) if "dhw" in assigned_csvs else None

    # Vent
    df_vent_bld = None
    df_vent_zn  = None
    if "vent_build" in assigned_csvs and "vent_zones" in assigned_csvs:
        df_vent_bld = load_assigned_csv(assigned_csvs["vent_build"])
        df_vent_zn  = load_assigned_csv(assigned_csvs["vent_zones"])
    elif "vent" in assigned_csvs:
        df_vent_bld = load_assigned_csv(assigned_csvs["vent"])

    # Elec
    df_elec  = load_assigned_csv(assigned_csvs["elec"])  if "elec"  in assigned_csvs else None

    # Fenestration
    df_fenez = load_assigned_csv(assigned_csvs["fenez"]) if "fenez" in assigned_csvs else None

    # -----------------------------------------------------------------------
    # 3) Filter data for this building
    # -----------------------------------------------------------------------
    def filter_for_building(df):
        if df is not None and not df.empty:
            return df[df["ogc_fid"] == building_id].copy()
        return pd.DataFrame()

    df_hvac_bld_sub = filter_for_building(df_hvac_bld)
    df_hvac_zn_sub  = filter_for_building(df_hvac_zn)
    df_dhw_sub      = filter_for_building(df_dhw)
    df_vent_bld_sub = filter_for_building(df_vent_bld)
    df_vent_zn_sub  = filter_for_building(df_vent_zn)
    df_elec_sub     = filter_for_building(df_elec)
    df_fenez_sub    = filter_for_building(df_fenez)

    # -----------------------------------------------------------------------
    # 4) Generate scenario picks (random or otherwise)
    # -----------------------------------------------------------------------
    # HVAC
    if not df_hvac_bld_sub.empty:
        if not df_hvac_zn_sub.empty:
            # multi-step scenario creation for building & zone
            create_hvac_scenarios(
                df_building=df_hvac_bld_sub,
                df_zones=df_hvac_zn_sub,
                building_id=building_id,
                num_scenarios=num_scenarios,
                picking_method=picking_method,
                random_seed=42,
                scenario_csv_out=scenario_csvs["hvac"]
            )
        else:
            hvac_scen = generate_multiple_param_sets(
                df_main_sub=df_hvac_bld_sub,
                num_sets=num_scenarios,
                picking_method=picking_method,
                scale_factor=scale_factor
            )
            save_param_scenarios_to_csv(hvac_scen, building_id, scenario_csvs["hvac"])

    # DHW
    if not df_dhw_sub.empty:
        create_dhw_scenarios(
            df_dhw_input=df_dhw_sub,
            building_id=building_id,
            num_scenarios=num_scenarios,
            picking_method=picking_method,
            random_seed=42,
            scenario_csv_out=scenario_csvs["dhw"]
        )

    # Vent
    if not df_vent_bld_sub.empty or not df_vent_zn_sub.empty:
        create_vent_scenarios(
            df_building=df_vent_bld_sub,
            df_zones=df_vent_zn_sub,
            building_id=building_id,
            num_scenarios=num_scenarios,
            picking_method=picking_method,
            random_seed=42,
            scenario_csv_out=scenario_csvs["vent"]
        )

    # Elec
    if not df_elec_sub.empty:
        create_elec_scenarios(
            df_lighting=df_elec_sub,
            building_id=building_id,
            num_scenarios=num_scenarios,
            picking_method=picking_method,
            random_seed=42,
            scenario_csv_out=scenario_csvs["elec"]
        )

    # Fenestration
    if not df_fenez_sub.empty:
        create_fenez_scenarios(
            df_struct_fenez=df_fenez_sub,
            building_id=building_id,
            num_scenarios=num_scenarios,
            picking_method=picking_method,
            random_seed=42,
            scenario_csv_out=scenario_csvs["fenez"]
        )

    # -----------------------------------------------------------------------
    # 5) Load scenario CSV => group by scenario_index
    # -----------------------------------------------------------------------
    def safe_load_scenario(csv_path):
        if os.path.isfile(csv_path):
            return load_scenario_csv(csv_path)
        return pd.DataFrame()

    df_hvac_scen  = safe_load_scenario(scenario_csvs["hvac"])
    df_dhw_scen   = safe_load_scenario(scenario_csvs["dhw"])
    df_vent_scen  = safe_load_scenario(scenario_csvs["vent"])
    df_elec_scen  = safe_load_scenario(scenario_csvs["elec"])
    df_fenez_scen = safe_load_scenario(scenario_csvs["fenez"])

    hvac_groups  = df_hvac_scen.groupby("scenario_index")  if not df_hvac_scen.empty  else None
    dhw_groups   = df_dhw_scen.groupby("scenario_index")   if not df_dhw_scen.empty   else None
    vent_groups  = df_vent_scen.groupby("scenario_index")  if not df_vent_scen.empty  else None
    elec_groups  = df_elec_scen.groupby("scenario_index")  if not df_elec_scen.empty  else None
    fenez_groups = df_fenez_scen.groupby("scenario_index") if not df_fenez_scen.empty else None

    # -----------------------------------------------------------------------
    # 6) For each scenario, load base IDF, apply parameters, save new IDF
    # -----------------------------------------------------------------------
    for i in range(num_scenarios):
        logger.info(f"[MODIFICATION] => Creating scenario #{i} for building {building_id}")

        hvac_df   = hvac_groups.get_group(i) if hvac_groups and i in hvac_groups.groups else pd.DataFrame()
        dhw_df    = dhw_groups.get_group(i)  if dhw_groups  and i in dhw_groups.groups  else pd.DataFrame()
        vent_df   = vent_groups.get_group(i) if vent_groups and i in vent_groups.groups else pd.DataFrame()
        elec_df   = elec_groups.get_group(i) if elec_groups and i in elec_groups.groups else pd.DataFrame()
        fenez_df  = fenez_groups.get_group(i)if fenez_groups and i in fenez_groups.groups else pd.DataFrame()

        hvac_bld_df   = hvac_df[hvac_df["zone_name"].isna()]
        hvac_zone_df  = hvac_df[hvac_df["zone_name"].notna()]
        hvac_params   = _make_param_dict(hvac_bld_df)

        dhw_params    = _make_param_dict(dhw_df)

        vent_bld_df   = vent_df[vent_df["zone_name"].isnull()]
        vent_zone_df  = vent_df[vent_df["zone_name"].notnull()]
        vent_params   = _make_param_dict(vent_bld_df)

        elec_params   = _make_param_dict(elec_df)

        # Load base IDF
        idf = load_idf(base_idf_path, idd_path)

        # Apply HVAC
        apply_building_level_hvac(idf, hvac_params)
        apply_zone_level_hvac(idf, hvac_zone_df)

        # Apply DHW
        apply_dhw_params_to_idf(idf, dhw_params, suffix=f"Scenario_{i}")

        # Apply Vent
        if not vent_bld_df.empty or not vent_zone_df.empty:
            apply_building_level_vent(idf, vent_params)
            apply_zone_level_vent(idf, vent_zone_df)

        # Apply Elec => building-level or object-level
        if not elec_df.empty:
            apply_building_level_elec(idf, elec_params, zonelist_name="ALL_ZONES")
            # or use apply_object_level_elec(idf, elec_df) if you prefer

        # Apply Fenestration => object-level
        apply_object_level_fenez(idf, fenez_df)

        # Save scenario IDF
        scenario_idf_name = f"building_{building_id}_scenario_{i}.idf"
        scenario_idf_path = os.path.join(scenario_idf_dir, scenario_idf_name)
        save_idf(idf, scenario_idf_path)
        logger.info(f"[MODIFICATION] Saved scenario IDF => {scenario_idf_path}")

    logger.info("[MODIFICATION] All scenario IDFs generated successfully.")

    # -----------------------------------------------------------------------
    # 7) (Optional) Simulations
    # -----------------------------------------------------------------------
    if run_sims:
        logger.info("[MODIFICATION] Running E+ simulations for all scenario IDFs.")
        base_sim_dir = sim_cfg.get("output_dir", "output/Sim_Results/Scenarios")

        # if job_output_dir is given, we can make the sim results go inside it, too:
        if job_output_dir and not os.path.isabs(base_sim_dir):
            base_sim_dir = os.path.join(job_output_dir, base_sim_dir)
        os.makedirs(base_sim_dir, exist_ok=True)

        num_workers  = sim_cfg.get("num_workers", 4)

        run_all_idfs_in_folder(
            folder_path=scenario_idf_dir,
            iddfile=idd_path,
            base_output_dir=base_sim_dir,
            default_lat=52.15,
            default_lon=4.40,
            default_year=2020,
            num_workers=num_workers
        )

    # -----------------------------------------------------------------------
    # 8) (Optional) Post-processing
    # -----------------------------------------------------------------------
    if do_postproc:
        logger.info("[MODIFICATION] Performing post-processing merges.")

        base_sim_dir = sim_cfg.get("output_dir", "output/Sim_Results/Scenarios")
        if job_output_dir and not os.path.isabs(base_sim_dir):
            base_sim_dir = os.path.join(job_output_dir, base_sim_dir)

        output_csv_as_is = postproc_cfg.get("output_csv_as_is", "")
        output_csv_daily_mean = postproc_cfg.get("output_csv_daily_mean", "")

        # Build full paths inside job_output_dir if they are relative
        if output_csv_as_is:
            if job_output_dir and not os.path.isabs(output_csv_as_is):
                output_csv_as_is = os.path.join(job_output_dir, output_csv_as_is)
            os.makedirs(os.path.dirname(output_csv_as_is), exist_ok=True)

            merge_all_results(
                base_output_dir=base_sim_dir,
                output_csv=output_csv_as_is,
                convert_to_daily=False,
                convert_to_monthly=False
            )

        if output_csv_daily_mean:
            if job_output_dir and not os.path.isabs(output_csv_daily_mean):
                output_csv_daily_mean = os.path.join(job_output_dir, output_csv_daily_mean)
            os.makedirs(os.path.dirname(output_csv_daily_mean), exist_ok=True)

            merge_all_results(
                base_output_dir=base_sim_dir,
                output_csv=output_csv_daily_mean,
                convert_to_daily=True,
                daily_aggregator="mean",
                convert_to_monthly=False
            )

        logger.info("[MODIFICATION] Post-processing step complete.")

    # -----------------------------------------------------------------------
    # 9) (Optional) Validation
    # -----------------------------------------------------------------------
    if do_validation:
        logger.info("[MODIFICATION] Performing scenario validation with config => %s", validation_cfg)

        # If the validation config references CSV paths that might be relative,
        # you could also adjust them to be inside job_output_dir here if desired.
        # e.g.:
        # real_csv = validation_cfg.get("real_data_csv", "")
        # if job_output_dir and not os.path.isabs(real_csv):
        #     real_csv = os.path.join(job_output_dir, real_csv)
        # validation_cfg["real_data_csv"] = real_csv

        run_validation_process(validation_cfg)
        logger.info("[MODIFICATION] Validation step complete.")


def _make_param_dict(df_scenario):
    """
    Builds a dict {param_name: value} from the scenario DataFrame columns,
    checking 'assigned_value' or 'param_value'.
    """
    if df_scenario.empty:
        return {}

    cols = df_scenario.columns.tolist()
    if "assigned_value" in cols:
        val_col = "assigned_value"
    elif "param_value" in cols:
        val_col = "param_value"
    else:
        raise AttributeError(
            "No 'assigned_value' or 'param_value' column found in scenario dataframe! "
            f"Columns are: {cols}"
        )

    result = {}
    for row in df_scenario.itertuples():
        p_name = row.param_name
        raw_val = getattr(row, val_col)
        try:
            result[p_name] = float(raw_val)
        except (ValueError, TypeError):
            result[p_name] = raw_val
    return result

------------------------------------------------------------

File: D:\Documents\E_Plus_2030_py\idf_creation.py
============================================================
"""
idf_creation.py

Handles the creation of EnergyPlus IDF files for a list of buildings,
plus optional simulation runs and post-processing.

Key functionalities:
  1) create_idf_for_building(...) builds a single IDF using geomeppy,
     applying geometry, fenestration, HVAC, etc.
  2) create_idfs_for_all_buildings(...) loops over multiple buildings,
     then optionally runs simulations and merges results in one or more ways.

Updated to allow writing logs/results inside a specific job folder via logs_base_dir.
"""

import os
import logging
import pandas as pd

# geomeppy for IDF manipulation
from geomeppy import IDF

# --- Import your custom submodules ---
from idf_objects.geomz.building import create_building_with_roof_type
from idf_objects.fenez.fenestration import add_fenestration
from idf_objects.fenez.materials import (
    update_construction_materials,
    assign_constructions_to_surfaces
)
from idf_objects.Elec.lighting import add_lights_and_parasitics
from idf_objects.DHW.water_heater import add_dhw_to_idf
from idf_objects.HVAC.custom_hvac import add_HVAC_Ideal_to_all_zones
from idf_objects.ventilation.add_ventilation import add_ventilation_to_idf
from idf_objects.setzone.add_outdoor_air_and_zone_sizing_to_all_zones import add_outdoor_air_and_zone_sizing_to_all_zones
from idf_objects.tempground.add_ground_temperatures import add_ground_temperatures
from idf_objects.other.zonelist import create_zonelist

# Output & simulation modules
from idf_objects.outputdef.assign_output_settings import assign_output_settings
from idf_objects.outputdef.add_output_definitions import add_output_definitions
from postproc.merge_results import merge_all_results
from epw.run_epw_sims import simulate_all

###############################################################################
# Global Default IDF Config
# (Override these via environment variables or main_config if needed.)
###############################################################################
idf_config = {
    "iddfile": "EnergyPlus/Energy+.idd",         # Default path to the IDD file
    "idf_file_path": "EnergyPlus/Minimal.idf",   # Default path to a minimal base IDF
    "output_dir": "output/output_IDFs"           # Default folder to save generated IDFs
}


def create_idf_for_building(
    building_row,
    building_index,
    scenario="scenario1",
    calibration_stage="pre_calibration",
    strategy="B",
    random_seed=42,
    # Geometry
    user_config_geom=None,
    assigned_geom_log=None,
    # Lighting
    user_config_lighting=None,
    assigned_lighting_log=None,
    # DHW
    user_config_dhw=None,
    assigned_dhw_log=None,
    # Fenestration
    res_data=None,
    nonres_data=None,
    assigned_fenez_log=None,
    # HVAC
    user_config_hvac=None,
    assigned_hvac_log=None,
    # Vent
    user_config_vent=None,
    assigned_vent_log=None,
    # Zone sizing
    assigned_setzone_log=None,
    # Ground temps
    assigned_groundtemp_log=None,
    # Output definitions
    output_definitions=None
):
    """
    Build an IDF for a single building, applying geometry, fenestration, lighting,
    HVAC, ventilation, zone sizing, ground temps, and user overrides.

    Returns
    -------
    out_path : str
        File path to the saved IDF.
    """
    # 1) Setup IDF from the minimal template
    IDF.setiddname(idf_config["iddfile"])
    idf = IDF(idf_config["idf_file_path"])

    # 2) Basic building object settings
    building_obj = idf.newidfobject("BUILDING")
    building_obj.Name = f"Sample_Building_{building_index}"

    orientation = building_row.get("building_orientation", 0.0)
    if not pd.isna(orientation):
        building_obj.North_Axis = orientation

    # 3) Create geometry
    if assigned_geom_log is not None and building_row.get("ogc_fid") not in assigned_geom_log:
        assigned_geom_log[building_row.get("ogc_fid")] = {}

    edge_types = []
    for side_col in ["north_side", "east_side", "south_side", "west_side"]:
        edge_types.append(building_row.get(side_col, "Facade"))

    create_building_with_roof_type(
        idf=idf,
        area=building_row.get("area", 100.0),
        perimeter=building_row.get("perimeter", 40.0),
        orientation=orientation,
        building_row=building_row,
        edge_types=edge_types,
        calibration_stage=calibration_stage,
        strategy=strategy,
        random_seed=random_seed,
        user_config=user_config_geom,
        assigned_geom_log=assigned_geom_log
    )

    # 4) Update materials & constructions
    construction_map = update_construction_materials(
        idf=idf,
        building_row=building_row,
        building_index=building_index,
        scenario=scenario,
        calibration_stage=calibration_stage,
        strategy=strategy,
        random_seed=random_seed,
        user_config_fenez=None,  # not used directly
        assigned_fenez_log=assigned_fenez_log
    )
    assign_constructions_to_surfaces(idf, construction_map)

    # Create zone list for convenience
    create_zonelist(idf, zonelist_name="ALL_ZONES")

    # 5) Fenestration
    add_fenestration(
        idf=idf,
        building_row=building_row,
        scenario=scenario,
        calibration_stage=calibration_stage,
        strategy=strategy,
        random_seed=random_seed,
        res_data=res_data,
        nonres_data=nonres_data,
        assigned_fenez_log=assigned_fenez_log
    )

    # 6) Lighting
    add_lights_and_parasitics(
        idf=idf,
        building_row=building_row,
        calibration_stage=calibration_stage,
        strategy=strategy,
        random_seed=random_seed,
        user_config=user_config_lighting,
        assigned_values_log=assigned_lighting_log
    )

    # 7) DHW
    add_dhw_to_idf(
        idf=idf,
        building_row=building_row,
        calibration_stage=calibration_stage,
        strategy=strategy,
        random_seed=random_seed,
        name_suffix=f"MyDHW_{building_index}",
        user_config_dhw=user_config_dhw,
        assigned_dhw_log=assigned_dhw_log,
        use_nta=True
    )

    # 8) HVAC
    add_HVAC_Ideal_to_all_zones(
        idf=idf,
        building_row=building_row,
        calibration_stage=calibration_stage,
        strategy=strategy,
        random_seed=random_seed,
        user_config_hvac=user_config_hvac,
        assigned_hvac_log=assigned_hvac_log
    )

    # 9) Ventilation
    add_ventilation_to_idf(
        idf=idf,
        building_row=building_row,
        calibration_stage=calibration_stage,
        strategy=strategy,
        random_seed=random_seed,
        user_config_vent=user_config_vent,
        assigned_vent_log=assigned_vent_log
    )

    # 10) Zone sizing
    add_outdoor_air_and_zone_sizing_to_all_zones(
        idf=idf,
        building_row=building_row,
        calibration_stage=calibration_stage,
        strategy=strategy,
        random_seed=random_seed,
        assigned_setzone_log=assigned_setzone_log
    )

    # 11) Ground temperatures
    add_ground_temperatures(
        idf=idf,
        calibration_stage=calibration_stage,
        strategy=strategy,
        random_seed=random_seed,
        assigned_groundtemp_log=assigned_groundtemp_log
    )

    # 12) Output definitions
    if output_definitions is None:
        output_definitions = {
            "desired_variables": ["Facility Total Electric Demand Power", "Zone Air Temperature"],
            "desired_meters": ["Electricity:Facility"],
            "override_variable_frequency": "Hourly",
            "override_meter_frequency": "Hourly",
            "include_tables": True,
            "include_summary": True
        }
    out_settings = assign_output_settings(
        desired_variables=output_definitions.get("desired_variables", []),
        desired_meters=output_definitions.get("desired_meters", []),
        override_variable_frequency=output_definitions.get("override_variable_frequency", "Hourly"),
        override_meter_frequency=output_definitions.get("override_meter_frequency", "Hourly"),
        include_tables=output_definitions.get("include_tables", True),
        include_summary=output_definitions.get("include_summary", True)
    )
    add_output_definitions(idf, out_settings)

    # 13) Save final IDF
    os.makedirs(idf_config["output_dir"], exist_ok=True)
    idf_filename = f"building_{building_index}.idf"
    out_path = os.path.join(idf_config["output_dir"], idf_filename)
    idf.save(out_path)
    print(f"[create_idf_for_building] IDF saved at: {out_path}")

    return out_path


def create_idfs_for_all_buildings(
    df_buildings,
    scenario="scenario1",
    calibration_stage="pre_calibration",
    strategy="B",
    random_seed=42,
    # partial user configs
    user_config_geom=None,
    user_config_lighting=None,
    user_config_dhw=None,
    res_data=None,
    nonres_data=None,
    user_config_hvac=None,
    user_config_vent=None,
    user_config_epw=None,  # pass epw config or list if relevant
    # output definitions
    output_definitions=None,
    # simulation & postprocess
    run_simulations=True,
    simulate_config=None,
    post_process=True,
    post_process_config=None,
    # NEW: Where to store logs and results
    logs_base_dir=None
):
    """
    Loops over df_buildings, calls create_idf_for_building for each building,
    optionally runs E+ simulations in parallel, and merges results if post_process=True.

    If logs_base_dir is provided, all assigned_*.csv and merged results go under that folder
    (e.g. logs_base_dir/assigned, logs_base_dir/Sim_Results, etc.).
    """
    logger = logging.getLogger(__name__)

    # A) Prepare dictionaries to store final picks for each module
    assigned_geom_log       = {}
    assigned_lighting_log   = {}
    assigned_dhw_log        = {}
    assigned_fenez_log      = {}
    assigned_hvac_log       = {}
    assigned_vent_log       = {}
    assigned_epw_log        = {}
    assigned_groundtemp_log = {}
    assigned_setzone_log    = {}

    # B) Create an IDF for each building
    for idx, row in df_buildings.iterrows():
        bldg_id = row.get("ogc_fid", idx)
        logger.info(f"--- Creating IDF for building index {idx}, ogc_fid={bldg_id} ---")

        idf_path = create_idf_for_building(
            building_row=row,
            building_index=idx,
            scenario=scenario,
            calibration_stage=calibration_stage,
            strategy=strategy,
            random_seed=random_seed,
            # geometry
            user_config_geom=user_config_geom,
            assigned_geom_log=assigned_geom_log,
            # lighting
            user_config_lighting=user_config_lighting,
            assigned_lighting_log=assigned_lighting_log,
            # DHW
            user_config_dhw=user_config_dhw,
            assigned_dhw_log=assigned_dhw_log,
            # Fenestration
            res_data=res_data,
            nonres_data=nonres_data,
            assigned_fenez_log=assigned_fenez_log,
            # HVAC
            user_config_hvac=user_config_hvac,
            assigned_hvac_log=assigned_hvac_log,
            # Vent
            user_config_vent=user_config_vent,
            assigned_vent_log=assigned_vent_log,
            # zone sizing
            assigned_setzone_log=assigned_setzone_log,
            # ground temps
            assigned_groundtemp_log=assigned_groundtemp_log,
            # output definitions
            output_definitions=output_definitions
        )
        # Store the final IDF filename in df_buildings
        df_buildings.loc[idx, "idf_name"] = os.path.basename(idf_path)

    # C) If we’re told to run simulations
    if run_simulations:
        logger.info("[create_idfs_for_all_buildings] => Running simulations ...")
        if simulate_config is None:
            simulate_config = {}

        # Decide on a base_output_dir for sim results
        if logs_base_dir:
            sim_output_dir = os.path.join(logs_base_dir, "Sim_Results")
        else:
            sim_output_dir = simulate_config.get("base_output_dir", "output/Sim_Results")

        idf_directory = idf_config["output_dir"]
        iddfile       = idf_config["iddfile"]

        simulate_all(
            df_buildings=df_buildings,
            idf_directory=idf_directory,
            iddfile=iddfile,
            base_output_dir=sim_output_dir,
            user_config_epw=user_config_epw,
            assigned_epw_log=assigned_epw_log,
            num_workers=simulate_config.get("num_workers", 4)
            # ep_force_overwrite=simulate_config.get("ep_force_overwrite", False)
        )

    # D) Post-processing
    if post_process:
        logger.info("[create_idfs_for_all_buildings] => Post-processing results & writing logs ...")

        if post_process_config is None:
            post_process_config = {
                "base_output_dir": "output/Sim_Results",
                "outputs": [
                    {
                        "convert_to_daily": False,
                        "convert_to_monthly": False,
                        "aggregator": "none",
                        "output_csv": "output/results/merged_as_is.csv"
                    }
                ]
            }

        # If logs_base_dir is set, we override base_output_dir
        if logs_base_dir:
            post_process_config["base_output_dir"] = os.path.join(logs_base_dir, "Sim_Results")

        base_output_dir = post_process_config.get("base_output_dir", "output/Sim_Results")
        multiple_outputs = post_process_config.get("outputs", [])

        # Possibly handle multiple post-process outputs
        for proc_item in multiple_outputs:
            convert_daily = proc_item.get("convert_to_daily", False)
            convert_monthly = proc_item.get("convert_to_monthly", False)
            aggregator = proc_item.get("aggregator", "mean")  # daily aggregator
            out_csv = proc_item.get("output_csv", "output/results/merged_default.csv")

            # If logs_base_dir is set and the out_csv is still something like "output/results/..."
            # We can relocate it under logs_base_dir, e.g. logs_base_dir/results
            # Let's do a check:
            if logs_base_dir and "output/" in out_csv:
                # redirect to logs_base_dir
                # e.g. logs_base_dir/results/merged_default.csv
                # you can pick your subfolder naming
                rel_filename = out_csv.split("output/")[-1]  # e.g. results/merged_default.csv
                out_csv = os.path.join(logs_base_dir, rel_filename)

            # Make sure directory exists
            os.makedirs(os.path.dirname(out_csv), exist_ok=True)

            # Now merge the results
            merge_all_results(
                base_output_dir=base_output_dir,
                output_csv=out_csv,
                convert_to_daily=convert_daily,
                daily_aggregator=aggregator,
                convert_to_monthly=convert_monthly
            )

        # Write CSV logs for assigned parameters
        _write_geometry_csv(assigned_geom_log, logs_base_dir)
        _write_lighting_csv(assigned_lighting_log, logs_base_dir)
        _write_fenestration_csv(assigned_fenez_log, logs_base_dir)
        _write_dhw_csv(assigned_dhw_log, logs_base_dir)
        _write_hvac_csv(assigned_hvac_log, logs_base_dir)
        _write_vent_csv(assigned_vent_log, logs_base_dir)
        # (If needed, also EPW or groundtemp logs, do similarly)

        logger.info("[create_idfs_for_all_buildings] => Done post-processing.")

    return df_buildings  # includes "idf_name" column


###############################################################################
# Internal Helper Functions to Write Assigned Logs
# -- Now accept logs_base_dir so we can place them in job_output_dir
###############################################################################
def _make_assigned_path(filename, logs_base_dir):
    """Helper to build the path for assigned_*.csv, given logs_base_dir."""
    if logs_base_dir:
        assigned_dir = os.path.join(logs_base_dir, "assigned")
    else:
        assigned_dir = "output/assigned"

    os.makedirs(assigned_dir, exist_ok=True)
    return os.path.join(assigned_dir, filename)


def _write_geometry_csv(assigned_geom_log, logs_base_dir):
    rows = []
    for bldg_id, param_dict in assigned_geom_log.items():
        for param_name, param_val in param_dict.items():
            rows.append({
                "ogc_fid": bldg_id,
                "param_name": param_name,
                "assigned_value": param_val
            })
    if not rows:
        return
    df = pd.DataFrame(rows)
    out_path = _make_assigned_path("assigned_geometry.csv", logs_base_dir)
    df.to_csv(out_path, index=False)


def _write_lighting_csv(assigned_lighting_log, logs_base_dir):
    rows = []
    for bldg_id, param_dict in assigned_lighting_log.items():
        for param_name, subdict in param_dict.items():
            assigned_val = subdict.get("assigned_value")
            min_v = subdict.get("min_val")
            max_v = subdict.get("max_val")
            obj_name = subdict.get("object_name", "")
            rows.append({
                "ogc_fid": bldg_id,
                "object_name": obj_name,
                "param_name": param_name,
                "assigned_value": assigned_val,
                "min_val": min_v,
                "max_val": max_v
            })
    if not rows:
        return
    df = pd.DataFrame(rows)
    out_path = _make_assigned_path("assigned_lighting.csv", logs_base_dir)
    df.to_csv(out_path, index=False)


def _write_fenestration_csv(assigned_fenez_log, logs_base_dir):
    rows = []
    for bldg_id, param_dict in assigned_fenez_log.items():
        for key, val in param_dict.items():
            rows.append({
                "ogc_fid": bldg_id,
                "param_name": key,
                "assigned_value": val
            })
    if not rows:
        return
    df = pd.DataFrame(rows)
    out_path = _make_assigned_path("assigned_fenez_params.csv", logs_base_dir)
    df.to_csv(out_path, index=False)


def _write_dhw_csv(assigned_dhw_log, logs_base_dir):
    rows = []
    for bldg_id, param_dict in assigned_dhw_log.items():
        for param_name, param_val in param_dict.items():
            rows.append({
                "ogc_fid": bldg_id,
                "param_name": param_name,
                "assigned_value": param_val
            })
    if not rows:
        return
    df = pd.DataFrame(rows)
    out_path = _make_assigned_path("assigned_dhw_params.csv", logs_base_dir)
    df.to_csv(out_path, index=False)


def _write_hvac_csv(assigned_hvac_log, logs_base_dir):
    rows = []
    for bldg_id, param_dict in assigned_hvac_log.items():
        for param_name, param_val in param_dict.items():
            rows.append({
                "ogc_fid": bldg_id,
                "param_name": param_name,
                "assigned_value": param_val
            })
    if not rows:
        return
    df = pd.DataFrame(rows)
    out_path = _make_assigned_path("assigned_hvac_params.csv", logs_base_dir)
    df.to_csv(out_path, index=False)


def _write_vent_csv(assigned_vent_log, logs_base_dir):
    rows = []
    for bldg_id, param_dict in assigned_vent_log.items():
        for param_name, param_val in param_dict.items():
            rows.append({
                "ogc_fid": bldg_id,
                "param_name": param_name,
                "assigned_value": param_val
            })
    if not rows:
        return
    df = pd.DataFrame(rows)
    out_path = _make_assigned_path("assigned_ventilation.csv", logs_base_dir)
    df.to_csv(out_path, index=False)

------------------------------------------------------------

File: D:\Documents\E_Plus_2030_py\combined.json
============================================================
{
  "dhw": [
    {
      "building_id": 4136730,
      "param_name": "occupant_density_m2_per_person",
      "min_val": 127.0,
      "max_val": 233.0
    },
    {
      "building_id": 4136730,
      "param_name": "liters_per_person_per_day",
      "fixed_value": 145.0
    },
    {
      "building_function": "residential",
      "age_range": "1992-2005",
      "param_name": "setpoint_c",
      "min_val": 58.0,
      "max_val": 60.0
    },
    {
      "building_function": "non_residential",
      "param_name": "occupant_density_m2_per_person",
      "min_val": 12.0,
      "max_val": 18.0
    }
  ],
  "epw": [
    {
      "building_id": 4136730,
      "fixed_epw_path": "data/weather/2050.epw"
    },
    {
      "desired_year": 2050,
      "override_year_to": 2020
    }
  ],
  "mail_user": [
    {
      "email": "amin.jalilzade1@gmail.com"
    }
  ],
  "fenestration": [
    {
      "building_id": 4136730,
      "building_function": "residential",
      "building_type": "Two-and-a-half-story House",
      "age_range": "1992-2005",
      "scenario": "scenario1",
      "param_name": "wwr",
      "min_val": 0.025,
      "max_val": 0.03
    },
    {
      "building_function": "non_residential",
      "age_range": "2015 and later",
      "scenario": "scenario1",
      "param_name": "roof_R_value",
      "fixed_value": 3.0
    }
  ],
  "geometry": [
    {
      "building_id": 4136730,
      "param_name": "perimeter_depth",
      "min_val": 3.5,
      "max_val": 4.0,
      "fixed_value": true
    },
    {
      "building_id": 4136730,
      "param_name": "has_core",
      "fixed_value": true
    },
    {
      "building_type": "Meeting Function",
      "param_name": "has_core",
      "fixed_value": true
    }
  ],
  "hvac": [
    {
      "building_id": 4136730,
      "param_name": "heating_day_setpoint",
      "min_val": 10.0,
      "max_val": 11.0
    },
    {
      "building_function": "residential",
      "param_name": "cooling_day_setpoint",
      "fixed_value": 25.6
    },
    {
      "scenario": "scenario1",
      "age_range": "2015 and later",
      "param_name": "max_heating_supply_air_temp",
      "min_val": 48.6,
      "max_val": 50.0
    }
  ],
  "lighting": [
    {
      "building_id": 4136730,
      "param_name": "lights_wm2",
      "min_val": 18.0,
      "max_val": 20.0
    },
    {
      "building_type": "Residential",
      "param_name": "tN",
      "min_val": 10999,
      "max_val": 11999
    },
    {
      "building_id": 4136731,
      "building_type": "non_residential",
      "param_name": "parasitic_wm2",
      "min_val": 0.28,
      "max_val": 0.3
    }
  ],
  "main_config": {
    "paths": {
      "building_data": "data/df_buildings.csv",
      "fenez_excel": "excel_data/envelop.xlsx",
      "dhw_excel": "excel_data/dhw_overrides.xlsx",
      "epw_excel": "excel_data/epw_overrides.xlsx",
      "lighting_excel": "excel_data/lighting_overrides.xlsx",
      "hvac_excel": "excel_data/hvac_overrides.xlsx",
      "vent_excel": "excel_data/vent_overrides.xlsx"
    },


    "use_database": false,
      "filter_by": "meestvoorkomendepostcode",
      "db_filter": {
        "meestvoorkomendepostcode": ["4816BL"], 
        "pand_id": ["0383100000001369", "0383100000001370"],
        "pand_ids": ["XYZ123", "XYZ999"],
        "bbox_xy": [120000.0, 487000.0, 121000.0, 488000.0],
        "bbox_latlon": [52.35, 4.85, 52.37, 4.92]


    },
    "excel_overrides": {
      "override_fenez_excel": false,
      "override_dhw_excel": false,
      "override_epw_excel": false,
      "override_lighting_excel": false,
      "override_hvac_excel": false,
      "override_vent_excel": false
    },
    "user_config_overrides": {
      "override_fenez_json": true,
      "override_dhw_json": true,
      "override_epw_json": true,
      "override_lighting_json": true,
      "override_hvac_json": true,
      "override_vent_json": true,
      "override_geometry_json": true,
      "override_shading_json": false
    },
    "default_dicts": {
      "res_data": {
        "R1": "some_res_data"
      },
      "nonres_data": {
        "NR1": "some_nonres_data"
      },
      "dhw": {
        "Office": {
          "setpoint_c": 55.0
        }
      },
      "epw": [
        {
          "city": "DefaultCity",
          "file": "Default.epw"
        }
      ],
      "lighting": {
        "lights_wm2": 100.0
      },
      "hvac": {
        "heating_day_setpoint": 20.0
      },
      "vent": {
        "infiltration_base": 1.0
      }
    },
    "idf_creation": {
      "perform_idf_creation": true,
      "scenario": "scenario1",
      "calibration_stage": "pre_calibration",
      "strategy": "B",
      "random_seed": 42,
      "iddfile": "EnergyPlus/Energy+.idd",
      "idf_file_path": "EnergyPlus/Minimal.idf",
      "output_idf_dir": "output_IDFs",
      "run_simulations": true,
      "simulate_config": {
        "num_workers": 4,
        "ep_force_overwrite": true
      },
      "post_process": true,
      "post_process_config": {
        "base_output_dir": "Sim_Results",
        "outputs": [
          {
            "convert_to_daily": false,
            "convert_to_monthly": false,
            "aggregator": "none",
            "output_csv": "output/results/merged_as_is.csv"
          },
          {
            "convert_to_daily": true,
            "convert_to_monthly": false,
            "aggregator": "mean",
            "output_csv": "output/results/merged_daily_mean.csv"
          }
        ]
      },
      "output_definitions": {
        "desired_variables": [
          "Facility Total Electric Demand Power",
          "Zone Air Temperature"
        ],
        "desired_meters": [
          "Electricity:Facility"
        ],
        "override_variable_frequency": "Hourly",
        "override_meter_frequency": "Hourly",
        "include_tables": true,
        "include_summary": true
      }
    },
    "structuring": {
      "perform_structuring": true,
      "dhw": {
        "csv_in": "assigned/assigned_dhw_params.csv",
        "csv_out": "assigned/structured_dhw_params.csv"
      },
      "fenestration": {
        "csv_in": "assigned/assigned_fenez_params.csv",
        "csv_out": "assigned/structured_fenez_params.csv"
      },
      "hvac": {
        "csv_in": "assigned/assigned_hvac_params.csv",
        "build_out": "assigned/assigned_hvac_building.csv",
        "zone_out": "assigned/assigned_hvac_zones.csv"
      },
      "vent": {
        "csv_in": "assigned/assigned_ventilation.csv",
        "build_out": "assigned/assigned_vent_building.csv",
        "zone_out": "assigned/assigned_vent_zones.csv"
      }
    },
    "modification": {
      "perform_modification": true,
      "modify_config": {


        "idd_path": "EnergyPlus/Energy+.idd",
        "assigned_csv": {
          "hvac_building": "assigned/assigned_hvac_building.csv",
          "hvac_zones": "assigned/assigned_hvac_zones.csv",
          "dhw": "assigned/assigned_dhw_params.csv",
          "vent_build": "assigned/assigned_vent_building.csv",
          "vent_zones": "assigned/assigned_vent_zones.csv",
          "elec": "assigned/assigned_lighting.csv",
          "fenez": "assigned/structured_fenez_params.csv"
        },
        "scenario_csv": {
          "hvac": "scenarios/scenario_params_hvac.csv",
          "dhw": "scenarios/scenario_params_dhw.csv",
          "vent": "scenarios/scenario_params_vent.csv",
          "elec": "scenarios/scenario_params_elec.csv",
          "fenez": "scenarios/scenario_params_fenez.csv"
        },
        "output_idf_dir": "scenario_idfs",
        "building_id": 4136730,
        "num_scenarios": 5,
        "picking_method": "random_uniform",
        "picking_scale_factor": 0.5,
        "run_simulations": true,
        "simulation_config": {
          "num_workers": 4,
          "output_dir": "Sim_Results/Scenarios"
        },
        "perform_post_process": true,
        "post_process_config": {
          "output_csv_as_is": "results_scenarioes/merged_as_is_scenarios.csv",
          "output_csv_daily_mean": "results_scenarioes/merged_daily_mean_scenarios.csv"
        },
        "perform_validation": false,
        "validation_config": {
          "real_data_csv": "data/mock_merged_daily_mean.csv",
          "sim_data_csv":  "results/merged_daily_mean.csv",
          "bldg_ranges": {
            "0": [0, 1, 2]
          },
          "variables_to_compare": [
            "Electricity:Facility [J](Hourly)",
            "Heating:EnergyTransfer [J](Hourly)",
            "Cooling:EnergyTransfer [J](Hourly)"
          ],
          "threshold_cv_rmse": 30.0,
          "skip_plots": true,
          "output_csv": "scenario_validation_report.csv"
        }
      }
    },

    


  "validation_base": {
    "perform_validation": true,
    "config": {
      "real_data_csv": "data/mock_merged_daily_mean.csv",
      "sim_data_csv":  "results/merged_daily_mean.csv",
      "bldg_ranges": {
        "0": [0,1,2,3]
      },
      "variables_to_compare": [
        "Electricity:Facility [J](Hourly)",
        "Heating:EnergyTransfer [J](Hourly)",
        "Cooling:EnergyTransfer [J](Hourly)"
      ],
      "threshold_cv_rmse": 30.0,
      "skip_plots": true,
      "output_csv": "validation_report_base.csv"
    }
  },

  "validation_scenarios": {
    "perform_validation": true,
    "config": {
      "real_data_csv": "data/mock_merged_daily_mean.csv",
      "sim_data_csv":  "results_scenarioes/merged_daily_mean_scenarios.csv",
      "bldg_ranges": {
        "0": [0,1,2]
      },
      "variables_to_compare": [
        "Electricity:Facility [J](Hourly)",
        "Heating:EnergyTransfer [J](Hourly)",
        "Cooling:EnergyTransfer [J](Hourly)"
      ],
      "threshold_cv_rmse": 30.0,
      "skip_plots": true,
      "output_csv": "validation_report_scenarios.csv"
    }
  },








    "sensitivity": {
      "perform_sensitivity": true,
      "scenario_folder": "scenarios",
      "method": "morris",
      "results_csv": "results_scenarioes/merged_daily_mean_scenarios.csv",
      "target_variable": [
        "Heating:EnergyTransfer [J](Hourly)",
        "Cooling:EnergyTransfer [J](Hourly)",
        "Electricity:Facility [J](Hourly)"
      ],
      "output_csv": "multi_corr_sensitivity.csv",
      "n_morris_trajectories": 10,
      "num_levels": 4
    },
    "surrogate": {
      "perform_surrogate": true,
      "scenario_folder": "scenarios",
      "results_csv": "results_scenarioes/merged_daily_mean_scenarios.csv",
      "target_variable": "Heating:EnergyTransfer [J](Hourly)",
      "model_out": "heating_surrogate_model.joblib",
      "cols_out": "heating_surrogate_columns.joblib",
      "test_size": 0.3
    },
    "calibration": {
      "perform_calibration": true,
      "scenario_folder": "scenarios",
      "scenario_files": [
        "scenario_params_dhw.csv",
        "scenario_params_elec.csv"
      ],
      "subset_sensitivity_csv": "multi_corr_sensitivity.csv",
      "top_n_params": 10,
      "method": "ga",
      "use_surrogate": true,
      "real_data_csv": "data/mock_merged_daily_mean.csv",
      "surrogate_model_path": "heating_surrogate_model.joblib",
      "surrogate_columns_path": "heating_surrogate_columns.joblib",
      "calibrate_min_max": true,
      "ga_pop_size": 10,
      "ga_generations": 5,
      "ga_crossover_prob": 0.7,
      "ga_mutation_prob": 0.2,
      "bayes_n_calls": 15,
      "random_n_iter": 20,
      "output_history_csv": "calibration_history.csv",
      "best_params_folder": "calibrated",
      "history_folder": "calibrated"
    }
  },
  "shading": [
    {
      "building_id": 4136730,
      "param_name": "top_n_buildings",
      "fixed_value": 5
    },
    {
      "building_function": "residential",
      "param_name": "summer_value",
      "min_val": 0.4,
      "max_val": 0.6
    },
    {
      "param_name": "top_n_trees",
      "fixed_value": 3
    }
  ],
  "vent": [
    {
      "building_id": 4136730,
      "param_name": "infiltration_base",
      "min_val": 100.3,
      "max_val": 100.4
    },
    {
      "building_function": "residential",
      "age_range": "1992-2005",
      "param_name": "year_factor",
      "min_val": 21.4,
      "max_val": 21.5
    },
    {
      "building_id": 4136731,
      "param_name": "system_type",
      "fixed_value": "D"
    },
    {
      "building_id": 4136731,
      "param_name": "fan_pressure",
      "min_val": 100,
      "max_val": 120
    }
  ]
}

------------------------------------------------------------

