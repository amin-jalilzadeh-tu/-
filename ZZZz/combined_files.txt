File: D:\Documents\E_Plus_2030_py\cal\unified_sensitivity.py
============================================================
"""
unified_sensitivity.py

Updated to handle multiple target variables for correlation-based sensitivity.
You can specify in your config:
    "target_variable": [
      "Heating:EnergyTransfer [J](Hourly)",
      "Cooling:EnergyTransfer [J](Hourly)",
      ...
    ]
or a single string (like "Heating:EnergyTransfer [J](Hourly)").

Author: Your Team
"""

import os
import numpy as np
import pandas as pd
from typing import Dict, Any, Optional, Union, List

# Attempt SALib imports
try:
    from SALib.sample import morris as morris_sample
    from SALib.sample import saltelli
    from SALib.analyze import morris as morris_analyze
    from SALib.analyze import sobol
    HAVE_SALIB = True
except ImportError:
    HAVE_SALIB = False
    morris_sample = None
    morris_analyze = None
    saltelli = None
    sobol = None


###############################################################################
# 1) HELPERS FOR CATEGORICAL ENCODING & NAME BUILD
###############################################################################

def encode_categorical_if_known(param_name: str, param_value) -> Optional[float]:
    """
    Tries to interpret 'param_value' as numeric:
      1) Direct float conversion
      2) If that fails, attempt known label encodings
      3) Return None if unknown => skip param

    Modify or expand the logic as you see fit to cover more discrete strings.
    """
    if param_value is None or pd.isna(param_value):
        return None

    # (A) Try direct float conversion
    try:
        return float(param_value)
    except (ValueError, TypeError):
        pass  # not a direct float

    # (B) Attempt known label encodings

    # Example 1: "Electricity" -> 0.0, "Gas" -> 1.0
    if param_name.lower().endswith("fuel_type"):
        if param_value == "Electricity":
            return 0.0
        elif param_value == "Gas":
            return 1.0
        return None

    # Example 2: Roughness => "MediumRough" -> 2.0, etc.
    if "roughness" in param_name.lower():
        rough_map = {
            "Smooth": 0.0,
            "MediumSmooth": 1.0,
            "MediumRough": 2.0,
            "Rough": 3.0
        }
        if param_value in rough_map:
            return rough_map[param_value]
        return None

    # Example 3: "Yes"/"No" => 1.0 / 0.0
    if param_value in ["Yes", "No"]:
        return 1.0 if param_value == "Yes" else 0.0

    # Example 4: "SpectralAverage" => encode as 0.0, or skip
    if param_value == "SpectralAverage":
        return 0.0

    # Example 5: Generic "Electricity"/"Gas" if not caught above:
    if param_value == "Electricity":
        return 0.0
    elif param_value == "Gas":
        return 1.0

    # If we get here => no recognized encoding => skip
    return None


def build_unified_param_name(row: pd.Series) -> str:
    """
    Combine columns (zone_name, object_name, sub_key, param_name)
    to produce a single unique param_name in the final DataFrame.
    Modify to your preference.
    """
    base_name = str(row.get("param_name", "UnknownParam"))
    name_parts = []

    zname = row.get("zone_name", None)
    if pd.notna(zname) and isinstance(zname, str) and zname.strip():
        name_parts.append(zname.strip())

    oname = row.get("object_name", None)
    if pd.notna(oname) and isinstance(oname, str) and oname.strip():
        name_parts.append(oname.strip())

    skey = row.get("sub_key", None)
    if pd.notna(skey) and isinstance(skey, str) and skey.strip():
        name_parts.append(skey.strip())

    # Finally the base param_name
    name_parts.append(base_name)

    # Join with double underscore
    return "__".join(name_parts)


###############################################################################
# 2) LOADING SCENARIO PARAMS
###############################################################################

def load_scenario_params(scenario_folder: str) -> pd.DataFrame:
    """
    Reads scenario_params_*.csv from scenario_folder, merges them into
    a single DataFrame with columns:
      ["scenario_index", "param_name", "assigned_value", "param_min", "param_max", "ogc_fid", "source_file"]
    and attempts to convert assigned_value to float or a known label-encoded numeric.

    If it cannot be encoded => we skip that row.
    """
    scenario_files = [
        "scenario_params_dhw.csv",
        "scenario_params_elec.csv",
        "scenario_params_equipment.csv",
        "scenario_params_fenez.csv",
        "scenario_params_hvac.csv",
        "scenario_params_vent.csv"
    ]

    all_rows = []

    for fname in scenario_files:
        fpath = os.path.join(scenario_folder, fname)
        if not os.path.isfile(fpath):
            continue

        df_raw = pd.read_csv(fpath)
        if df_raw.empty:
            continue

        for _, row in df_raw.iterrows():
            scenario_index = row.get("scenario_index", None)
            unified_name = build_unified_param_name(row)

            # assigned_value might be 'assigned_value' or 'param_value'
            val = row.get("assigned_value", None)
            if val is None or pd.isna(val):
                val = row.get("param_value", None)

            # Attempt to convert/encode
            numeric_val = encode_categorical_if_known(unified_name, val)
            if numeric_val is None:
                print(f"[WARNING] Skipping param '{unified_name}' => "
                      f"no numeric encoding for value: {val}")
                continue

            # param_min / param_max
            pmin = row.get("param_min", np.nan)
            pmax = row.get("param_max", np.nan)

            # ogc_fid
            ogc_fid = row.get("ogc_fid", None)

            all_rows.append({
                "scenario_index": scenario_index,
                "param_name": unified_name,
                "assigned_value": numeric_val,
                "param_min": pmin,
                "param_max": pmax,
                "ogc_fid": ogc_fid,
                "source_file": fname
            })

    if not all_rows:
        print(f"[WARNING] No valid numeric parameters found in '{scenario_folder}' (all skipped?)")
        return pd.DataFrame()

    return pd.DataFrame(all_rows)


###############################################################################
# 3) CORRELATION-BASED SENSITIVITY (SINGLE or MULTIPLE Variables)
###############################################################################

def correlation_sensitivity(
    df_scenarios: pd.DataFrame,
    df_results: pd.DataFrame,
    target_variables: Union[str, List[str]],
    scenario_index_col: str = "scenario_index",
    assigned_val_col: str = "assigned_value"
) -> pd.DataFrame:
    """
    Performs correlation-based sensitivity between each parameter and
    one or more target variables from the results.

    If target_variables is a single string, we produce a DF with:
       [Parameter, Correlation, AbsCorrelation]

    If target_variables is a list of strings, we produce one row per param,
    with correlation columns for each variable, e.g.:
       [Parameter,
        Corr_<var1>, AbsCorr_<var1>,
        Corr_<var2>, AbsCorr_<var2>, ...
       ]

    Steps:
      1) Pivot df_scenarios => wide (index=scenario_index, columns=param_name)
      2) Melt df_results => sum across days => pivot wide so each variable has
         its own column.
      3) Merge scenario pivot with results pivot
      4) Correlate each param col with each variable col

    Returns a DataFrame of correlation results.
    """
    # Normalize target_variables to a list
    if isinstance(target_variables, str):
        target_vars_list = [target_variables]
    elif isinstance(target_variables, list):
        target_vars_list = target_variables
    else:
        raise ValueError("target_variables must be a string or a list of strings.")

    # 1) Pivot scenario => wide
    pivot_df = df_scenarios.pivot_table(
        index=scenario_index_col,
        columns="param_name",
        values=assigned_val_col,
        aggfunc="first"
    ).reset_index()

    # 2) Melt results & sum across days
    if "BuildingID" in df_results.columns and scenario_index_col != "BuildingID":
        df_results = df_results.rename(columns={"BuildingID": scenario_index_col})

    melted = df_results.melt(
        id_vars=[scenario_index_col, "VariableName"],
        var_name="Day",
        value_name="Value"
    )
    daily_sum = melted.groupby([scenario_index_col, "VariableName"])["Value"].sum().reset_index()
    daily_sum.rename(columns={"Value": "SumValue"}, inplace=True)

    # 3) Pivot variables => each var a column
    #    If multiple target variables, we want them all to appear as columns
    #    in the pivot. If you have more variables than in target_vars_list,
    #    that's OK; we can keep them, or filter them.
    pivot_results = daily_sum.pivot(
        index=scenario_index_col,
        columns="VariableName",
        values="SumValue"
    ).reset_index()

    # If you want to filter pivot_results to only keep the target vars:
    all_cols = list(pivot_results.columns)
    # the first is scenario_index_col, so skip it
    keep_cols = [scenario_index_col]
    # keep only columns in target_vars_list if they exist
    for varname in target_vars_list:
        if varname in all_cols:
            keep_cols.append(varname)
    pivot_results = pivot_results[keep_cols]

    # 4) Merge scenario pivot with results pivot
    merged = pd.merge(
        pivot_df,
        pivot_results,
        on=scenario_index_col,
        how="inner"
    )

    # Now columns = [scenario_index, paramA, paramB, ..., var1, var2, ...]
    # We do correlation paramX vs. varY for each pair
    # param_cols are the scenario param columns, var_cols are the target variable columns
    var_cols = [c for c in pivot_results.columns if c != scenario_index_col]
    param_cols = [c for c in pivot_df.columns if c != scenario_index_col]

    # If there's only 1 target var, produce the old layout.
    if len(var_cols) == 1:
        var_col = var_cols[0]
        corr_list = []
        for col in param_cols:
            if pd.api.types.is_numeric_dtype(merged[col]):
                cval = merged[[col, var_col]].corr().iloc[0, 1]
                corr_list.append((col, cval))
            else:
                corr_list.append((col, np.nan))
        corr_df = pd.DataFrame(corr_list, columns=["Parameter", "Correlation"])
        corr_df["AbsCorrelation"] = corr_df["Correlation"].abs()
        corr_df.sort_values("AbsCorrelation", ascending=False, inplace=True)
        return corr_df

    # Else multiple variables => produce one row per param, with correlation columns for each var
    # e.g. param, Corr_var1, AbsCorr_var1, Corr_var2, AbsCorr_var2, ...
    corr_rows = []
    for col in param_cols:
        row_dict = {"Parameter": col}
        if not pd.api.types.is_numeric_dtype(merged[col]):
            # skip or store NaNs
            for v in var_cols:
                row_dict[f"Corr_{v}"] = np.nan
                row_dict[f"AbsCorr_{v}"] = np.nan
        else:
            # param is numeric
            for v in var_cols:
                if not pd.api.types.is_numeric_dtype(merged[v]):
                    row_dict[f"Corr_{v}"] = np.nan
                    row_dict[f"AbsCorr_{v}"] = np.nan
                else:
                    cval = merged[[col, v]].corr().iloc[0, 1]
                    row_dict[f"Corr_{v}"] = cval
                    row_dict[f"AbsCorr_{v}"] = abs(cval)
        corr_rows.append(row_dict)

    corr_df = pd.DataFrame(corr_rows)
    # Optionally you can sort by one variable's AbsCorr_ if you want:
    # e.g. if the first var is var_cols[0], sort by that
    # but let's just leave it unsorted or sort by "Parameter"
    corr_df.sort_values("Parameter", inplace=True)
    return corr_df


###############################################################################
# 4) SALib UTILS: EXTRACT RANGES, BUILD PROBLEM
###############################################################################

def extract_parameter_ranges(
    merged_df: pd.DataFrame,
    param_name_col: str = "param_name",
    assigned_val_col: str = "assigned_value",
    param_min_col: str = "param_min",
    param_max_col: str = "param_max"
) -> pd.DataFrame:
    """
    Builds DF [name, min_value, max_value].
    If param_min / param_max are missing/invalid, fallback to Â±20% around assigned_value.
    """
    out_rows = []
    unique_params = merged_df[param_name_col].unique()
    for p in unique_params:
        sub = merged_df[merged_df[param_name_col] == p]
        row = sub.iloc[0]  # any row for param p

        assigned_val = row.get(assigned_val_col, np.nan)
        if pd.isna(assigned_val):
            continue

        pmin = row.get(param_min_col, np.nan)
        pmax = row.get(param_max_col, np.nan)
        try:
            pmin = float(pmin)
        except:
            pmin = np.nan
        try:
            pmax = float(pmax)
        except:
            pmax = np.nan

        if np.isnan(pmin) or np.isnan(pmax) or (pmin >= pmax):
            base = float(assigned_val)
            delta = abs(base) * 0.2
            pmin = base - delta
            pmax = base + delta
            if pmin >= pmax:
                pmax += 1e-4

        out_rows.append({
            "name": p,
            "min_value": pmin,
            "max_value": pmax
        })

    return pd.DataFrame(out_rows)


def build_salib_problem(params_df: pd.DataFrame) -> Dict[str, Any]:
    """
    Convert DF [name, min_value, max_value] => SALib problem dict
    """
    return {
        "num_vars": len(params_df),
        "names": params_df["name"].tolist(),
        "bounds": [
            [row["min_value"], row["max_value"]]
            for _, row in params_df.iterrows()
        ]
    }


###############################################################################
# 5) SALib: MORRIS & SOBOL
###############################################################################

def default_simulation_function(param_dict: Dict[str, float]) -> float:
    """
    Example: sum of param_dict + random noise.
    Replace with your E+ or Surrogate call if you want real analysis.
    """
    base_sum = sum(param_dict.values())
    noise = np.random.uniform(-0.5, 0.5)
    return base_sum + noise


def run_morris_method(params_meta: pd.DataFrame, simulate_func, n_trajectories=10, num_levels=4):
    """
    SALib Morris
    """
    if not HAVE_SALIB:
        raise ImportError("SALib not installed. Cannot run Morris.")
    problem = build_salib_problem(params_meta)
    X = morris_sample.sample(problem, N=n_trajectories, num_levels=num_levels)
    Y = []
    for row in X:
        param_dict = {}
        for i, name in enumerate(problem["names"]):
            param_dict[name] = row[i]
        Y.append(simulate_func(param_dict))
    Y = np.array(Y)
    res = morris_analyze.analyze(problem, X, Y, conf_level=0.95, print_to_console=False)
    return res, X, Y


def run_sobol_method(params_meta: pd.DataFrame, simulate_func, n_samples=256):
    """
    SALib Sobol
    """
    if not HAVE_SALIB:
        raise ImportError("SALib not installed. Cannot run Sobol.")
    problem = build_salib_problem(params_meta)
    X = saltelli.sample(problem, n_samples, calc_second_order=True)
    Y = []
    for row in X:
        param_dict = {}
        for i, name in enumerate(problem["names"]):
            param_dict[name] = row[i]
        Y.append(simulate_func(param_dict))
    Y = np.array(Y)
    sres = sobol.analyze(problem, Y, calc_second_order=True, print_to_console=False)
    return sres, X, Y


###############################################################################
# 6) MAIN ORCHESTRATION
###############################################################################
def run_sensitivity_analysis(
    scenario_folder: str,
    method: str = "morris",
    results_csv: Optional[str] = None,
    target_variable: Union[str, List[str], None] = None,
    output_csv: str = "sensitivity_output.csv",
    n_morris_trajectories: int = 10,
    num_levels: int = 4,
    n_sobol_samples: int = 256
):
    """
    Called from main.py to do correlation, Morris, or Sobol sensitivity.
    Now supports multiple target variables in correlation-based approach.

    :param scenario_folder: path to folder with scenario_params_*.csv
    :param method: "correlation", "morris", or "sobol"
    :param results_csv: path to results CSV (for correlation)
    :param target_variable: string or list of strings (for correlation).
    :param output_csv: results file
    :param n_morris_trajectories: int
    :param num_levels: Morris design
    :param n_sobol_samples: int
    """
    print(f"[INFO] run_sensitivity_analysis => method={method}, folder='{scenario_folder}'")

    # 1) Load scenario parameters
    df_params = load_scenario_params(scenario_folder)
    if df_params.empty:
        print("[WARNING] No numeric scenario parameters => no analysis.")
        return

    # 2) If correlation-based
    if method.lower() == "correlation":
        if not results_csv or not os.path.isfile(results_csv):
            raise ValueError("For correlation-based, must provide a valid results_csv.")
        if not target_variable:
            raise ValueError("For correlation-based, must provide target_variable (string or list).")

        df_res = pd.read_csv(results_csv)
        corr_df = correlation_sensitivity(
            df_scenarios=df_params,
            df_results=df_res,
            target_variables=target_variable
        )
        corr_df.to_csv(output_csv, index=False)
        print(f"[INFO] Correlation-based results => {output_csv}")
        return

    # 3) SALib-based => extract ranges
    params_meta = extract_parameter_ranges(df_params)
    if params_meta.empty:
        print("[WARNING] All parameters were invalid or had no numeric data => no SALib analysis.")
        return

    # Replace with real E+ or surrogate
    simulate_func = default_simulation_function

    if method.lower() == "morris":
        # Morris
        res, X, Y = run_morris_method(
            params_meta=params_meta,
            simulate_func=simulate_func,
            n_trajectories=n_morris_trajectories,
            num_levels=num_levels
        )
        df_out = pd.DataFrame({
            "param": params_meta["name"].values,
            "mu_star": res["mu_star"],
            "mu_star_conf": res["mu_star_conf"],
            "sigma": res["sigma"]
        })
        df_out.to_csv(output_csv, index=False)
        print(f"[INFO] Morris sensitivity results => {output_csv}")

    elif method.lower() == "sobol":
        # Sobol
        sres, X, Y = run_sobol_method(
            params_meta=params_meta,
            simulate_func=simulate_func,
            n_samples=n_sobol_samples
        )
        df_out = pd.DataFrame({
            "param": params_meta["name"].values,
            "S1": sres["S1"],
            "S1_conf": sres["S1_conf"],
            "ST": sres["ST"],
            "ST_conf": sres["ST_conf"]
        })
        df_out.to_csv(output_csv, index=False)
        print(f"[INFO] Sobol sensitivity results => {output_csv}")

    else:
        raise ValueError(f"Unknown method='{method}'. Must be 'correlation','morris','sobol'.")

------------------------------------------------------------

File: D:\Documents\E_Plus_2030_py\cal\main cal.py
============================================================
"""
main cal.py

Example script that demonstrates how to call functions
from unified_sensitivity.py for different sensitivity methods.
"""

from cal.unified_sensitivity import run_sensitivity_analysis

def main():
    scenario_folder = r"D:\Documents\E_Plus_2030_py\output\scenarios"

    # Example 1: Correlation-based analysis
    # Uncomment and adjust paths/variable to use correlation analysis:
    """
    run_sensitivity_analysis(
        scenario_folder=scenario_folder,
        method="correlation",
        results_csv=r"D:\Documents\E_Plus_2030_py\output\results\merged_daily_mean_mocked.csv",
        target_variable="Heating:EnergyTransfer [J](Hourly)",
        output_csv="correlation_sensitivity.csv"
    )
    """

    # Example 2: Morris analysis
    run_sensitivity_analysis(
        scenario_folder=scenario_folder,
        method="morris",
        param_min_col="param_min",
        param_max_col="param_max",
        output_csv="morris_sensitivity.csv",
        n_morris_trajectories=10,
        num_levels=4
    )

    # Example 3: Sobol analysis
    """
    run_sensitivity_analysis(
        scenario_folder=scenario_folder,
        method="sobol",
        param_min_col="param_min",
        param_max_col="param_max",
        output_csv="sobol_sensitivity.csv",
        n_sobol_samples=128
    )
    """

if __name__ == "__main__":
    main()







"""
main.py

Example usage of the functions/classes from unified_surrogate.py:
1) Load scenario CSVs from a folder
2) Pivot into a wide DataFrame
3) (Optionally) filter top parameters
4) Load & aggregate simulation results
5) Merge to get X,y
6) Train & save surrogate
7) Reload & predict
"""

import os
import pandas as pd
from cal.unified_surrogate import (
    load_scenario_params,
    pivot_scenario_params,
    filter_top_parameters,
    load_sim_results,
    aggregate_results,
    merge_params_with_results,
    build_and_save_surrogate,
    load_surrogate_and_predict
)

def main():
    # A) scenario CSVs
    scenario_folder = r"D:\Documents\E_Plus_2030_py\output\scenarios"
    if not os.path.isdir(scenario_folder):
        print(f"[ERROR] Scenario folder not found: {scenario_folder}")
        return

    df_scen = load_scenario_params(scenario_folder)
    pivot_df = pivot_scenario_params(df_scen)
    print("[INFO] pivot shape:", pivot_df.shape)

    # Optional: Filter top parameters from a sensitivity file
    # pivot_df = filter_top_parameters(pivot_df, "morris_sensitivity.csv", top_n=5)

    # B) Load & aggregate sim results
    results_csv = r"D:\Documents\E_Plus_2030_py\output\results\merged_daily_mean_mocked.csv"
    if not os.path.isfile(results_csv):
        print(f"[ERROR] Results CSV not found: {results_csv}")
        return

    df_sim = load_sim_results(results_csv)
    df_agg = aggregate_results(df_sim)

    # C) Merge param pivot with aggregated results for a chosen target var
    target_variable = "Heating:EnergyTransfer [J](Hourly)"
    merged_df = merge_params_with_results(pivot_df, df_agg, target_variable)

    # We'll rename "TotalEnergy_J" -> "target"
    merged_df.rename(columns={"TotalEnergy_J": "target"}, inplace=True)

    # D) Build & Save Surrogate
    model_out = "heating_surrogate_model.joblib"
    col_out   = "heating_surrogate_columns.joblib"
    rf_model, trained_cols = build_and_save_surrogate(
        df_data=merged_df,
        target_col="target",
        model_out_path=model_out,
        columns_out_path=col_out,
        test_size=0.3,
        random_state=42
    )

    if rf_model is None:
        print("[ERROR] Surrogate training unsuccessful or insufficient data.")
        return

    # E) Reload & Predict (using the first row as a sample)
    sample_row = merged_df.iloc[0].drop(["BuildingID","ogc_fid","VariableName","target"])
    sample_dict = sample_row.to_dict()
    y_pred = load_surrogate_and_predict(model_out, col_out, sample_dict)

    print(f"\n[SAMPLE PREDICTION] => {y_pred[0]:.2f} J (approx) for {sample_dict}")

if __name__ == "__main__":
    main()







"""
main.py

Example script that demonstrates how to call the calibration methods
from unified_calibration.py to run random search, GA, or Bayesian optimization.
"""

import os
from unified_calibration import (
    load_scenario_params,
    build_param_specs_from_scenario,
    simulate_or_surrogate,
    CalibrationManager,
    save_history_to_csv
)

def main():
    """
    1) Load scenario param CSVs => get param_min, param_max
    2) Build param_specs
    3) Define a calibration objective => simulate_or_surrogate
    4) Choose method (random, ga, bayes)
    5) Save best params + error, optional CSV log
    """
    scenario_folder = r"D:\Documents\E_Plus_2030_py\output\scenarios"
    if not os.path.isdir(scenario_folder):
        print(f"[ERROR] Scenario folder not found: {scenario_folder}")
        return

    print("[INFO] Loading scenario data")
    df_scen = load_scenario_params(scenario_folder)

    # Build param specs
    param_specs = build_param_specs_from_scenario(df_scen)
    print(f"[INFO] Found {len(param_specs)} unique parameters from scenario data.")
    for spec in param_specs[:5]:
        print(f"   - {spec.name}: {spec.min_value} .. {spec.max_value}, int={spec.is_integer}")

    # Create a manager with our placeholder evaluation function
    manager = CalibrationManager(param_specs, simulate_or_surrogate)

    # EXAMPLE: run GA
    print("\n=== Running GA Calibration ===")
    best_params_ga, best_err_ga, hist_ga = manager.run_calibration(
        method="ga",
        pop_size=10,
        generations=5,
        crossover_prob=0.7,
        mutation_prob=0.2
    )
    print(f"[GA] Best error={best_err_ga:.3f}, best params={best_params_ga}")
    save_history_to_csv(hist_ga, "calibration_history_ga.csv")

    # EXAMPLE: run Bayesian
    print("\n=== Running Bayesian Calibration ===")
    best_params_bayes, best_err_bayes, hist_bayes = manager.run_calibration(
        method="bayes",
        n_calls=15
    )
    print(f"[BAYES] Best error={best_err_bayes:.3f}, best params={best_params_bayes}")
    save_history_to_csv(hist_bayes, "calibration_history_bayes.csv")

    # EXAMPLE: run Random
    print("\n=== Running Random Search Calibration ===")
    best_params_rand, best_err_rand, hist_rand = manager.run_calibration(
        method="random",
        n_iterations=20
    )
    print(f"[RANDOM] Best error={best_err_rand:.3f}, best params={best_params_rand}")
    save_history_to_csv(hist_rand, "calibration_history_random.csv")

    # Compare final best
    results = [
        ("GA", best_params_ga, best_err_ga),
        ("Bayes", best_params_bayes, best_err_bayes),
        ("Random", best_params_rand, best_err_rand)
    ]
    results.sort(key=lambda x: x[2])  # sort by error ascending
    print("\n=== Overall Best Among the 3 ===")
    print(f"Method={results[0][0]}, error={results[0][2]:.3f}, params={results[0][1]}")

if __name__ == "__main__":
    main()

------------------------------------------------------------

File: D:\Documents\E_Plus_2030_py\orchestrator.py
============================================================
"""
orchestrator.py

Orchestrates the entire EnergyPlus workflow using a job-specific subfolder
for config files and a job-specific folder in /output for results.

Steps:
  1. Retrieve 'job_id' from job_config (set by job_manager or app).
  2. Form an output directory: <OUTPUT_DIR>/<job_id>.
  3. Load main_config.json from user_configs/<job_id>.
  4. Merge with posted_data["main_config"] if present.
  5. Apply Excel overrides, JSON overrides, create IDFs, run sims, etc.
  6. If scenario modification is enabled, override paths so scenario IDFs/results
     stay in the same job folder, then run scenario-based modifications.
  7. Perform structuring (e.g., flatten assigned CSVs) if requested.
  8. Perform global validation, sensitivity, surrogate, calibration if requested;
     patch any relative CSV paths to be inside the job folder (unless "data/").
  9. Zip & email final results if mail_user.json is present.
  10. Respect any cancel_event from job_manager.
"""

import os
import json
import logging
import threading
import time
from contextlib import contextmanager
import pandas as pd

# Splitting / deep-merge
from splitter import deep_merge_dicts

# DB loading if needed
from database_handler import load_buildings_from_db

# Excel overrides
from excel_overrides import (
    override_dhw_lookup_from_excel_file,
    override_epw_lookup_from_excel_file,
    override_lighting_lookup_from_excel_file,
    override_hvac_lookup_from_excel_file,
    override_vent_lookup_from_excel_file
)

# Fenestration config
from idf_objects.fenez.fenez_config_manager import build_fenez_config


# IDF creation
import idf_creation
from idf_creation import create_idfs_for_all_buildings

# Scenario modification
from main_modifi import run_modification_workflow

# Validation
from validation.main_validation import run_validation_process

# Sensitivity / Surrogate / Calibration
from cal.unified_sensitivity import run_sensitivity_analysis
from cal.unified_surrogate import (
    load_scenario_params as sur_load_scenario_params,
    pivot_scenario_params,
    load_sim_results,
    aggregate_results,
    merge_params_with_results,
    build_and_save_surrogate
)
from cal.unified_calibration import run_unified_calibration

# Zip & email
from zip_and_mail import zip_user_output, send_results_email

from cleanup_old_jobs import cleanup_old_results


class WorkflowCanceled(Exception):
    """Custom exception used to stop the workflow if a cancel_event is set."""
    pass


@contextmanager
def step_timer(logger, name: str):
    """Context manager to log step durations."""
    logger.info(f"[STEP] Starting {name} ...")
    start = time.perf_counter()
    try:
        yield
    finally:
        elapsed = time.perf_counter() - start
        logger.info(f"[STEP] Finished {name} in {elapsed:.2f} seconds.")


def orchestrate_workflow(job_config: dict, cancel_event: threading.Event = None):
    """
    Orchestrates the entire E+ workflow using a job-specific subfolder for config JSON,
    plus a job-specific output folder for results.

    Args:
        job_config (dict): includes:
            {
              "job_id": "<unique_id_for_this_job>",
              "job_subfolder": "user_configs/<job_id>",
              "posted_data": {...} (optional),
              ...
            }
        cancel_event (threading.Event): If set, we gracefully exit early.

    Raises:
        WorkflowCanceled if cancel_event is set mid-way.

    Returns:
        None (logs extensively, optionally zips/emails final results).
    """
    logger = logging.getLogger(__name__)
    logger.info("=== Starting orchestrate_workflow ===")
    overall_start = time.perf_counter()

    # -------------------------------------------------------------------------
    # 0) Identify job_id, define check_canceled
    # -------------------------------------------------------------------------
    job_id = job_config.get("job_id", "unknown_job_id")
    logger.info(f"[INFO] Orchestrator for job_id={job_id}")

    def check_canceled():
        """Raise WorkflowCanceled if cancel_event is set."""
        if cancel_event and cancel_event.is_set():
            logger.warning("=== CANCEL event detected. Stopping workflow. ===")
            raise WorkflowCanceled("Workflow was canceled by user request.")


    # -------------------------------------------------------------------------
    # 12) Helper to handle patching CSVs that are "relative" but not "data/".
    # -------------------------------------------------------------------------
    def patch_if_relative(csv_path: str):
        """
        1) If absolute, return as-is.
        2) If starts with 'data/', interpret as /usr/src/app/data/... (no job folder).
        3) Else, join with job_output_dir.
        """
        if not csv_path:
            return csv_path
        if os.path.isabs(csv_path):
            return csv_path
        if csv_path.startswith("data/"):
            return os.path.join("/usr/src/app", csv_path)
        return os.path.join(job_output_dir, csv_path)





    # -------------------------------------------------------------------------
    # 1) Identify the user_configs folder (where main_config.json resides)
    # -------------------------------------------------------------------------
    user_configs_folder = job_config.get("job_subfolder")
    if not user_configs_folder or not os.path.isdir(user_configs_folder):
        logger.error(f"[ERROR] job_subfolder not found or invalid => {user_configs_folder}")
        return

    # -------------------------------------------------------------------------
    # 2) Build an output directory for this job under OUTPUT_DIR
    #    e.g. /usr/src/app/output/<job_id>
    # -------------------------------------------------------------------------
    env_out_dir = os.environ.get("OUTPUT_DIR", "/usr/src/app/output")
    job_output_dir = os.path.join(env_out_dir, job_id)
    os.makedirs(job_output_dir, exist_ok=True)
    logger.info(f"[INFO] Using job-specific output folder: {job_output_dir}")

    # -------------------------------------------------------------------------
    # 3) Load main_config.json from user_configs/<job_id>
    # -------------------------------------------------------------------------
    main_config_path = os.path.join(user_configs_folder, "main_config.json")
    if not os.path.isfile(main_config_path):
        logger.error(f"[ERROR] Cannot find main_config.json at {main_config_path}")
        return

    with open(main_config_path, "r") as f:
        existing_config_raw = json.load(f)
    main_config = existing_config_raw.get("main_config", {})
    logger.info(f"[INFO] Loaded existing main_config from {main_config_path}.")

    # Merge posted_data["main_config"] if present
    posted_data = job_config.get("posted_data", {})
    if "main_config" in posted_data:
        logger.info("[INFO] Deep merging posted_data['main_config'] into main_config.")
        deep_merge_dicts(main_config, posted_data["main_config"])
        # optionally re-save
        with open(main_config_path, "w") as f:
            json.dump({"main_config": main_config}, f, indent=2)

    # -------------------------------------------------------------------------
    # 4) Extract sub-sections from main_config
    # -------------------------------------------------------------------------
    check_canceled()
    paths_dict       = main_config.get("paths", {})
    excel_flags      = main_config.get("excel_overrides", {})
    user_flags       = main_config.get("user_config_overrides", {})
    def_dicts        = main_config.get("default_dicts", {})
    structuring_cfg  = main_config.get("structuring", {})
    modification_cfg = main_config.get("modification", {})
    validation_cfg   = main_config.get("validation", {})
    sens_cfg         = main_config.get("sensitivity", {})
    sur_cfg          = main_config.get("surrogate", {})
    cal_cfg          = main_config.get("calibration", {})

    # IDF creation block
    idf_cfg = main_config.get("idf_creation", {})
    perform_idf_creation = idf_cfg.get("perform_idf_creation", False)
    scenario             = idf_cfg.get("scenario", "scenario1")
    calibration_stage    = idf_cfg.get("calibration_stage", "pre_calibration")
    strategy             = idf_cfg.get("strategy", "B")
    random_seed          = idf_cfg.get("random_seed", 42)
    run_simulations      = idf_cfg.get("run_simulations", True)
    simulate_config      = idf_cfg.get("simulate_config", {})
    post_process         = idf_cfg.get("post_process", True)
    post_process_config  = idf_cfg.get("post_process_config", {})
    output_definitions   = idf_cfg.get("output_definitions", {})
    use_database         = main_config.get("use_database", False)
    db_filter            = main_config.get("db_filter", {})
    filter_by            = main_config.get("filter_by")  # if using DB

    # Summarize which major steps will run
    steps_to_run = []
    if perform_idf_creation:
        steps_to_run.append("IDF creation")
        if run_simulations:
            steps_to_run.append("simulations")
    if structuring_cfg.get("perform_structuring", False):
        steps_to_run.append("structuring")
    if modification_cfg.get("perform_modification", False):
        steps_to_run.append("modification")
    if main_config.get("validation_base", {}).get("perform_validation", False):
        steps_to_run.append("base validation")
    if main_config.get("validation_scenarios", {}).get("perform_validation", False):
        steps_to_run.append("scenario validation")
    if sens_cfg.get("perform_sensitivity", False):
        steps_to_run.append("sensitivity analysis")
    if sur_cfg.get("perform_surrogate", False):
        steps_to_run.append("surrogate modeling")
    if cal_cfg.get("perform_calibration", False):
        steps_to_run.append("calibration")

    if steps_to_run:
        logger.info("[INFO] Steps to execute: " + ", ".join(steps_to_run))
    else:
        logger.info("[INFO] No major steps are enabled in configuration.")

    # -------------------------------------------------------------------------
    # 5) Possibly override idf_creation.idf_config from env, then force IDFs
    #    to go in <job_output_dir>/output_IDFs
    # -------------------------------------------------------------------------
    check_canceled()

    env_idd_path = os.environ.get("IDD_PATH")
    if env_idd_path:
        idf_creation.idf_config["iddfile"] = env_idd_path
    env_base_idf = os.environ.get("BASE_IDF_PATH")
    if env_base_idf:
        idf_creation.idf_config["idf_file_path"] = env_base_idf

    job_idf_dir = os.path.join(job_output_dir, "output_IDFs")
    os.makedirs(job_idf_dir, exist_ok=True)
    idf_creation.idf_config["output_dir"] = job_idf_dir

    # If user explicitly set these in main_config, override again
    if "iddfile" in idf_cfg:
        idf_creation.idf_config["iddfile"] = idf_cfg["iddfile"]
    if "idf_file_path" in idf_cfg:
        idf_creation.idf_config["idf_file_path"] = idf_cfg["idf_file_path"]

    if "output_idf_dir" in idf_cfg:
        subfolder = idf_cfg["output_idf_dir"]  # e.g. "output_IDFs"
        full_dir = os.path.join(job_output_dir, subfolder)
        idf_creation.idf_config["output_dir"] = full_dir
    else:
        idf_creation.idf_config["output_dir"] = os.path.join(job_output_dir, "output_IDFs")

    # -------------------------------------------------------------------------
    # 6) Setup default dictionaries
    # -------------------------------------------------------------------------
    base_res_data    = def_dicts.get("res_data", {})
    base_nonres_data = def_dicts.get("nonres_data", {})
    dhw_lookup       = def_dicts.get("dhw", {})
    epw_lookup       = def_dicts.get("epw", [])
    lighting_lookup  = def_dicts.get("lighting", {})
    hvac_lookup      = def_dicts.get("hvac", {})
    vent_lookup      = def_dicts.get("vent", {})

    # -------------------------------------------------------------------------
    # 7) Apply Excel overrides if flags are set
    # -------------------------------------------------------------------------
    check_canceled()

    updated_res_data, updated_nonres_data = build_fenez_config(
        base_res_data=base_res_data,
        base_nonres_data=base_nonres_data,
        excel_path=paths_dict.get("fenez_excel", ""),
        do_excel_override=excel_flags.get("override_fenez_excel", False),
        user_fenez_overrides=[]
    )

    if excel_flags.get("override_dhw_excel", False):
        dhw_lookup = override_dhw_lookup_from_excel_file(
            dhw_excel_path=paths_dict.get("dhw_excel", ""),
            default_dhw_lookup=dhw_lookup,
            override_dhw_flag=True
        )

    if excel_flags.get("override_epw_excel", False):
        epw_lookup = override_epw_lookup_from_excel_file(
            epw_excel_path=paths_dict.get("epw_excel", ""),
            epw_lookup=epw_lookup,
            override_epw_flag=True
        )

    if excel_flags.get("override_lighting_excel", False):
        lighting_lookup = override_lighting_lookup_from_excel_file(
            lighting_excel_path=paths_dict.get("lighting_excel", ""),
            lighting_lookup=lighting_lookup,
            override_lighting_flag=True
        )

    if excel_flags.get("override_hvac_excel", False):
        hvac_lookup = override_hvac_lookup_from_excel_file(
            hvac_excel_path=paths_dict.get("hvac_excel", ""),
            hvac_lookup=hvac_lookup,
            override_hvac_flag=True
        )

    if excel_flags.get("override_vent_excel", False):
        vent_lookup = override_vent_lookup_from_excel_file(
            vent_excel_path=paths_dict.get("vent_excel", ""),
            vent_lookup=vent_lookup,
            override_vent_flag=True
        )

    # -------------------------------------------------------------------------
    # 8) JSON overrides from user_configs/<job_id> if user_flags are set
    # -------------------------------------------------------------------------
    check_canceled()

    def safe_load_subjson(fname, key):
        """
        Loads user_configs/<job_id>/fname if it exists, returns data.get(key).
        """
        full_path = os.path.join(user_configs_folder, fname)
        if os.path.isfile(full_path):
            try:
                with open(full_path, "r") as ff:
                    data = json.load(ff)
                return data.get(key)
            except Exception as e:
                logger.error(f"[ERROR] loading {fname} => {e}")
        return None

    # Fenestration
    user_fenez_data = []
    if user_flags.get("override_fenez_json", False):
        loaded = safe_load_subjson("fenestration.json", "fenestration")
        if loaded:
            user_fenez_data = loaded

    updated_res_data, updated_nonres_data = build_fenez_config(
        base_res_data=updated_res_data,
        base_nonres_data=updated_nonres_data,
        excel_path="",
        do_excel_override=False,
        user_fenez_overrides=user_fenez_data
    )

    # DHW
    user_config_dhw = None
    if user_flags.get("override_dhw_json", False):
        user_config_dhw = safe_load_subjson("dhw.json", "dhw")

    # EPW
    user_config_epw = []
    if user_flags.get("override_epw_json", False):
        e = safe_load_subjson("epw.json", "epw")
        if e:
            user_config_epw = e

    # Lighting
    user_config_lighting = None
    if user_flags.get("override_lighting_json", False):
        user_config_lighting = safe_load_subjson("lighting.json", "lighting")

    # HVAC
    user_config_hvac = None
    if user_flags.get("override_hvac_json", False):
        user_config_hvac = safe_load_subjson("hvac.json", "hvac")

    # Vent
    user_config_vent = []
    if user_flags.get("override_vent_json", False):
        v = safe_load_subjson("vent.json", "vent")
        if v:
            user_config_vent = v

    # Geometry
    geom_data = {}
    if user_flags.get("override_geometry_json", False):
        g = safe_load_subjson("geometry.json", "geometry")
        if g:
            geom_data["geometry"] = g

    # Shading
    shading_data = {}
    if user_flags.get("override_shading_json", False):
        s = safe_load_subjson("shading.json", "shading")
        if s:
            shading_data["shading"] = s

    # -------------------------------------------------------------------------
    # 9) IDF creation
    # -------------------------------------------------------------------------
    check_canceled()
    df_buildings = pd.DataFrame()

    if perform_idf_creation:
        logger.info("[INFO] IDF creation is ENABLED.")
        with step_timer(logger, "IDF creation and simulations"):
            # a) Load building data
            if use_database:
                logger.info("[INFO] Loading building data from DB.")
                if not filter_by:
                    raise ValueError("[ERROR] 'filter_by' must be specified when 'use_database' is True.")
                df_buildings = load_buildings_from_db(db_filter, filter_by)

                # Optionally save the raw DB buildings
                extracted_csv_path = os.path.join(job_output_dir, "extracted_buildings.csv")
                df_buildings.to_csv(extracted_csv_path, index=False)
                logger.info(f"[INFO] Saved extracted buildings to {extracted_csv_path}")

            else:
                bldg_data_path = paths_dict.get("building_data", "")
                if os.path.isfile(bldg_data_path):
                    df_buildings = pd.read_csv(bldg_data_path)
                else:
                    logger.warning(f"[WARN] building_data CSV not found => {bldg_data_path}")

            logger.info(f"[INFO] Number of buildings to simulate: {len(df_buildings)}")

            # b) Create IDFs & (optionally) run sims in job folder
            df_buildings = create_idfs_for_all_buildings(
                df_buildings=df_buildings,
                scenario=scenario,
                calibration_stage=calibration_stage,
                strategy=strategy,
                random_seed=random_seed,
                user_config_geom=geom_data.get("geometry", []),
                user_config_lighting=user_config_lighting,
                user_config_dhw=user_config_dhw,
                res_data=updated_res_data,
                nonres_data=updated_nonres_data,
                user_config_hvac=user_config_hvac,
                user_config_vent=user_config_vent,
                user_config_epw=user_config_epw,
                output_definitions=output_definitions,
                run_simulations=run_simulations,
                simulate_config=simulate_config,
                post_process=post_process,
                post_process_config=post_process_config,
                logs_base_dir=job_output_dir
            )

            # === Store the mapping (ogc_fid -> idf_name) so we can look it up later ===
            idf_map_csv = os.path.join(job_output_dir, "extracted_idf_buildings.csv")
            df_buildings.to_csv(idf_map_csv, index=False)
            logger.info(f"[INFO] Wrote building -> IDF map to {idf_map_csv}")

    else:
        logger.info("[INFO] Skipping IDF creation.")

    # -------------------------------------------------------------------------
    # 10) Perform structuring if requested
    # -------------------------------------------------------------------------
    check_canceled()
    if structuring_cfg.get("perform_structuring", False):
        with step_timer(logger, "structuring"):
            logger.info("[INFO] Performing structuring ...")

            # --- Fenestration -------------------------------------------------
            from idf_objects.structuring.fenestration_structuring import transform_fenez_log_to_structured_with_ranges
            fenez_conf = structuring_cfg.get("fenestration", {})
            fenez_in = fenez_conf.get("csv_in", "assigned/assigned_fenez_params.csv")
            fenez_out = fenez_conf.get("csv_out", "assigned/structured_fenez_params.csv")
            if not os.path.isabs(fenez_in):
                fenez_in = os.path.join(job_output_dir, fenez_in)
            if not os.path.isabs(fenez_out):
                fenez_out = os.path.join(job_output_dir, fenez_out)
            if os.path.isfile(fenez_in):
                transform_fenez_log_to_structured_with_ranges(csv_input=fenez_in, csv_output=fenez_out)
            else:
                logger.warning(f"[STRUCTURING] Fenestration input CSV not found => {fenez_in}")

            # --- DHW ---------------------------------------------------------
            from idf_objects.structuring.dhw_structuring import transform_dhw_log_to_structured
            dhw_conf = structuring_cfg.get("dhw", {})
            dhw_in = dhw_conf.get("csv_in", "assigned/assigned_dhw_params.csv")
            dhw_out = dhw_conf.get("csv_out", "assigned/structured_dhw_params.csv")
            if not os.path.isabs(dhw_in):
                dhw_in = os.path.join(job_output_dir, dhw_in)
            if not os.path.isabs(dhw_out):
                dhw_out = os.path.join(job_output_dir, dhw_out)
            if os.path.isfile(dhw_in):
                transform_dhw_log_to_structured(dhw_in, dhw_out)
            else:
                logger.warning(f"[STRUCTURING] DHW input CSV not found => {dhw_in}")


            # NEW:--- Shading ----------------------------------------------------
            from idf_objects.structuring.shading_structuring import transform_shading_log_to_structured
            shading_conf = structuring_cfg.get("shading", {})
            user_shading_rules = safe_load_subjson("shading.json", "shading") or []
            
            if shading_conf:
                shading_in = patch_if_relative(shading_conf.get("csv_in"))
                shading_out = patch_if_relative(shading_conf.get("csv_out"))

                transform_shading_log_to_structured(
                    csv_input=shading_in,
                    csv_output=shading_out,
                    user_shading_rules=user_shading_rules
                )
            else:
                logger.warning("[STRUCTURING] 'shading' configuration not found in structuring settings.")


            # --- Equipment --------------------------------------------------
            from idf_objects.structuring.equipment_structuring import transform_equipment_log_to_structured
            equip_conf = structuring_cfg.get("equipment", {})
            user_equip_rules = safe_load_subjson("equipment.json", "equipment") or []
            
            if equip_conf:
                equip_in = patch_if_relative(equip_conf.get("csv_in"))
                equip_out = patch_if_relative(equip_conf.get("csv_out"))

                transform_equipment_log_to_structured(
                    csv_input=equip_in,
                    csv_output=equip_out,
                    user_equipment_rules=user_equip_rules
                )
            else:
                logger.warning("[STRUCTURING] 'equipment' configuration not found in structuring settings.")



            # --- Zone Sizing ------------------------------------------------
            from idf_objects.structuring.zone_sizing_structuring import transform_zone_sizing_log_to_structured
            sizing_conf = structuring_cfg.get("zone_sizing", {})
            user_sizing_rules = safe_load_subjson("zone_sizing.json", "zone_sizing") or []
            
            if sizing_conf:
                sizing_in = patch_if_relative(sizing_conf.get("csv_in"))
                sizing_out = patch_if_relative(sizing_conf.get("csv_out"))

                transform_zone_sizing_log_to_structured(
                    csv_input=sizing_in,
                    csv_output=sizing_out,
                    user_sizing_rules=user_sizing_rules
                )
            else:
                logger.warning("[STRUCTURING] 'zone_sizing' configuration not found.")



            # --- HVAC flatten -----------------------------------------------
            from idf_objects.structuring.flatten_hvac import flatten_hvac_data, parse_assigned_value as parse_hvac
            hvac_conf = structuring_cfg.get("hvac", {})
            hvac_in = hvac_conf.get("csv_in", "assigned/assigned_hvac_params.csv")
            hvac_bld = hvac_conf.get("build_out", "assigned/assigned_hvac_building.csv")
            hvac_zone = hvac_conf.get("zone_out", "assigned/assigned_hvac_zones.csv")
            if not os.path.isabs(hvac_in):
                hvac_in = os.path.join(job_output_dir, hvac_in)
            if not os.path.isabs(hvac_bld):
                hvac_bld = os.path.join(job_output_dir, hvac_bld)
            if not os.path.isabs(hvac_zone):
                hvac_zone = os.path.join(job_output_dir, hvac_zone)
            if os.path.isfile(hvac_in):
                df_hvac = pd.read_csv(hvac_in)
                if "assigned_value" in df_hvac.columns:
                    df_hvac["assigned_value"] = df_hvac["assigned_value"].apply(parse_hvac)
                    flatten_hvac_data(
                        df_input=df_hvac,
                        out_build_csv=hvac_bld,
                        out_zone_csv=hvac_zone,
                    )
                else:
                    logger.warning(
                        f"[STRUCTURING] 'assigned_value' column missing in {hvac_in}. Skipping HVAC flatten."
                    )
            else:
                logger.warning(f"[STRUCTURING] HVAC input CSV not found => {hvac_in}")

            # --- Vent flatten -----------------------------------------------
            from idf_objects.structuring.flatten_assigned_vent import flatten_ventilation_data, parse_assigned_value as parse_vent
            vent_conf = structuring_cfg.get("vent", {})
            vent_in = vent_conf.get("csv_in", "assigned/assigned_ventilation.csv")
            vent_bld = vent_conf.get("build_out", "assigned/assigned_vent_building.csv")
            vent_zone = vent_conf.get("zone_out", "assigned/assigned_vent_zones.csv")
            if not os.path.isabs(vent_in):
                vent_in = os.path.join(job_output_dir, vent_in)
            if not os.path.isabs(vent_bld):
                vent_bld = os.path.join(job_output_dir, vent_bld)
            if not os.path.isabs(vent_zone):
                vent_zone = os.path.join(job_output_dir, vent_zone)
            if os.path.isfile(vent_in):
                df_vent = pd.read_csv(vent_in)
                if "assigned_value" in df_vent.columns:
                    df_vent["assigned_value"] = df_vent["assigned_value"].apply(parse_vent)
                    flatten_ventilation_data(
                        df_input=df_vent,
                        out_build_csv=vent_bld,
                        out_zone_csv=vent_zone,
                    )
                else:
                    logger.warning(
                        f"[STRUCTURING] 'assigned_value' column missing in {vent_in}. Skipping ventilation flatten."
                    )
            else:
                logger.warning(f"[STRUCTURING] Vent input CSV not found => {vent_in}")
    else:
        logger.info("[INFO] Skipping structuring.")

    # -------------------------------------------------------------------------
    # 11) Scenario Modification
    # -------------------------------------------------------------------------
    check_canceled()
    if modification_cfg.get("perform_modification", False):
        with step_timer(logger, "modification"):
            logger.info("[INFO] Scenario modification is ENABLED.")

            mod_cfg = modification_cfg["modify_config"]

            # 1) Ensure scenario IDFs go to <job_output_dir>/scenario_idfs
            scenario_idf_dir = os.path.join(job_output_dir, "scenario_idfs")
            os.makedirs(scenario_idf_dir, exist_ok=True)
            mod_cfg["output_idf_dir"] = scenario_idf_dir

            # 2) Ensure scenario sims => <job_output_dir>/Sim_Results/Scenarios
            if "simulation_config" in mod_cfg:
                sim_out = os.path.join(job_output_dir, "Sim_Results", "Scenarios")
                os.makedirs(sim_out, exist_ok=True)
                mod_cfg["simulation_config"]["output_dir"] = sim_out

            # 3) Post-process => <job_output_dir>/results_scenarioes
            if "post_process_config" in mod_cfg:
                ppcfg = mod_cfg["post_process_config"]
                as_is_csv = os.path.join(job_output_dir, "results_scenarioes", "merged_as_is_scenarios.csv")
                daily_csv = os.path.join(job_output_dir, "results_scenarioes", "merged_daily_mean_scenarios.csv")
                os.makedirs(os.path.dirname(as_is_csv), exist_ok=True)
                os.makedirs(os.path.dirname(daily_csv), exist_ok=True)
                ppcfg["output_csv_as_is"] = as_is_csv
                ppcfg["output_csv_daily_mean"] = daily_csv

            # 4) Fix assigned_csv paths
            assigned_csv_dict = mod_cfg.get("assigned_csv", {})
            for key, rel_path in assigned_csv_dict.items():
                assigned_csv_dict[key] = os.path.join(job_output_dir, rel_path)

            # 5) Fix scenario_csv paths
            scenario_csv_dict = mod_cfg.get("scenario_csv", {})
            for key, rel_path in scenario_csv_dict.items():
                scenario_csv_dict[key] = os.path.join(job_output_dir, rel_path)

            # ----------------------------------------------------------------------
            # NEW LOGIC: pick the base_idf_path from building_id automatically
            # ----------------------------------------------------------------------
            # The user sets "building_id" in the config, e.g. 20233330
            building_id = mod_cfg["building_id"]

            # We need the CSV that was saved right after create_idfs_for_all_buildings(...)
            idf_map_csv = os.path.join(job_output_dir, "extracted_idf_buildings.csv")
            if not os.path.isfile(idf_map_csv):
                raise FileNotFoundError(
                    f"Cannot find building->IDF map CSV at {idf_map_csv}. "
                    f"Did you skip 'perform_idf_creation'?"
                )

        # Read the mapping: each row has "ogc_fid" and "idf_name"
            df_idf_map = pd.read_csv(idf_map_csv)
            row_match = df_idf_map.loc[df_idf_map["ogc_fid"] == building_id]

            if row_match.empty:
                raise ValueError(
                    f"No building found for building_id={building_id} in {idf_map_csv}"
                )

        # e.g. "building_0.idf", "building_16.idf", "building_16_ba62d0.idf", etc.
            idf_filename = row_match.iloc[0]["idf_name"]

        # Build the full path to that IDF in output_IDFs
            base_idf_path = os.path.join(job_output_dir, "output_IDFs", idf_filename)
            mod_cfg["base_idf_path"] = base_idf_path
            logger.info(f"[INFO] Auto-selected base IDF => {base_idf_path}")
        # ----------------------------------------------------------------------

            # Finally, run the scenario workflow
            run_modification_workflow(mod_cfg)
    else:
        logger.info("[INFO] Skipping scenario modification.")


    # -------------------------------------------------------------------------
    # 12) Helper to handle patching CSVs that are "relative" but not "data/".
    # -------------------------------------------------------------------------
    def patch_if_relative(csv_path: str):
        """
        1) If absolute, return as-is.
        2) If starts with 'data/', interpret as /usr/src/app/data/... (no job folder).
        3) Else, join with job_output_dir.
        """
        if not csv_path:
            return csv_path
        if os.path.isabs(csv_path):
            return csv_path
        if csv_path.startswith("data/"):
            return os.path.join("/usr/src/app", csv_path)
        return os.path.join(job_output_dir, csv_path)

    # -------------------------------------------------------------------------
    # 13) Global Validation
    # -------------------------------------------------------------------------
        # (A) Validation - BASE
    # -------------------------------------------------------------------------
    check_canceled()
    base_validation_cfg = main_config.get("validation_base", {})
    if base_validation_cfg.get("perform_validation", False):
        with step_timer(logger, "base validation"):
            logger.info("[INFO] BASE Validation is ENABLED.")
            val_conf = base_validation_cfg["config"]

            # Patch relative paths
            sim_csv = val_conf.get("sim_data_csv")
            if sim_csv:
                val_conf["sim_data_csv"] = patch_if_relative(sim_csv)

            real_csv = val_conf.get("real_data_csv")
            if real_csv:
                val_conf["real_data_csv"] = patch_if_relative(real_csv)

            out_csv = val_conf.get("output_csv")
            if out_csv:
                val_conf["output_csv"] = patch_if_relative(out_csv)

            # Now run the validation
            run_validation_process(val_conf)
    else:
        logger.info("[INFO] Skipping BASE validation or not requested.")

    # (B) Validation - SCENARIOS
    # -------------------------------------------------------------------------
    check_canceled()
    scenario_validation_cfg = main_config.get("validation_scenarios", {})
    if scenario_validation_cfg.get("perform_validation", False):
        with step_timer(logger, "scenario validation"):
            logger.info("[INFO] SCENARIO Validation is ENABLED.")
            val_conf = scenario_validation_cfg["config"]

            # Patch relative paths
            sim_csv = val_conf.get("sim_data_csv")
            if sim_csv:
                val_conf["sim_data_csv"] = patch_if_relative(sim_csv)

            real_csv = val_conf.get("real_data_csv")
            if real_csv:
                val_conf["real_data_csv"] = patch_if_relative(real_csv)

            out_csv = val_conf.get("output_csv")
            if out_csv:
                val_conf["output_csv"] = patch_if_relative(out_csv)

            # Now run the validation
            run_validation_process(val_conf)
    else:
        logger.info("[INFO] Skipping SCENARIO validation or not requested.")


    # -------------------------------------------------------------------------
    # 14) Sensitivity Analysis
    # -------------------------------------------------------------------------
    check_canceled()
    if sens_cfg.get("perform_sensitivity", False):
        with step_timer(logger, "sensitivity analysis"):
            logger.info("[INFO] Sensitivity Analysis is ENABLED.")

            scenario_folder = sens_cfg.get("scenario_folder", "")
            sens_cfg["scenario_folder"] = patch_if_relative(scenario_folder)

            results_csv = sens_cfg.get("results_csv", "")
            sens_cfg["results_csv"] = patch_if_relative(results_csv)

            out_csv = sens_cfg.get("output_csv", "sensitivity_output.csv")
            sens_cfg["output_csv"] = patch_if_relative(out_csv)

            run_sensitivity_analysis(
                scenario_folder=sens_cfg["scenario_folder"],
                method=sens_cfg["method"],
                results_csv=sens_cfg.get("results_csv", ""),
                target_variable=sens_cfg.get("target_variable", []),
                output_csv=sens_cfg.get("output_csv", "sensitivity_output.csv"),
                n_morris_trajectories=sens_cfg.get("n_morris_trajectories", 10),
                num_levels=sens_cfg.get("num_levels", 4),
                n_sobol_samples=sens_cfg.get("n_sobol_samples", 128)
            )
    else:
        logger.info("[INFO] Skipping sensitivity analysis.")

    # -------------------------------------------------------------------------
    # 15) Surrogate Modeling
    # -------------------------------------------------------------------------
    check_canceled()
    if sur_cfg.get("perform_surrogate", False):
        with step_timer(logger, "surrogate modeling"):
            logger.info("[INFO] Surrogate Modeling is ENABLED.")

            scenario_folder = sur_cfg.get("scenario_folder", "")
            sur_cfg["scenario_folder"] = patch_if_relative(scenario_folder)

            results_csv = sur_cfg.get("results_csv", "")
            sur_cfg["results_csv"] = patch_if_relative(results_csv)

            model_out = sur_cfg.get("model_out", "")
            sur_cfg["model_out"] = patch_if_relative(model_out)

            cols_out = sur_cfg.get("cols_out", "")
            sur_cfg["cols_out"] = patch_if_relative(cols_out)

            target_var = sur_cfg["target_variable"]
            test_size  = sur_cfg["test_size"]

            df_scen = sur_load_scenario_params(sur_cfg["scenario_folder"])
            pivot_df = pivot_scenario_params(df_scen)

            df_sim = load_sim_results(sur_cfg["results_csv"])
            df_agg = aggregate_results(df_sim)
            merged_df = merge_params_with_results(pivot_df, df_agg, target_var)

            rf_model, trained_cols = build_and_save_surrogate(
                df_data=merged_df,
                target_col=target_var,
                model_out_path=sur_cfg["model_out"],
                columns_out_path=sur_cfg["cols_out"],
                test_size=test_size,
                random_state=42
            )
            if rf_model:
                logger.info("[INFO] Surrogate model built & saved.")
            else:
                logger.warning("[WARN] Surrogate modeling failed or insufficient data.")
    else:
        logger.info("[INFO] Skipping surrogate modeling.")

    # -------------------------------------------------------------------------
    # 16) Calibration
    # -------------------------------------------------------------------------
    check_canceled()
    if cal_cfg.get("perform_calibration", False):
        with step_timer(logger, "calibration"):
            logger.info("[INFO] Calibration is ENABLED.")

            scen_folder = cal_cfg.get("scenario_folder", "")
            cal_cfg["scenario_folder"] = patch_if_relative(scen_folder)

            real_csv = cal_cfg.get("real_data_csv", "")
            cal_cfg["real_data_csv"] = patch_if_relative(real_csv)

            sur_model_path = cal_cfg.get("surrogate_model_path", "")
            cal_cfg["surrogate_model_path"] = patch_if_relative(sur_model_path)

            sur_cols_path = cal_cfg.get("surrogate_columns_path", "")
            cal_cfg["surrogate_columns_path"] = patch_if_relative(sur_cols_path)

            hist_csv = cal_cfg.get("output_history_csv", "")
            cal_cfg["output_history_csv"] = patch_if_relative(hist_csv)

            best_params_folder = cal_cfg.get("best_params_folder", "")
            cal_cfg["best_params_folder"] = patch_if_relative(best_params_folder)

            run_unified_calibration(cal_cfg)
    else:
        logger.info("[INFO] Skipping calibration.")

    # -------------------------------------------------------------------------
    # 17) Zip & Email final results, if mail_user.json present
    # -------------------------------------------------------------------------
    try:
        with step_timer(logger, "zipping and email"):
            mail_user_path = os.path.join(user_configs_folder, "mail_user.json")
            mail_info = {}
            if os.path.isfile(mail_user_path):
                with open(mail_user_path, "r") as f:
                    mail_info = json.load(f)

                mail_user_list = mail_info.get("mail_user", [])
                if len(mail_user_list) > 0:
                    first_user = mail_user_list[0]
                    recipient_email = first_user.get("email", "")
                    if recipient_email:
                        zip_path = zip_user_output(job_output_dir)
                        send_results_email(zip_path, recipient_email)
                        logger.info(f"[INFO] Emailed zip {zip_path} to {recipient_email}")
                    else:
                        logger.warning("[WARN] mail_user.json => missing 'email'")
                else:
                    logger.warning("[WARN] mail_user.json => 'mail_user' list is empty.")
            else:
                logger.info("[INFO] No mail_user.json found, skipping email.")
    except Exception as e:
        logger.error(f"[ERROR] Zipping/Emailing results failed => {e}")

    # -------------------------------------------------------------------------
    # LAST STEP: (Optional) Call the cleanup function
    # -------------------------------------------------------------------------
    try:
        cleanup_old_results()  # This will remove any job folder older than MAX_AGE_HOURS
    except Exception as e:
        logger.error(f"[CLEANUP ERROR] => {e}")

    total_time = time.perf_counter() - overall_start
    logger.info(f"=== End of orchestrate_workflow (took {total_time:.2f} seconds) ===")

------------------------------------------------------------

File: D:\Documents\E_Plus_2030_py\combined.json
============================================================
{
  "dhw": [
    {
      "building_id": 413673000,
      "param_name": "occupant_density_m2_per_person",
      "min_val": 127.0,
      "max_val": 233.0
    },
    {
      "building_id": 413673000,
      "param_name": "liters_per_person_per_day",
      "fixed_value": 145.0
    },
    {
      "building_function": "residential00",
      "age_range": "1992-2005",
      "param_name": "setpoint_c",
      "min_val": 58.0,
      "max_val": 60.0
    },
    {
      "building_function": "non_residential000",
      "param_name": "occupant_density_m2_per_person",
      "min_val": 12.0,
      "max_val": 18.0
    }
  ],
  "epw": [
    {
      "building_id": 413673000,
      "fixed_epw_path": "data/weather/2050.epw"
    },
    {
      "desired_year": 2050,
      "override_year_to": 2020
    }
  ],
  "zone_sizing": [
    {
      "param_name": "cooling_supply_air_temp",
      "min_val": 12.0,
      "max_val": 15.0
    },
    {
      "param_name": "heating_supply_air_temp",
      "min_val": 40.0,
      "max_val": 50.0
    },
    {
      "param_name": "cooling_supply_air_hr",
      "min_val": 0.008,
      "max_val": 0.010
    },
    {
      "param_name": "heating_supply_air_hr",
      "min_val": 0.003,
      "max_val": 0.005
    },
    {
      "param_name": "cooling_design_air_flow_method",
      "choices": [
        "Flow/Zone",
        "DesignDay",
        "DesignDayWithLimit"
      ]
    }
  ],
  "equipment": [
    {
      "param_name": "equip_wm2",
      "min_val": 3.0,
      "max_val": 7.0
    },
    {
      "param_name": "tD",
      "description": "Schedule value for day occupancy (W)",
      "min_val": 400.0,
      "max_val": 800.0
    },
    {
      "param_name": "tN",
      "description": "Schedule value for night occupancy (W)",
      "min_val": 150.0,
      "max_val": 350.0
    }
  ],
  "shading": [
    {
      "param_name": "slat_angle_deg",
      "min_val": 30.0,
      "max_val": 60.0
    },
    {
      "param_name": "slat_beam_solar_reflectance",
      "min_val": 0.6,
      "max_val": 0.8
    },
    {
      "param_name": "blind_to_glass_distance",
      "fixed_value": 0.075
    },
    {
      "param_name": "slat_width",
      "min_val": 0.020,
      "max_val": 0.030
    }
  ],
  "mail_user": [
    {
      "email": "amin.jalilzade1@gmail.com"
    }
  ],
  "fenestration": [
    {
      "building_id": 413673000,
      "building_function": "residential",
      "building_type": "Two-and-a-half-story House",
      "age_range": "1992-2005",
      "scenario": "scenario1",
      "param_name": "wwr",
      "min_val": 0.025,
      "max_val": 0.03
    },
    {
      "building_function": "non_residential0",
      "age_range": "2015 and later",
      "scenario": "scenario1",
      "param_name": "roof_R_value",
      "fixed_value": 3.0
    }
  ],
  "geometry": [
    {
      "building_id": 413673,
      "param_name": "perimeter_depth",
      "min_val": 3.5,
      "max_val": 4.0,
      "fixed_value": true
    },
    {
      "building_id": 4136733,
      "param_name": "has_core",
      "fixed_value": true
    },
    {
      "building_id": 4136737,
      "param_name": "has_core",
      "fixed_value": false
    },
    {
      "building_type": "Meeting Function",
      "param_name": "has_core",
      "fixed_value": true
    },
    {
      "building_id": 4136738,
      "param_name": "has_core",
      "fixed_value": false
    }
  ],
  "hvac": [
    {
      "building_id": 413673000,
      "param_name": "heating_day_setpoint",
      "min_val": 10.0,
      "max_val": 11.0
    },
    {
      "building_function": "residential0",
      "param_name": "cooling_day_setpoint",
      "fixed_value": 25.6
    },
    {
      "scenario": "scenario10",
      "age_range": "2015 and later",
      "param_name": "max_heating_supply_air_temp",
      "min_val": 48.6,
      "max_val": 50.0
    }
  ],
  "lighting": [
    {
      "building_id": 413673000,
      "param_name": "lights_wm2",
      "min_val": 18.0,
      "max_val": 20.0
    },
    {
      "building_type": "Residential0",
      "param_name": "tN",
      "min_val": 10999,
      "max_val": 11999
    },
    {
      "building_id": 4136731,
      "building_type": "non_residential",
      "param_name": "parasitic_wm2",
      "min_val": 0.28,
      "max_val": 0.3
    }
  ],
  "main_config": {
    "paths": {
      "building_data": "data/df_buildings.csv",
      "fenez_excel": "excel_data/envelop.xlsx",
      "dhw_excel": "excel_data/dhw_overrides.xlsx",
      "epw_excel": "excel_data/epw_overrides.xlsx",
      "lighting_excel": "excel_data/lighting_overrides.xlsx",
      "hvac_excel": "excel_data/hvac_overrides.xlsx",
      "vent_excel": "excel_data/vent_overrides.xlsx"
    },
    "use_database": false,
    "filter_by": "meestvoorkomendepostcode",
    "db_filter": {
      "meestvoorkomendepostcode": [
        "3066HG"
      ],
      "pand_id": [
        "0383100000001369",
        "0383100000001370",
        "5473VX"
      ],
      "pand_ids": [
        "XYZ123",
        "XYZ999"
      ],
      "bbox_xy": [
        120000.0,
        487000.0,
        121000.0,
        488000.0
      ],
      "bbox_latlon": [
        51.873,
        5.589,
        51.878,
        5.6
      ]
    },
    "excel_overrides": {
      "override_fenez_excel": false,
      "override_dhw_excel": false,
      "override_epw_excel": false,
      "override_lighting_excel": false,
      "override_hvac_excel": false,
      "override_vent_excel": false
    },
    "user_config_overrides": {
      "override_fenez_json": true,
      "override_dhw_json": true,
      "override_epw_json": true,
      "override_lighting_json": true,
      "override_hvac_json": true,
      "override_vent_json": true,
      "override_geometry_json": true,
      "override_shading_json": false
    },
    "idf_creation": {
      "perform_idf_creation": true,
      "scenario": "scenario1",
      "calibration_stage": "pre_calibration",
      "strategy": "B",
      "random_seed": 42,
      "iddfile": "EnergyPlus/Energy+.idd",
      "idf_file_path": "EnergyPlus/Minimal.idf",
      "output_idf_dir": "output_IDFs",
      "run_simulations": true,
      "simulate_config": {
        "num_workers": 4,
        "ep_force_overwrite": true
      },
      "post_process": true,
      "post_process_config": {
        "base_output_dir": "Sim_Results",
        "outputs": [
          {
            "convert_to_daily": true,
            "convert_to_monthly": false,
            "aggregator": "none",
            "output_csv": "output/results/merged_as_is.csv"
          },
          {
            "convert_to_daily": true,
            "convert_to_monthly": false,
            "aggregator": "mean",
            "output_csv": "output/results/merged_daily_mean.csv"
          },
          {
            "convert_to_daily": true,
            "convert_to_monthly": true,
            "aggregator": "mean",
            "output_csv": "output/results/merged_monthly_mean.csv"
          }
        ]
      },
      "output_definitions": {
        "desired_variables": [
          "Facility Total Electric Demand Power",
          "Zone Air Temperature",
          "Boiler Heating Energy",
          "Water Heater Heating Energy"
        ],
        "desired_meters": [
          "Electricity:Facility",
          "Heating:EnergyTransfer",
          "Cooling:EnergyTransfer"
        ],
        "override_variable_frequency": "Daily",
        "override_meter_frequency": "Daily",
        "include_tables": true,
        "include_summary": true
      }
    },
    "structuring": {
      "perform_structuring": true,
      "dhw": {
        "csv_in": "assigned/assigned_dhw_params.csv",
        "csv_out": "assigned/structured_dhw_params.csv"
      },
      "fenestration": {
        "csv_in": "assigned/assigned_fenez_params.csv",
        "csv_out": "assigned/structured_fenez_params.csv"
      },
      "hvac": {
        "csv_in": "assigned/assigned_hvac_params.csv",
        "build_out": "assigned/assigned_hvac_building.csv",
        "zone_out": "assigned/assigned_hvac_zones.csv"
      },
      "vent": {
        "csv_in": "assigned/assigned_ventilation.csv",
        "build_out": "assigned/assigned_vent_building.csv",
        "zone_out": "assigned/assigned_vent_zones.csv"
      },
      "shading": {
        "csv_in": "assigned/assigned_shading_params.csv",
        "csv_out": "assigned/structured_shading_params.csv"
      },
      "equipment": {
        "csv_in": "assigned/assigned_equipment.csv",
        "csv_out": "assigned/structured_equipment.csv"
      },
      "zone_sizing": {
        "csv_in": "assigned/assigned_zone_sizing_outdoor_air.csv",
        "csv_out": "assigned/structured_zone_sizing.csv"
      }
    },
    "modification": {
      "perform_modification": true,
      "modify_config": {
        "idd_path": "EnergyPlus/Energy+.idd",
        "assigned_csv": {
          "hvac_building": "assigned/assigned_hvac_building.csv",
          "hvac_zones": "assigned/assigned_hvac_zones.csv",
          "dhw": "assigned/assigned_dhw_params.csv",
          "vent_build": "assigned/assigned_vent_building.csv",
          "vent_zones": "assigned/assigned_vent_zones.csv",
          "elec": "assigned/assigned_lighting.csv",
          "fenez": "assigned/structured_fenez_params.csv",
          "shading": "assigned/structured_shading_params.csv",
          "equip": "assigned/structured_equipment.csv",
          "zone_sizing": "assigned/structured_zone_sizing.csv"
        },
        "scenario_csv": {
          "hvac": "scenarios/scenario_params_hvac.csv",
          "dhw": "scenarios/scenario_params_dhw.csv",
          "vent": "scenarios/scenario_params_vent.csv",
          "elec": "scenarios/scenario_params_elec.csv",
          "fenez": "scenarios/scenario_params_fenez.csv",
          "shading": "scenarios/scenario_params_shading.csv",
          "equip": "scenarios/scenario_params_equipment.csv",
          "zone_sizing": "scenarios/scenario_params_zone_sizing.csv"
        },
        "output_idf_dir": "scenario_idfs",
        "building_id": 4136733,
        "num_scenarios": 5,
        "picking_method": "random_uniform",
        "picking_scale_factor": 0.5,
        "run_simulations": true,
        "simulation_config": {
          "num_workers": 4,
          "output_dir": "Sim_Results/Scenarios"
        },
        "perform_post_process": true,
        "post_process_config": {
          "output_csv_as_is": "results_scenarioes/merged_as_is_scenarios.csv",
          "output_csv_daily_mean": "results_scenarioes/merged_daily_mean_scenarios.csv"
        },
        "perform_validation": false,
        "validation_config": {
          "real_data_csv": "data/mock_merged_daily_mean.csv",
          "sim_data_csv": "results/merged_daily_mean.csv",
          "bldg_ranges": {
            "0": [
              0,
              1,
              2
            ]
          },
          "variables_to_compare": [
            "Electricity:Facility [J](Hourly)",
            "Heating:EnergyTransfer [J](Hourly)",
            "Cooling:EnergyTransfer [J](Hourly)"
          ],
          "threshold_cv_rmse": 30.0,
          "skip_plots": true,
          "output_csv": "scenario_validation_report.csv"
        }
      }
    },
    "validation_base": {
      "perform_validation": true,
      "config": {
        "real_data_csv": "data/mock_merged_daily_mean.csv",
        "sim_data_csv": "results/merged_daily_mean.csv",
        "bldg_ranges": {"0": [0, 1, 2, 3]       },
        "variables_to_compare": [
          "Electricity:Facility [J](Hourly)", "Heating:EnergyTransfer [J](Hourly)", "Cooling:EnergyTransfer [J](Hourly)"],
        "threshold_cv_rmse": 30.0,
        "skip_plots": true,
        "output_csv": "validation_report_base.csv",

        "analysis_options": {
          "date_format": "%d/%m/%Y %H:%M",
          "granularity": ["annual", "monthly", "seasonal"],
          "weekday_weekend": true,
          "diurnal_profiles": true,
          "peak_analysis": {
            "perform": true,
            "n_peaks": 10
          },
          "ramp_rate_analysis": true,
          "extreme_day_analysis": {
            "perform": true,
            "n_days": 5,
            "weather_variable": "Environment:Site Outdoor Air Drybulb Temperature [C](Daily)"
      }}}
    },
    "validation_scenarios": {
      "perform_validation": false,
      "config": {
        "real_data_csv": "data/mock_merged_daily_mean.csv",
        "sim_data_csv": "results_scenarioes/merged_daily_mean_scenarios.csv",
        "bldg_ranges": {
          "0": [
            0,
            1,
            2
          ]
        },
        "variables_to_compare": [
          "Electricity:Facility [J](Hourly)",
          "Heating:EnergyTransfer [J](Hourly)",
          "Cooling:EnergyTransfer [J](Hourly)"
        ],
        "threshold_cv_rmse": 30.0,
        "skip_plots": true,
        "output_csv": "validation_report_scenarios.csv"
      }
    },
    "sensitivity": {
      "perform_sensitivity": true,
      "scenario_folder": "scenarios",
      "method": "morris",
      "results_csv": "results_scenarioes/merged_daily_mean_scenarios.csv",
      "target_variable": [
        "Heating:EnergyTransfer [J](Hourly)",
        "Cooling:EnergyTransfer [J](Hourly)",
        "Electricity:Facility [J](Hourly)"
      ],
      "output_csv": "multi_corr_sensitivity.csv",
      "n_morris_trajectories": 10,
      "num_levels": 4
    },
    "surrogate": {
      "perform_surrogate": true,
      "scenario_folder": "scenarios",
      "results_csv": "results_scenarioes/merged_daily_mean_scenarios.csv",
      "target_variable": "Heating:EnergyTransfer [J](Hourly)",
      "model_out": "heating_surrogate_model.joblib",
      "cols_out": "heating_surrogate_columns.joblib",
      "test_size": 0.3
    },
    "calibration": {
      "perform_calibration": true,
      "scenario_folder": "scenarios",
      "scenario_files": [
        "scenario_params_dhw.csv",
        "scenario_params_elec.csv"
      ],
      "subset_sensitivity_csv": "multi_corr_sensitivity.csv",
      "top_n_params": 10,
      "method": "ga",
      "use_surrogate": true,
      "real_data_csv": "data/mock_merged_daily_mean.csv",
      "surrogate_model_path": "heating_surrogate_model.joblib",
      "surrogate_columns_path": "heating_surrogate_columns.joblib",
      "calibrate_min_max": true,
      "ga_pop_size": 10,
      "ga_generations": 5,
      "ga_crossover_prob": 0.7,
      "ga_mutation_prob": 0.2,
      "bayes_n_calls": 15,
      "random_n_iter": 20,
      "output_history_csv": "calibration_history.csv",
      "best_params_folder": "calibrated",
      "history_folder": "calibrated"
    }
  },
  "shading_tree": [
    {
      "building_id": 413673000,
      "param_name": "top_n_buildings",
      "fixed_value": 5
    },
    {
      "building_function": "residential",
      "param_name": "summer_value",
      "min_val": 0.4,
      "max_val": 0.6
    },
    {
      "param_name": "top_n_trees",
      "fixed_value": 3
    }
  ],
  "vent": [
    {
      "building_id": 413673000,
      "param_name": "infiltration_base",
      "min_val": 100.3,
      "max_val": 100.4
    },
    {
      "building_function": "residential",
      "age_range": "1992-2005",
      "param_name": "year_factor",
      "min_val": 21.4,
      "max_val": 21.5
    },
    {
      "building_id": 4136731,
      "param_name": "system_type",
      "fixed_value": "D"
    },
    {
      "building_id": 4136731,
      "param_name": "fan_pressure",
      "min_val": 100,
      "max_val": 120
    }
  ]
}
------------------------------------------------------------

