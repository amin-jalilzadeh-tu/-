============================================================
File: D:\Documents\E_Plus_2030_py\job_manager.py
============================================================
"""
job_manager.py

A more comprehensive in-memory job manager to handle multiple
workflow runs (jobs). Each job can be in states like CREATED,
QUEUED, RUNNING, FINISHED, ERROR, or CANCELED.

Features:
  - Unique job_id generation
  - Concurrency limit (max simultaneous running jobs)
  - Job queue (if concurrency limit is reached)
  - In-memory log queue for real-time streaming
  - Support for cancellation
  - Placeholders for persistence (if needed in future)

CAVEATS:
  - If the Python process exits, in-memory jobs are lost.
  - For production usage, consider a persistent store (Redis, DB).
"""

import uuid
import queue
import threading
import time
import enum
from typing import Any, Dict, Optional, Union

from orchestrator import WorkflowCanceled


###############################################################################
# Global Job Store and Concurrency
###############################################################################

# You can store up to X jobs as RUNNING at once. Others will wait in a queue.
MAX_RUNNING_JOBS = 2

# The global store for jobs and a global queue for waiting jobs.
_jobs: Dict[str, Dict] = {}
_waiting_jobs: queue.Queue = queue.Queue()  # For jobs awaiting execution if concurrency is at max.

# A global lock to protect shared state, e.g., reading/writing _jobs, counting running jobs, etc.
_jobs_lock = threading.Lock()

###############################################################################
# Enum: JobStatus
###############################################################################
class JobStatus(str, enum.Enum):
    CREATED = "CREATED"
    QUEUED = "QUEUED"
    RUNNING = "RUNNING"
    FINISHED = "FINISHED"
    ERROR = "ERROR"
    CANCELED = "CANCELED"


###############################################################################
# Data Structure for Each Job
###############################################################################
# Each job dictionary in _jobs has keys:
#   {
#       "status": JobStatus,
#       "config": dict,             # includes "job_id" and other user config
#       "logs": queue.Queue,        # For log streaming
#       "thread": threading.Thread or None,
#       "result": Any,              # Store final result or summary
#       "cancel_event": threading.Event # Signals that we want to cancel
#   }


###############################################################################
# Main API
###############################################################################

def create_job(config: dict) -> str:
    """
    Create a new job, store it in _jobs, return its job_id.
    
    :param config: Dictionary of user-supplied config for the job
    :return: job_id (string)
    """
    job_id = str(uuid.uuid4())
    with _jobs_lock:
        # We also store job_id in the config so orchestrator can see it.
        config["job_id"] = job_id

        _jobs[job_id] = {
            "status": JobStatus.CREATED,
            "config": config,
            "logs": queue.Queue(),
            "thread": None,
            "result": None,
            "cancel_event": threading.Event(),
        }
    return job_id


def enqueue_job(job_id: str) -> None:
    """
    Place a job into the waiting queue if concurrency is at max, or run it immediately if not.
    """
    with _jobs_lock:
        job = _jobs.get(job_id)
        if not job:
            return
        # If job is not in CREATED state, do nothing
        if job["status"] != JobStatus.CREATED:
            return

        # Count how many are running
        running_count = sum(1 for j in _jobs.values() if j["status"] == JobStatus.RUNNING)
        if running_count < MAX_RUNNING_JOBS:
            # We can start this job immediately
            _start_job(job_id)
        else:
            # We have to queue it
            job["status"] = JobStatus.QUEUED
            _waiting_jobs.put(job_id)


def get_job(job_id: str) -> Optional[Dict]:
    """Retrieve the entire job dictionary by job_id."""
    with _jobs_lock:
        return _jobs.get(job_id)


def get_job_status(job_id: str) -> Optional[str]:
    """Return the status of the job as a string (or None if not found)."""
    with _jobs_lock:
        job = _jobs.get(job_id)
        if job:
            return job["status"]
    return None


def get_job_logs_queue(job_id: str) -> Optional[queue.Queue]:
    """Return the logs queue for a job, if it exists."""
    with _jobs_lock:
        job = _jobs.get(job_id)
        if job:
            return job["logs"]
    return None


def cancel_job(job_id: str) -> bool:
    """
    Signal that a job should be canceled (if RUNNING or QUEUED).
    This sets the 'cancel_event' so the job's thread can check and stop if possible.
    
    :return: True if job was canceled or is in the process of being canceled, False if not found.
    """
    with _jobs_lock:
        job = _jobs.get(job_id)
        if not job:
            return False

        status = job["status"]
        if status in [JobStatus.FINISHED, JobStatus.ERROR, JobStatus.CANCELED]:
            return False  # It's already done or canceled

        # Signal the thread to stop
        job["cancel_event"].set()

        if status == JobStatus.QUEUED:
            # If queued, remove it from the queue
            # We'll need to rebuild the queue without that job_id
            _remove_queued_job(job_id)
            job["status"] = JobStatus.CANCELED
        elif status == JobStatus.RUNNING:
            # The thread should eventually notice the cancel_event
            pass
        else:
            # If CREATED and not yet enqueued, we'll just mark it canceled
            if status == JobStatus.CREATED:
                job["status"] = JobStatus.CANCELED

        return True


def set_job_result(job_id: str, result_data: Any) -> None:
    """Store final results or summary data for the given job."""
    with _jobs_lock:
        if job_id in _jobs:
            _jobs[job_id]["result"] = result_data


def get_job_result(job_id: str) -> Any:
    """Retrieve whatever was set as the result data."""
    with _jobs_lock:
        job = _jobs.get(job_id)
        if job:
            return job["result"]
    return None


###############################################################################
# Internal Worker Helpers
###############################################################################

def _start_job(job_id: str) -> None:
    """
    Transition job to RUNNING and spawn a thread to execute it.
    """
    job = _jobs[job_id]
    job["status"] = JobStatus.RUNNING

    # We'll create a new thread that calls _job_thread_runner(job_id)
    t = threading.Thread(target=_job_thread_runner, args=(job_id,), daemon=True)
    job["thread"] = t
    t.start()


def _job_thread_runner(job_id: str) -> None:
    """Worker function that runs in a separate thread for each job."""
    try:
        job = _jobs[job_id]
        cancel_event = job["cancel_event"]

        # 1) Import orchestrate_workflow
        from orchestrator import orchestrate_workflow

        # 2) Run the workflow, passing the entire job["config"] which has "job_id"
        orchestrate_workflow(job["config"], cancel_event=cancel_event)

        # If orchestrate_workflow finishes with no exception => FINISHED
        job["status"] = JobStatus.FINISHED

    except WorkflowCanceled:
        job["status"] = JobStatus.CANCELED
    except Exception as e:
        job["status"] = JobStatus.ERROR
        err_msg = f"[Job {job_id}] crashed => {e}"
        job["logs"].put(err_msg)
    finally:
        _signal_end_of_logs(job_id)
        _try_start_next_queued_job()


def _try_start_next_queued_job():
    """
    Check if there's a queued job. If concurrency allows, start it.
    """
    with _jobs_lock:
        running_count = sum(1 for j in _jobs.values() if j["status"] == JobStatus.RUNNING)
        if running_count >= MAX_RUNNING_JOBS:
            return  # No slot available

        # Otherwise, pop from waiting queue if any
        while not _waiting_jobs.empty():
            next_job_id = _waiting_jobs.get()
            job = _jobs.get(next_job_id)
            if not job:
                continue  # might have been canceled or removed

            if job["status"] == JobStatus.QUEUED:
                # Start it
                _start_job(next_job_id)
                break


def _remove_queued_job(job_id: str):
    """
    Remove a job from the waiting queue if it's in there by rebuilding the queue 
    without that job_id.
    """
    temp_list = []
    while not _waiting_jobs.empty():
        j_id = _waiting_jobs.get()
        if j_id != job_id:
            temp_list.append(j_id)
    # re-queue the others
    for j_id in temp_list:
        _waiting_jobs.put(j_id)


def _signal_end_of_logs(job_id: str):
    """
    Put a `None` sentinel in the job's log queue to indicate no more logs.
    The client log streamer can break on encountering None.
    """
    with _jobs_lock:
        job = _jobs.get(job_id)
        if job:
            job["logs"].put(None)


============================================================
File: D:\Documents\E_Plus_2030_py\main_modifi.py
============================================================
"""
main_modifi.py

Handles the generation of scenario-based IDFs for sensitivity, surrogate,
calibration, or any parametric runs, then optionally runs E+ simulation,
post-processing, and validation in a job-specific folder if provided.

Usage:
  - Typically invoked from your orchestrator (or command line) with a config dict:
    {
      "base_idf_path": "output_IDFs/building_0.idf",
      "idd_path": "EnergyPlus/Energy+.idd",
      "assigned_csv": {
        "hvac_building": "output/assigned/assigned_hvac_building.csv",
        "hvac_zones": "output/assigned/assigned_hvac_zones.csv",
        "dhw": "output/assigned/assigned_dhw_params.csv",
        "vent_build": "output/assigned/assigned_vent_building.csv",
        "vent_zones": "output/assigned/assigned_vent_zones.csv",
        "elec": "output/assigned/assigned_lighting.csv",
        "fenez": "output/assigned/structured_fenez_params.csv"
      },
      "scenario_csv": {
        "hvac": "output/scenarios/scenario_params_hvac.csv",
        "dhw": "output/scenarios/scenario_params_dhw.csv",
        "vent": "output/scenarios/scenario_params_vent.csv",
        "elec": "output/scenarios/scenario_params_elec.csv",
        "fenez": "output/scenarios/scenario_params_fenez.csv"
      },
      "output_idf_dir": "output/scenario_idfs",
      "building_id": 4136730,
      "num_scenarios": 5,
      "picking_method": "random_uniform",
      "picking_scale_factor": 0.5,

      "run_simulations": true,
      "simulation_config": {
        "num_workers": 4,
        "output_dir": "output/Sim_Results/Scenarios"
      },
      "perform_post_process": true,
      "post_process_config": {
        "output_csv_as_is": "output/results_scenarioes/merged_as_is_scenarios.csv",
        "output_csv_daily_mean": "output/results_scenarioes/merged_daily_mean_scenarios.csv"
      },
      "perform_validation": true,
      "validation_config": {
        "real_data_csv": "data/mock_merged_daily_mean.csv",
        "sim_data_csv": "output/results_scenarioes/merged_daily_mean_scenarios.csv",
        "bldg_ranges": { "0": [0,1,2] },
        "variables_to_compare": [...],
        "threshold_cv_rmse": 30.0,
        "skip_plots": true,
        "output_csv": "scenario_validation_report.csv"
      },

      "job_output_dir": "/usr/src/app/output/xxxx-uuid"   # (Optional)
    }
"""

import os
import logging
import pandas as pd

# ---------------------------------------------------------------------------
# A) Common Utilities
# ---------------------------------------------------------------------------
from modification.common_utils import (
    load_assigned_csv,
    load_scenario_csv,
    load_idf,
    save_idf,
    generate_multiple_param_sets,
    save_param_scenarios_to_csv
)

# ---------------------------------------------------------------------------
# B) Modules for scenario creation & application
# ---------------------------------------------------------------------------
from modification.hvac_functions import (
    create_hvac_scenarios,
    apply_building_level_hvac,
    apply_zone_level_hvac
)
from modification.dhw_functions import (
    create_dhw_scenarios,
    apply_dhw_params_to_idf
)
from modification.vent_functions import (
    create_vent_scenarios,
    apply_building_level_vent,
    apply_zone_level_vent
)
from modification.elec_functions import (
    create_elec_scenarios,
    apply_building_level_elec,
    apply_object_level_elec
)
from modification.fenez_functions2 import (
    create_fenez_scenarios,
    apply_object_level_fenez
)

# ---------------------------------------------------------------------------
# C) Simulation + Post-processing + Validation
# ---------------------------------------------------------------------------
from epw.run_epw_sims import simulate_all
from postproc.merge_results import merge_all_results
from validation.main_validation import run_validation_process


def run_all_idfs_in_folder(
    folder_path: str,
    iddfile: str,
    base_output_dir: str,
    default_lat: float = 52.15,
    default_lon: float = 4.40,
    default_year: int = 2020,
    num_workers: int = 4
):
    """
    Utility function to find .idf files in folder_path and run them with simulate_all(...).
    Adjust lat/lon/year or load them from a side CSV if needed.
    """
    logger = logging.getLogger(__name__)
    logger.info(f"[run_all_idfs_in_folder] Searching .idf files in {folder_path}")

    if not os.path.isdir(folder_path):
        logger.warning(f"[run_all_idfs_in_folder] Folder not found => {folder_path}")
        return

    idf_files = [f for f in os.listdir(folder_path) if f.lower().endswith(".idf")]
    if not idf_files:
        logger.warning(f"[run_all_idfs_in_folder] No .idf files in {folder_path} to run.")
        return

    data_rows = []
    for idx, idf_name in enumerate(idf_files):
        data_rows.append({
            "idf_name": idf_name,
            "lat": default_lat,
            "lon": default_lon,
            "desired_climate_year": default_year,
            "ogc_fid": idx  # or parse from filename
        })

    df_scenarios = pd.DataFrame(data_rows)
    logger.info(f"[run_all_idfs_in_folder] Running {len(df_scenarios)} scenario IDFs with simulate_all...")

    simulate_all(
        df_buildings=df_scenarios,
        idf_directory=folder_path,
        iddfile=iddfile,
        base_output_dir=base_output_dir,
        user_config_epw=None,
        assigned_epw_log=None,
        num_workers=num_workers
    )
    logger.info("[run_all_idfs_in_folder] Simulations triggered.")


def run_modification_workflow(config):
    """
    Main function for scenario-based IDF creation + optional E+ simulation,
    post-processing, and validation.

    Steps:
      1) Resolve folder paths (scenario IDFs, results) based on config + optional job_output_dir.
      2) Load assigned CSV data (HVAC, DHW, Vent, Elec, Fenez).
      3) Filter for the chosen building.
      4) Generate scenario param picks (random or otherwise).
      5) For each scenario, load a fresh base IDF, apply picks, save scenario IDF.
      6) (Optional) run E+ sims for these scenario IDFs, then post-process, then validate.

    :param config: dict
    :return: None
    """
    logger = logging.getLogger(__name__)
    logger.info("[MODIFICATION] Starting scenario-based workflow...")

    # -----------------------------------------------------------------------
    # 1) Extract config parts & resolve paths
    # -----------------------------------------------------------------------
    base_idf_path   = config["base_idf_path"]
    idd_path        = config["idd_path"]
    assigned_csvs   = config["assigned_csv"]
    scenario_csvs   = config["scenario_csv"]
    building_id     = config["building_id"]
    num_scenarios   = config["num_scenarios"]
    picking_method  = config["picking_method"]
    scale_factor    = config.get("picking_scale_factor", 1.0)

    # The user might specify something like "output/scenario_idfs" or just "scenario_idfs".
    scenario_idf_dir = config.get("output_idf_dir", "output/scenario_idfs")

    # Also we have simulation, post-processing, and validation flags:
    run_sims        = config.get("run_simulations", False)
    sim_cfg         = config.get("simulation_config", {})
    do_postproc     = config.get("perform_post_process", False)
    postproc_cfg    = config.get("post_process_config", {})
    do_validation   = config.get("perform_validation", False)
    validation_cfg  = config.get("validation_config", {})

    # If "job_output_dir" is provided, make scenario_idf_dir relative to it (if it's not absolute).
    job_output_dir = config.get("job_output_dir")  # optional
    if job_output_dir and not os.path.isabs(scenario_idf_dir):
        scenario_idf_dir = os.path.join(job_output_dir, scenario_idf_dir)
    os.makedirs(scenario_idf_dir, exist_ok=True)

    logger.info(f"[MODIFICATION] Scenario IDFs will be placed in: {scenario_idf_dir}")

    # -----------------------------------------------------------------------
    # 2) Load assigned CSV data
    # -----------------------------------------------------------------------
    # HVAC
    df_hvac_bld = None
    df_hvac_zn  = None
    if "hvac_building" in assigned_csvs and "hvac_zones" in assigned_csvs:
        df_hvac_bld = load_assigned_csv(assigned_csvs["hvac_building"])
        df_hvac_zn  = load_assigned_csv(assigned_csvs["hvac_zones"])
    elif "hvac" in assigned_csvs:
        df_hvac_bld = load_assigned_csv(assigned_csvs["hvac"])

    # DHW
    df_dhw = load_assigned_csv(assigned_csvs["dhw"]) if "dhw" in assigned_csvs else None

    # Vent
    df_vent_bld = None
    df_vent_zn  = None
    if "vent_build" in assigned_csvs and "vent_zones" in assigned_csvs:
        df_vent_bld = load_assigned_csv(assigned_csvs["vent_build"])
        df_vent_zn  = load_assigned_csv(assigned_csvs["vent_zones"])
    elif "vent" in assigned_csvs:
        df_vent_bld = load_assigned_csv(assigned_csvs["vent"])

    # Elec
    df_elec  = load_assigned_csv(assigned_csvs["elec"])  if "elec"  in assigned_csvs else None

    # Fenestration
    df_fenez = load_assigned_csv(assigned_csvs["fenez"]) if "fenez" in assigned_csvs else None

    # -----------------------------------------------------------------------
    # 3) Filter data for this building
    # -----------------------------------------------------------------------
    def filter_for_building(df):
        if df is not None and not df.empty:
            return df[df["ogc_fid"] == building_id].copy()
        return pd.DataFrame()

    df_hvac_bld_sub = filter_for_building(df_hvac_bld)
    df_hvac_zn_sub  = filter_for_building(df_hvac_zn)
    df_dhw_sub      = filter_for_building(df_dhw)
    df_vent_bld_sub = filter_for_building(df_vent_bld)
    df_vent_zn_sub  = filter_for_building(df_vent_zn)
    df_elec_sub     = filter_for_building(df_elec)
    df_fenez_sub    = filter_for_building(df_fenez)

    # -----------------------------------------------------------------------
    # 4) Generate scenario picks (random or otherwise)
    # -----------------------------------------------------------------------
    # HVAC
    if not df_hvac_bld_sub.empty:
        if not df_hvac_zn_sub.empty:
            # multi-step scenario creation for building & zone
            create_hvac_scenarios(
                df_building=df_hvac_bld_sub,
                df_zones=df_hvac_zn_sub,
                building_id=building_id,
                num_scenarios=num_scenarios,
                picking_method=picking_method,
                random_seed=42,
                scenario_csv_out=scenario_csvs["hvac"]
            )
        else:
            hvac_scen = generate_multiple_param_sets(
                df_main_sub=df_hvac_bld_sub,
                num_sets=num_scenarios,
                picking_method=picking_method,
                scale_factor=scale_factor
            )
            save_param_scenarios_to_csv(hvac_scen, building_id, scenario_csvs["hvac"])

    # DHW
    if not df_dhw_sub.empty:
        create_dhw_scenarios(
            df_dhw_input=df_dhw_sub,
            building_id=building_id,
            num_scenarios=num_scenarios,
            picking_method=picking_method,
            random_seed=42,
            scenario_csv_out=scenario_csvs["dhw"]
        )

    # Vent
    if not df_vent_bld_sub.empty or not df_vent_zn_sub.empty:
        create_vent_scenarios(
            df_building=df_vent_bld_sub,
            df_zones=df_vent_zn_sub,
            building_id=building_id,
            num_scenarios=num_scenarios,
            picking_method=picking_method,
            random_seed=42,
            scenario_csv_out=scenario_csvs["vent"]
        )

    # Elec
    if not df_elec_sub.empty:
        create_elec_scenarios(
            df_lighting=df_elec_sub,
            building_id=building_id,
            num_scenarios=num_scenarios,
            picking_method=picking_method,
            random_seed=42,
            scenario_csv_out=scenario_csvs["elec"]
        )

    # Fenestration
    if not df_fenez_sub.empty:
        create_fenez_scenarios(
            df_struct_fenez=df_fenez_sub,
            building_id=building_id,
            num_scenarios=num_scenarios,
            picking_method=picking_method,
            random_seed=42,
            scenario_csv_out=scenario_csvs["fenez"]
        )

    # -----------------------------------------------------------------------
    # 5) Load scenario CSV => group by scenario_index
    # -----------------------------------------------------------------------
    def safe_load_scenario(csv_path):
        if os.path.isfile(csv_path):
            return load_scenario_csv(csv_path)
        return pd.DataFrame()

    df_hvac_scen  = safe_load_scenario(scenario_csvs["hvac"])
    df_dhw_scen   = safe_load_scenario(scenario_csvs["dhw"])
    df_vent_scen  = safe_load_scenario(scenario_csvs["vent"])
    df_elec_scen  = safe_load_scenario(scenario_csvs["elec"])
    df_fenez_scen = safe_load_scenario(scenario_csvs["fenez"])

    hvac_groups  = df_hvac_scen.groupby("scenario_index")  if not df_hvac_scen.empty  else None
    dhw_groups   = df_dhw_scen.groupby("scenario_index")   if not df_dhw_scen.empty   else None
    vent_groups  = df_vent_scen.groupby("scenario_index")  if not df_vent_scen.empty  else None
    elec_groups  = df_elec_scen.groupby("scenario_index")  if not df_elec_scen.empty  else None
    fenez_groups = df_fenez_scen.groupby("scenario_index") if not df_fenez_scen.empty else None

    # -----------------------------------------------------------------------
    # 6) For each scenario, load base IDF, apply parameters, save new IDF
    # -----------------------------------------------------------------------
    for i in range(num_scenarios):
        logger.info(f"[MODIFICATION] => Creating scenario #{i} for building {building_id}")

        hvac_df   = hvac_groups.get_group(i) if hvac_groups and i in hvac_groups.groups else pd.DataFrame()
        dhw_df    = dhw_groups.get_group(i)  if dhw_groups  and i in dhw_groups.groups  else pd.DataFrame()
        vent_df   = vent_groups.get_group(i) if vent_groups and i in vent_groups.groups else pd.DataFrame()
        elec_df   = elec_groups.get_group(i) if elec_groups and i in elec_groups.groups else pd.DataFrame()
        fenez_df  = fenez_groups.get_group(i)if fenez_groups and i in fenez_groups.groups else pd.DataFrame()

        hvac_bld_df   = hvac_df[hvac_df["zone_name"].isna()]
        hvac_zone_df  = hvac_df[hvac_df["zone_name"].notna()]
        hvac_params   = _make_param_dict(hvac_bld_df)

        dhw_params    = _make_param_dict(dhw_df)

        vent_bld_df   = vent_df[vent_df["zone_name"].isnull()]
        vent_zone_df  = vent_df[vent_df["zone_name"].notnull()]
        vent_params   = _make_param_dict(vent_bld_df)

        elec_params   = _make_param_dict(elec_df)

        # Load base IDF
        idf = load_idf(base_idf_path, idd_path)

        # Apply HVAC
        apply_building_level_hvac(idf, hvac_params)
        apply_zone_level_hvac(idf, hvac_zone_df)

        # Apply DHW
        apply_dhw_params_to_idf(idf, dhw_params, suffix=f"Scenario_{i}")

        # Apply Vent
        if not vent_bld_df.empty or not vent_zone_df.empty:
            apply_building_level_vent(idf, vent_params)
            apply_zone_level_vent(idf, vent_zone_df)

        # Apply Elec => building-level or object-level
        if not elec_df.empty:
            apply_building_level_elec(idf, elec_params, zonelist_name="ALL_ZONES")
            # or use apply_object_level_elec(idf, elec_df) if you prefer

        # Apply Fenestration => object-level
        apply_object_level_fenez(idf, fenez_df)

        # Save scenario IDF
        scenario_idf_name = f"building_{building_id}_scenario_{i}.idf"
        scenario_idf_path = os.path.join(scenario_idf_dir, scenario_idf_name)
        save_idf(idf, scenario_idf_path)
        logger.info(f"[MODIFICATION] Saved scenario IDF => {scenario_idf_path}")

    logger.info("[MODIFICATION] All scenario IDFs generated successfully.")

    # -----------------------------------------------------------------------
    # 7) (Optional) Simulations
    # -----------------------------------------------------------------------
    if run_sims:
        logger.info("[MODIFICATION] Running E+ simulations for all scenario IDFs.")
        base_sim_dir = sim_cfg.get("output_dir", "output/Sim_Results/Scenarios")

        # if job_output_dir is given, we can make the sim results go inside it, too:
        if job_output_dir and not os.path.isabs(base_sim_dir):
            base_sim_dir = os.path.join(job_output_dir, base_sim_dir)
        os.makedirs(base_sim_dir, exist_ok=True)

        num_workers  = sim_cfg.get("num_workers", 4)

        run_all_idfs_in_folder(
            folder_path=scenario_idf_dir,
            iddfile=idd_path,
            base_output_dir=base_sim_dir,
            default_lat=52.15,
            default_lon=4.40,
            default_year=2020,
            num_workers=num_workers
        )

    # -----------------------------------------------------------------------
    # 8) (Optional) Post-processing
    # -----------------------------------------------------------------------
    if do_postproc:
        logger.info("[MODIFICATION] Performing post-processing merges.")

        base_sim_dir = sim_cfg.get("output_dir", "output/Sim_Results/Scenarios")
        if job_output_dir and not os.path.isabs(base_sim_dir):
            base_sim_dir = os.path.join(job_output_dir, base_sim_dir)

        output_csv_as_is = postproc_cfg.get("output_csv_as_is", "")
        output_csv_daily_mean = postproc_cfg.get("output_csv_daily_mean", "")

        # Build full paths inside job_output_dir if they are relative
        if output_csv_as_is:
            if job_output_dir and not os.path.isabs(output_csv_as_is):
                output_csv_as_is = os.path.join(job_output_dir, output_csv_as_is)
            os.makedirs(os.path.dirname(output_csv_as_is), exist_ok=True)

            merge_all_results(
                base_output_dir=base_sim_dir,
                output_csv=output_csv_as_is,
                convert_to_daily=False,
                convert_to_monthly=False
            )

        if output_csv_daily_mean:
            if job_output_dir and not os.path.isabs(output_csv_daily_mean):
                output_csv_daily_mean = os.path.join(job_output_dir, output_csv_daily_mean)
            os.makedirs(os.path.dirname(output_csv_daily_mean), exist_ok=True)

            merge_all_results(
                base_output_dir=base_sim_dir,
                output_csv=output_csv_daily_mean,
                convert_to_daily=True,
                daily_aggregator="mean",
                convert_to_monthly=False
            )

        logger.info("[MODIFICATION] Post-processing step complete.")

    # -----------------------------------------------------------------------
    # 9) (Optional) Validation
    # -----------------------------------------------------------------------
    if do_validation:
        logger.info("[MODIFICATION] Performing scenario validation with config => %s", validation_cfg)

        # If the validation config references CSV paths that might be relative,
        # you could also adjust them to be inside job_output_dir here if desired.
        # e.g.:
        # real_csv = validation_cfg.get("real_data_csv", "")
        # if job_output_dir and not os.path.isabs(real_csv):
        #     real_csv = os.path.join(job_output_dir, real_csv)
        # validation_cfg["real_data_csv"] = real_csv

        run_validation_process(validation_cfg)
        logger.info("[MODIFICATION] Validation step complete.")


def _make_param_dict(df_scenario):
    """
    Builds a dict {param_name: value} from the scenario DataFrame columns,
    checking 'assigned_value' or 'param_value'.
    """
    if df_scenario.empty:
        return {}

    cols = df_scenario.columns.tolist()
    if "assigned_value" in cols:
        val_col = "assigned_value"
    elif "param_value" in cols:
        val_col = "param_value"
    else:
        raise AttributeError(
            "No 'assigned_value' or 'param_value' column found in scenario dataframe! "
            f"Columns are: {cols}"
        )

    result = {}
    for row in df_scenario.itertuples():
        p_name = row.param_name
        raw_val = getattr(row, val_col)
        try:
            result[p_name] = float(raw_val)
        except (ValueError, TypeError):
            result[p_name] = raw_val
    return result


============================================================
File: D:\Documents\E_Plus_2030_py\main.py
============================================================
"""
main.py as an API server that:
  - Provides a single endpoint (/run-workflow) which:
    * Receives a combined JSON payload
    * Splits it into sub-JSON files in user_configs/ folder
    * (Optionally) deep merges main_config with existing main_config.json
    * Spawns the entire E+ workflow in a background thread
    * Streams logs to the client in real time
"""

import os
import json
import logging
import threading
import queue
import time
from flask import Flask, request, Response, jsonify

import pandas as pd

# ------------------------------------------------------------------------
# (1) Import from your own modules
# ------------------------------------------------------------------------
from splitter import split_combined_json, deep_merge_dicts  # <--- from splitter.py
from database_handler import load_buildings_from_db
from excel_overrides import (
    override_dhw_lookup_from_excel_file,
    override_epw_lookup_from_excel_file,
    override_lighting_lookup_from_excel_file,
    override_hvac_lookup_from_excel_file,
    override_vent_lookup_from_excel_file
)
from idf_objects.fenez.fenez_config_manager import build_fenez_config
import idf_creation
from idf_creation import create_idfs_for_all_buildings
from main_modifi import run_modification_workflow
from validation.main_validation import run_validation_process
from cal.unified_sensitivity import run_sensitivity_analysis
from cal.unified_surrogate import (
    load_scenario_params as sur_load_scenario_params,
    pivot_scenario_params,
    filter_top_parameters,
    load_sim_results,
    aggregate_results,
    merge_params_with_results,
    build_and_save_surrogate
)
from cal.unified_calibration import run_unified_calibration

###############################################################################
# Custom Logging: We'll create a special handler that sends logs into a queue
###############################################################################
class QueueLoggerHandler(logging.Handler):
    """Custom logging handler that pushes logs into a queue."""
    def __init__(self, log_queue):
        super().__init__()
        self.log_queue = log_queue

    def emit(self, record):
        try:
            msg = self.format(record)
            self.log_queue.put(msg)
        except Exception:
            self.handleError(record)


###############################################################################
# 1) Logging Setup
###############################################################################
def setup_logging_for_queue(log_queue, log_level=logging.INFO):
    """
    Configure the root logger to also send logs to an in-memory queue
    so we can stream them out to the HTTP client.
    """
    logger = logging.getLogger()
    logger.setLevel(log_level)

    # Optional: remove old handlers if you want a fresh start
    for h in logger.handlers[:]:
        logger.removeHandler(h)

    # 1) Console/logfile handlers as you wish
    console_handler = logging.StreamHandler()
    console_handler.setLevel(log_level)
    console_fmt = logging.Formatter("%(asctime)s [%(levelname)s] %(name)s: %(message)s")
    console_handler.setFormatter(console_fmt)
    logger.addHandler(console_handler)

    # 2) Our queue handler
    queue_handler = QueueLoggerHandler(log_queue)
    queue_handler.setLevel(log_level)
    queue_handler.setFormatter(console_fmt)
    logger.addHandler(queue_handler)

    return logger


###############################################################################
# 2) Utility: load JSON from file
###############################################################################
def load_json(filepath):
    if not os.path.isfile(filepath):
        raise FileNotFoundError(f"JSON file not found: {filepath}")
    with open(filepath, "r") as f:
        return json.load(f)


###############################################################################
# 3) The Main Orchestration Workflow
###############################################################################
def orchestrate_workflow(override_json=None):
    """
    The entire logic that was previously in main().
    `override_json` can be any final overrides you want to merge with main_config.
    """
    logger = logging.getLogger(__name__)
    logger.info("=== Starting orchestrate_workflow ===")

    # --------------------------------------------------------------------------
    # A) Load the existing main_config.json from user_configs
    # --------------------------------------------------------------------------
    current_dir = os.getcwd()
    user_configs_folder = os.path.join(current_dir, "user_configs")
    main_config_path = os.path.join(user_configs_folder, "main_config.json")

    if not os.path.isfile(main_config_path):
        logger.error(f"[ERROR] Cannot find main_config.json at {main_config_path}")
        return  # or raise an exception

    main_config_file = load_json(main_config_path)
    # main_config_file should look like {"main_config": {...}} if we splitted it using split_combined_json
    main_config = main_config_file.get("main_config", {})

    # --------------------------------------------------------------------------
    # B) Deep Merge override_json["main_config"] if provided
    # --------------------------------------------------------------------------
    if override_json and "main_config" in override_json:
        # override_json itself might contain other top-level keys, but let's specifically
        # deep-merge the "main_config" portion
        logger.info("[INFO] Deep merging override_json['main_config'] into existing main_config ...")
        from splitter import deep_merge_dicts  # or use the one already imported
        deep_merge_dicts(main_config, override_json["main_config"])
        # Re-save the updated main_config to disk
        with open(main_config_path, "w") as f:
            json.dump({"main_config": main_config}, f, indent=2)

    # Now we have our final main_config in memory
    logger.info("[INFO] Final main_config loaded & possibly merged.")

    # --------------------------------------------------------------------------
    # C) Possibly override idf_creation config with environment variables
    # --------------------------------------------------------------------------
    # NOTE: 'idf_creation.idf_config' is a module-level global in 'idf_creation.py'.
    # This approach can be fragile if used by multiple threads or multiple calls.
    # But we'll keep it for now.
    env_idd_path = os.environ.get("IDD_PATH")
    if env_idd_path:
        idf_creation.idf_config["iddfile"] = env_idd_path

    env_base_idf = os.environ.get("BASE_IDF_PATH")
    if env_base_idf:
        idf_creation.idf_config["idf_file_path"] = env_base_idf

    env_out_dir = os.environ.get("OUTPUT_DIR")
    if env_out_dir:
        out_idf_dir = os.path.join(env_out_dir, "output_IDFs")
        idf_creation.idf_config["output_dir"] = out_idf_dir

    # Merge the local config from main_config["idf_creation"]
    idf_cfg = main_config.get("idf_creation", {})
    if "iddfile" in idf_cfg:
        idf_creation.idf_config["iddfile"] = idf_cfg["iddfile"]
    if "idf_file_path" in idf_cfg:
        idf_creation.idf_config["idf_file_path"] = idf_cfg["idf_file_path"]
    if "output_idf_dir" in idf_cfg:
        idf_creation.idf_config["output_dir"] = idf_cfg["output_idf_dir"]

    # --------------------------------------------------------------------------
    # D) Extract top-level fields from main_config
    # --------------------------------------------------------------------------
    paths_dict     = main_config.get("paths", {})
    excel_flags    = main_config.get("excel_overrides", {})
    user_flags     = main_config.get("user_config_overrides", {})
    def_dicts      = main_config.get("default_dicts", {})
    structuring_cfg  = main_config.get("structuring", {})
    modification_cfg = main_config.get("modification", {})
    validation_cfg   = main_config.get("validation", {})
    sens_cfg         = main_config.get("sensitivity", {})
    sur_cfg          = main_config.get("surrogate", {})
    cal_cfg          = main_config.get("calibration", {})

    # From idf_creation
    perform_idf_creation = idf_cfg.get("perform_idf_creation", False)
    scenario             = idf_cfg.get("scenario", "scenario1")
    calibration_stage    = idf_cfg.get("calibration_stage", "pre_calibration")
    strategy             = idf_cfg.get("strategy", "B")
    random_seed          = idf_cfg.get("random_seed", 42)
    run_simulations      = idf_cfg.get("run_simulations", True)
    simulate_config      = idf_cfg.get("simulate_config", {})
    post_process         = idf_cfg.get("post_process", True)
    post_process_config  = idf_cfg.get("post_process_config", {})
    output_definitions   = idf_cfg.get("output_definitions", {})
    use_database         = main_config.get("use_database", False)
    db_filter            = main_config.get("db_filter", {})

    # Setup default dicts
    base_res_data    = def_dicts.get("res_data", {})
    base_nonres_data = def_dicts.get("nonres_data", {})
    dhw_lookup       = def_dicts.get("dhw", {})
    epw_lookup       = def_dicts.get("epw", [])
    lighting_lookup  = def_dicts.get("lighting", {})
    hvac_lookup      = def_dicts.get("hvac", {})
    vent_lookup      = def_dicts.get("vent", {})

    # --------------------------------------------------------------------------
    # E) Potentially override fenestration, DHW, EPW, etc. from Excel
    # (if excel_overrides flags are set)
    # --------------------------------------------------------------------------
    from idf_objects.fenez.fenez_config_manager import build_fenez_config
    updated_res_data, updated_nonres_data = build_fenez_config(
        base_res_data=base_res_data,
        base_nonres_data=base_nonres_data,
        excel_path=paths_dict.get("fenez_excel", ""),
        do_excel_override=excel_flags.get("override_fenez_excel", False),
        user_fenez_overrides=[]
    )

    if excel_flags.get("override_dhw_excel", False):
        from excel_overrides import override_dhw_lookup_from_excel_file
        dhw_lookup = override_dhw_lookup_from_excel_file(
            dhw_excel_path=paths_dict.get("dhw_excel", ""),
            default_dhw_lookup=dhw_lookup,
            override_dhw_flag=True
        )

    if excel_flags.get("override_epw_excel", False):
        from excel_overrides import override_epw_lookup_from_excel_file
        epw_lookup = override_epw_lookup_from_excel_file(
            epw_excel_path=paths_dict.get("epw_excel", ""),
            epw_lookup=epw_lookup,
            override_epw_flag=True
        )

    if excel_flags.get("override_lighting_excel", False):
        from excel_overrides import override_lighting_lookup_from_excel_file
        lighting_lookup = override_lighting_lookup_from_excel_file(
            lighting_excel_path=paths_dict.get("lighting_excel", ""),
            lighting_lookup=lighting_lookup,
            override_lighting_flag=True
        )

    if excel_flags.get("override_hvac_excel", False):
        from excel_overrides import override_hvac_lookup_from_excel_file
        hvac_lookup = override_hvac_lookup_from_excel_file(
            hvac_excel_path=paths_dict.get("hvac_excel", ""),
            hvac_lookup=hvac_lookup,
            override_hvac_flag=True
        )

    if excel_flags.get("override_vent_excel", False):
        from excel_overrides import override_vent_lookup_from_excel_file
        vent_lookup = override_vent_lookup_from_excel_file(
            vent_excel_path=paths_dict.get("vent_excel", ""),
            vent_lookup=vent_lookup,
            override_vent_flag=True
        )

    # --------------------------------------------------------------------------
    # F) JSON overrides from user_configs/* if user_flags are set
    #    e.g. fenestration.json, dhw.json, epw.json, lighting.json, hvac.json, vent.json, ...
    # --------------------------------------------------------------------------
    def safe_load_subjson(fname, key):
        """
        Loads user_configs/fname if it exists, returns data.get(key, None).
        """
        full_path = os.path.join(user_configs_folder, fname)
        if os.path.isfile(full_path):
            try:
                data = load_json(full_path)
                return data.get(key, None)
            except Exception as e:
                logger.error(f"[ERROR] loading {fname} => {e}")
        return None

    # Fenestration
    user_fenez_data = []
    if user_flags.get("override_fenez_json", False):
        fenez_data = safe_load_subjson("fenestration.json", "fenestration")
        if fenez_data:
            user_fenez_data = fenez_data
    updated_res_data, updated_nonres_data = build_fenez_config(
        base_res_data=updated_res_data,
        base_nonres_data=updated_nonres_data,
        excel_path="",  # no further Excel overrides
        do_excel_override=False,
        user_fenez_overrides=user_fenez_data
    )

    # DHW
    user_config_dhw = None
    if user_flags.get("override_dhw_json", False):
        user_config_dhw = safe_load_subjson("dhw.json", "dhw")

    # EPW
    user_config_epw = []
    if user_flags.get("override_epw_json", False):
        epw_data = safe_load_subjson("epw.json", "epw")
        if epw_data:
            user_config_epw = epw_data

    # Lighting
    user_config_lighting = None
    if user_flags.get("override_lighting_json", False):
        user_config_lighting = safe_load_subjson("lighting.json", "lighting")

    # HVAC
    user_config_hvac = None
    if user_flags.get("override_hvac_json", False):
        user_config_hvac = safe_load_subjson("hvac.json", "hvac")

    # Vent
    user_config_vent = []
    if user_flags.get("override_vent_json", False):
        vent_data = safe_load_subjson("vent.json", "vent")
        if vent_data:
            user_config_vent = vent_data

    # Geometry
    geom_data = {}
    if user_flags.get("override_geometry_json", False):
        geometry_loaded = safe_load_subjson("geometry.json", "geometry")
        if geometry_loaded:
            geom_data["geometry"] = geometry_loaded

    # Shading
    shading_data = {}
    if user_flags.get("override_shading_json", False):
        shading_loaded = safe_load_subjson("shading.json", "shading")
        if shading_loaded:
            shading_data["shading"] = shading_loaded

    # --------------------------------------------------------------------------
    # G) IDF Creation Step
    # --------------------------------------------------------------------------
    if perform_idf_creation:
        logger.info("[INFO] IDF creation is ENABLED.")

        # (1) Get building DataFrame
        df_buildings = pd.DataFrame()
        if use_database:
            logger.info("[INFO] Loading building data from DB.")
            df_buildings = load_buildings_from_db(db_filter)
            if df_buildings.empty:
                logger.warning("[WARN] No buildings returned from DB filters.")
        else:
            bldg_data_path = paths_dict.get("building_data", "")
            if os.path.isfile(bldg_data_path):
                df_buildings = pd.read_csv(bldg_data_path)
            else:
                logger.warning(f"[WARN] Building data CSV not found => {bldg_data_path}")

        # (2) Create IDFs
        from idf_creation import create_idfs_for_all_buildings
        create_idfs_for_all_buildings(
            df_buildings=df_buildings,
            scenario=scenario,
            calibration_stage=calibration_stage,
            strategy=strategy,
            random_seed=random_seed,
            user_config_geom=geom_data.get("geometry", []),
            user_config_lighting=user_config_lighting,
            user_config_dhw=user_config_dhw,
            res_data=updated_res_data,
            nonres_data=updated_nonres_data,
            user_config_hvac=user_config_hvac,
            user_config_vent=user_config_vent,
            user_config_epw=user_config_epw,
            output_definitions=output_definitions,
            run_simulations=run_simulations,
            simulate_config=simulate_config,
            post_process=post_process,
            post_process_config=post_process_config
        )
    else:
        logger.info("[INFO] Skipping IDF creation.")

    # --------------------------------------------------------------------------
    # H) Structuring Step
    # --------------------------------------------------------------------------
    if structuring_cfg.get("perform_structuring", False):
        logger.info("[INFO] Performing structuring ...")

        # Example fenestration struct
        from idf_objects.structuring.fenestration_structuring import (
            transform_fenez_log_to_structured_with_ranges
        )
        fenez_conf = structuring_cfg.get("fenestration", {})
        fenez_in   = fenez_conf.get("csv_in",  "output/assigned/assigned_fenez_params.csv")
        fenez_out  = fenez_conf.get("csv_out", "output/assigned/structured_fenez_params.csv")
        transform_fenez_log_to_structured_with_ranges(csv_input=fenez_in, csv_output=fenez_out)

        # Similarly for DHW, HVAC, Vent ...
        from idf_objects.structuring.dhw_structuring import transform_dhw_log_to_structured
        dhw_conf = structuring_cfg.get("dhw", {})
        dhw_in   = dhw_conf.get("csv_in",  "output/assigned/assigned_dhw_params.csv")
        dhw_out  = dhw_conf.get("csv_out", "output/assigned/structured_dhw_params.csv")
        transform_dhw_log_to_structured(csv_input=dhw_in, csv_output=dhw_out)

        from idf_objects.structuring.flatten_hvac import flatten_hvac_data, parse_assigned_value as parse_hvac
        hvac_conf = structuring_cfg.get("hvac", {})
        hvac_in   = hvac_conf.get("csv_in", "output/assigned/assigned_hvac_params.csv")
        hvac_bld  = hvac_conf.get("build_out", "output/assigned/assigned_hvac_building.csv")
        hvac_zone = hvac_conf.get("zone_out",  "output/assigned/assigned_hvac_zones.csv")
        if os.path.isfile(hvac_in):
            df_hvac = pd.read_csv(hvac_in)
            df_hvac["assigned_value"] = df_hvac["assigned_value"].apply(parse_hvac)
            flatten_hvac_data(df_input=df_hvac, out_build_csv=hvac_bld, out_zone_csv=hvac_zone)

        from idf_objects.structuring.flatten_assigned_vent import (
            flatten_ventilation_data,
            parse_assigned_value as parse_vent
        )
        vent_conf = structuring_cfg.get("vent", {})
        vent_in   = vent_conf.get("csv_in", "output/assigned/assigned_ventilation.csv")
        vent_bld  = vent_conf.get("build_out", "output/assigned/assigned_vent_building.csv")
        vent_zone = vent_conf.get("zone_out", "output/assigned/assigned_vent_zones.csv")
        if os.path.isfile(vent_in):
            df_vent = pd.read_csv(vent_in)
            df_vent["assigned_value"] = df_vent["assigned_value"].apply(parse_vent)
            flatten_ventilation_data(df_input=df_vent, out_build_csv=vent_bld, out_zone_csv=vent_zone)
    else:
        logger.info("[INFO] Skipping structuring.")

    # --------------------------------------------------------------------------
    # I) Scenario Modification
    # --------------------------------------------------------------------------
    if modification_cfg.get("perform_modification", False):
        logger.info("[INFO] Scenario modification is ENABLED.")
        run_modification_workflow(modification_cfg["modify_config"])
    else:
        logger.info("[INFO] Skipping scenario modification.")

    # --------------------------------------------------------------------------
    # J) Global Validation
    # --------------------------------------------------------------------------
    if validation_cfg.get("perform_validation", False):
        logger.info("[INFO] Global Validation is ENABLED.")
        run_validation_process(validation_cfg["config"])
    else:
        logger.info("[INFO] Skipping global validation.")

    # --------------------------------------------------------------------------
    # K) Sensitivity Analysis
    # --------------------------------------------------------------------------
    if sens_cfg.get("perform_sensitivity", False):
        logger.info("[INFO] Sensitivity Analysis is ENABLED.")
        run_sensitivity_analysis(
            scenario_folder=sens_cfg["scenario_folder"],
            method=sens_cfg["method"],
            results_csv=sens_cfg.get("results_csv", ""),
            target_variable=sens_cfg.get("target_variable", ""),
            output_csv=sens_cfg.get("output_csv", "sensitivity_output.csv"),
            n_morris_trajectories=sens_cfg.get("n_morris_trajectories", 10),
            num_levels=sens_cfg.get("num_levels", 4),
            n_sobol_samples=sens_cfg.get("n_sobol_samples", 128)
        )
    else:
        logger.info("[INFO] Skipping sensitivity analysis.")

    # --------------------------------------------------------------------------
    # L) Surrogate Modeling
    # --------------------------------------------------------------------------
    if sur_cfg.get("perform_surrogate", False):
        logger.info("[INFO] Surrogate Modeling is ENABLED.")
        scenario_folder = sur_cfg["scenario_folder"]
        results_csv     = sur_cfg["results_csv"]
        target_var      = sur_cfg["target_variable"]
        model_out       = sur_cfg["model_out"]
        cols_out        = sur_cfg["cols_out"]
        test_size       = sur_cfg["test_size"]

        df_scen = sur_load_scenario_params(scenario_folder)
        pivot_df = pivot_scenario_params(df_scen)

        # pivot_df = filter_top_parameters(...) # if needed
        df_sim = load_sim_results(results_csv)
        df_agg = aggregate_results(df_sim)
        merged_df = merge_params_with_results(pivot_df, df_agg, target_var)

        rf_model, trained_cols = build_and_save_surrogate(
            df_data=merged_df,
            target_col=target_var,
            model_out_path=model_out,
            columns_out_path=cols_out,
            test_size=test_size,
            random_state=42
        )
        if rf_model:
            logger.info("[INFO] Surrogate model built & saved.")
        else:
            logger.warning("[WARN] Surrogate modeling failed or insufficient data.")
    else:
        logger.info("[INFO] Skipping surrogate modeling.")

    # --------------------------------------------------------------------------
    # M) Calibration
    # --------------------------------------------------------------------------
    if cal_cfg.get("perform_calibration", False):
        logger.info("[INFO] Calibration is ENABLED.")
        run_unified_calibration(cal_cfg)
    else:
        logger.info("[INFO] Skipping calibration.")

    logger.info("=== End of orchestrate_workflow ===")


###############################################################################
# 4) Flask App for Real-Time Streaming
###############################################################################
app = Flask(__name__)

@app.route("/run-workflow", methods=["POST"])
def run_workflow_api():
    """
    This single endpoint:
      1) Receives a combined JSON in the request body.
      2) Splits it into sub-JSON files in user_configs/ (via split_combined_json).
      3) Deep merges 'main_config' from the request (if present) with existing user_configs/main_config.json
      4) Spawns the orchestrate_workflow in a background thread
      5) Streams logs back to the client in real time.
    """
    if not request.is_json:
        return jsonify({"error": "Expected JSON payload"}), 400

    posted_data = request.get_json()
    user_configs_folder = os.path.join(os.getcwd(), "user_configs")

    # 1) Split the posted data into separate JSON files (dhw.json, epw.json, etc.)
    split_combined_json(posted_data, user_configs_folder)

    # 2) We'll pass the entire posted_data as override_json to orchestrate_workflow.
    #    The orchestrate_workflow function itself will do the deep merge of "main_config".
    override_json = posted_data

    # 3) Create a thread-safe queue for logs
    log_queue = queue.Queue()
    setup_logging_for_queue(log_queue, log_level=logging.INFO)

    # 4) Background function to run the workflow
    def background_workflow():
        try:
            orchestrate_workflow(override_json=override_json)
        except Exception as e:
            logging.getLogger(__name__).exception(f"Workflow crashed: {e}")
        finally:
            # Put a sentinel (None) to signal that logs are done
            log_queue.put(None)

    # 5) Start the background thread
    t = threading.Thread(target=background_workflow, daemon=True)
    t.start()

    # 6) Define a generator that yields log lines
    def log_stream():
        while True:
            msg = log_queue.get()
            if msg is None:
                break
            yield msg + "\n"

    # 7) Return a streaming response
    return Response(log_stream(), mimetype='text/plain')


if __name__ == "__main__":
    # Run the Flask app on port 8000
    app.run(host="0.0.0.0", port=8000, debug=True)


